{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LGBM with Fourier transform","metadata":{}},{"cell_type":"markdown","source":"In this notebook I will show you how to easily generate features from time series using Fourier transformation\n\nFourier transform is widely used in real world task such as sound noise reduction, image compression. And it is also used in medical applications so it\\`s perfect to apply it for this task!","metadata":{}},{"cell_type":"markdown","source":"You can read more about Fourier transform:\n- [here](https://realpython.com/python-scipy-fft/) or\n- [here](https://thefouriertransform.com/)  \nAnd watch this video to get visual intuition about it:\n- [video](https://www.youtube.com/watch?v=spUNpyF58BY)","metadata":{}},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"markdown","source":"Our train and test data consist of 13 sensor indications at every step. The index is id of subject and sequence","metadata":{}},{"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nX_train = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\nX_train.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-10T12:30:21.196598Z","iopub.execute_input":"2022-04-10T12:30:21.197433Z","iopub.status.idle":"2022-04-10T12:30:28.963733Z","shell.execute_reply.started":"2022-04-10T12:30:21.197329Z","shell.execute_reply":"2022-04-10T12:30:28.963027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\ny_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:30:28.965154Z","iopub.execute_input":"2022-04-10T12:30:28.965553Z","iopub.status.idle":"2022-04-10T12:30:28.984988Z","shell.execute_reply.started":"2022-04-10T12:30:28.965508Z","shell.execute_reply":"2022-04-10T12:30:28.984187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:30:28.986161Z","iopub.execute_input":"2022-04-10T12:30:28.986373Z","iopub.status.idle":"2022-04-10T12:30:32.657719Z","shell.execute_reply.started":"2022-04-10T12:30:28.986348Z","shell.execute_reply":"2022-04-10T12:30:32.656894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"Now let\\`s get a closer look at data. It always a good improvment in your final score if you find some data leak which other competitors or organizers don\\`t suppose to exist. My first idea was to check whether there are same subjects in train and test","metadata":{}},{"cell_type":"markdown","source":"But we see no intersection here","metadata":{}},{"cell_type":"code","source":"set(X_train['subject']).intersection(set(X_test['subject']))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:30:33.381006Z","iopub.execute_input":"2022-04-10T12:30:33.381239Z","iopub.status.idle":"2022-04-10T12:30:33.605833Z","shell.execute_reply.started":"2022-04-10T12:30:33.381212Z","shell.execute_reply":"2022-04-10T12:30:33.604919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The second idea was to analyze how many times subject appears in train and test","metadata":{}},{"cell_type":"code","source":"%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:30:32.65933Z","iopub.execute_input":"2022-04-10T12:30:32.65963Z","iopub.status.idle":"2022-04-10T12:30:32.666122Z","shell.execute_reply.started":"2022-04-10T12:30:32.659536Z","shell.execute_reply":"2022-04-10T12:30:32.665248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\n_, axs = plt.subplots(1, 2, figsize=(15, 5))\nX_train.groupby('subject')['sequence'].nunique().value_counts().plot(kind='hist', ax=axs[0], title='Train')\nX_test.groupby('subject')['sequence'].nunique().value_counts().plot(kind='hist', ax=axs[1], title='Test')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:56:47.121647Z","iopub.execute_input":"2022-04-10T12:56:47.121939Z","iopub.status.idle":"2022-04-10T12:56:47.661403Z","shell.execute_reply.started":"2022-04-10T12:56:47.12191Z","shell.execute_reply":"2022-04-10T12:56:47.659827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see the most of subjects appears up to 5 times in train and test. But there are subjects with many appearence. And if a subject appears to much in data, it has also a bigger target rate. It defenetly could be used as a feature","metadata":{}},{"cell_type":"code","source":"from IPython.display import display\n\n# add title\ntarget_rate = X_train[['subject', 'sequence']].drop_duplicates()\\\n    .merge(y_train, on='sequence')\ntarget_rate.groupby('subject').agg({'state': 'mean', 'sequence': 'count'})\\\n    .sort_values('sequence').plot(x='sequence', y='state', ylabel='target rate', xlabel='appearence')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:58:27.147347Z","iopub.execute_input":"2022-04-10T12:58:27.147695Z","iopub.status.idle":"2022-04-10T12:58:27.477505Z","shell.execute_reply.started":"2022-04-10T12:58:27.147657Z","shell.execute_reply":"2022-04-10T12:58:27.476853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Every sequence has the same length of 60 steps","metadata":{}},{"cell_type":"code","source":"X_train.groupby(['subject', 'sequence'])['step'].nunique().value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:30:33.949894Z","iopub.execute_input":"2022-04-10T12:30:33.950141Z","iopub.status.idle":"2022-04-10T12:30:34.365332Z","shell.execute_reply.started":"2022-04-10T12:30:33.950113Z","shell.execute_reply":"2022-04-10T12:30:34.364523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make frequency features with Fourier transform","metadata":{}},{"cell_type":"markdown","source":"Now let\\`s get to Fourier transform with it\\`s scypi realization. It has a bunch of functions:\n- fft\n- ifft\n- rfft\n- irfft\n- ...  \nwhere ft stands for Fourier transform, first f stands for \"Fast\" - the name of Fourier transform realization, i - stands for inverse transform (ifft(fft(x))==x) and r stands for real meaning our input data doesn\\`t contain complex numbers\n\nSo we first group data by sequence, and for each sensor data column in seuqence with real fast Fourier transform get it representation in \"frequncy space\". We are able to get len(sequnce) / 2 + 1 = 31 frequnces by sequence. And as rfft returns it\\`s values in complex space we use absolute value of frequnce \"power\"\n\nIt\\`s also a good idea to select only \"low\" frequences from rfft as \"high\" frequences often represent noise, but for now we keep all of them","metadata":{}},{"cell_type":"code","source":"from scipy.fft import rfft\nimport numpy as np\n\ndef make_fft_features(group):\n    return pd.concat(\n        [pd.Series(np.abs(rfft(group[col].values)), \n                   index=[f'{col}_freq_{i}' for i in range(31)]) \n         for col in group.columns if col not in ['sequence', 'subject', 'step']\n        ])\n\ntrain_df = X_train.sort_values(['subject', 'sequence', 'step'])\\\n    .groupby(['sequence', 'subject']).apply(make_fft_features)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:30:34.366747Z","iopub.execute_input":"2022-04-10T12:30:34.367147Z","iopub.status.idle":"2022-04-10T12:32:04.165433Z","shell.execute_reply.started":"2022-04-10T12:30:34.367097Z","shell.execute_reply":"2022-04-10T12:32:04.16463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also don\\`t forget to use number of subject appearences as a feature. To correctly use it on train and test normalize it by maximum value respectively","metadata":{}},{"cell_type":"code","source":"n_sequence = X_train.groupby('subject')['sequence'].nunique()\nperc_sequence = n_sequence.rank(method='max').apply(lambda x: 100.0*(x-1)/len(n_sequence))\nperc_sequence.name = 'n_sequence_percentile'\nperc_sequence.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:04.167752Z","iopub.execute_input":"2022-04-10T12:32:04.168022Z","iopub.status.idle":"2022-04-10T12:32:04.293259Z","shell.execute_reply.started":"2022-04-10T12:32:04.167987Z","shell.execute_reply":"2022-04-10T12:32:04.292359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Build the final dataset","metadata":{}},{"cell_type":"code","source":"train_df = train_df.reset_index()\\\n    .merge(perc_sequence, on='subject')\\\n    .merge(y_train, on='sequence')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:04.295017Z","iopub.execute_input":"2022-04-10T12:32:04.295637Z","iopub.status.idle":"2022-04-10T12:32:04.495388Z","shell.execute_reply.started":"2022-04-10T12:32:04.295592Z","shell.execute_reply":"2022-04-10T12:32:04.494444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"At current version simple Light GBM model is used with no parameter tuning. But I decided to show how to use group KFold validation. It is very useful when model prediction is sensible to some hidden variable information like subject id in our case. Also it allows to fit several models on different data, to use them for blending later","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nimport lightgbm\nfrom sklearn.metrics import roc_auc_score\n\nmodel_list = []\ngroup_kfold = GroupKFold(n_splits=5)\nfor train_index, test_index in group_kfold.split(train_df.drop(['sequence', 'subject', 'state'], axis=1), \n                                                 train_df['state'], \n                                                 train_df['subject']):\n    X_train_group = train_df.drop(['sequence', 'subject', 'state'], axis=1).iloc[train_index] \n    X_test_group = train_df.drop(['sequence', 'subject', 'state'], axis=1).iloc[test_index]\n    y_train_group = train_df['state'].iloc[train_index] \n    y_test_group = train_df['state'].iloc[test_index]\n    \n    lgbm = lightgbm.LGBMClassifier()\n    model_list.append(lgbm.fit(X_train_group, y_train_group))\n    fold_score = roc_auc_score(y_test_group, lgbm.predict_proba(X_test_group)[:, 1])\n    print(f'Fold score: {fold_score}')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:04.496735Z","iopub.execute_input":"2022-04-10T12:32:04.497Z","iopub.status.idle":"2022-04-10T12:32:36.173168Z","shell.execute_reply.started":"2022-04-10T12:32:04.49697Z","shell.execute_reply":"2022-04-10T12:32:36.172273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict and final submission","metadata":{}},{"cell_type":"markdown","source":"To get final predictions of our model, perform the same transformations with test data as we did with train","metadata":{}},{"cell_type":"code","source":"test_df = X_test.sort_values(['subject', 'sequence', 'step'])\\\n    .groupby(['sequence', 'subject']).apply(make_fft_features)\nn_sequence_test = X_test.groupby('subject')['sequence'].nunique()\nperc_sequence_test = n_sequence_test.rank(method='max').apply(lambda x: 100.0*(x-1)/len(n_sequence_test))\nperc_sequence_test.name = 'n_sequence_percentile'\ntest_df = test_df.reset_index()\\\n    .merge(perc_sequence_test, on='subject')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:36.174926Z","iopub.execute_input":"2022-04-10T12:32:36.175226Z","iopub.status.idle":"2022-04-10T12:33:17.626287Z","shell.execute_reply.started":"2022-04-10T12:32:36.175184Z","shell.execute_reply":"2022-04-10T12:33:17.625374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get predictions of all models and average them to get final model predictions","metadata":{}},{"cell_type":"code","source":"submission = test_df[['sequence']]\npredictions = pd.DataFrame(\n    [m.predict_proba(test_df.drop(['sequence', 'subject'], axis=1))[:, 1] for m in model_list]).T\nsubmission['state'] = predictions.mean(axis=1)\nsubmission.to_csv('lgbm_fourier.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:33:17.627924Z","iopub.execute_input":"2022-04-10T12:33:17.628231Z","iopub.status.idle":"2022-04-10T12:33:18.463869Z","shell.execute_reply.started":"2022-04-10T12:33:17.628191Z","shell.execute_reply":"2022-04-10T12:33:18.46284Z"},"trusted":true},"execution_count":null,"outputs":[]}]}