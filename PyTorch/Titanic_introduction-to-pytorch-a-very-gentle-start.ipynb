{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "ここら辺を見て、上手いこと改良する\n",
    "\n",
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%capture\\n!pip install --user pycaret -full\\n!pip install numba==0.53\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Takanori\\\\Desktop\\\\Kaggle\\\\titanic\\\\input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet & Library Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/titanic/train.csv')\n",
    "df_test  = pd.read_csv('../input/titanic/test.csv')\n",
    "df_sub   = pd.read_csv('../input/titanic/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    batch_size = 32\n",
    "    epochs = 2000\n",
    "    folds = 10\n",
    "    seed = 42\n",
    "    target = 'Survived'\n",
    "    lr = 0.1\n",
    "    model_path = \"models\"\n",
    "    test_pred = ['pred' + str(i) for i in range(folds)]\n",
    "    pred = 'pred'\n",
    "    early_stopping = 100\n",
    "    lr_factor = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CFG.model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要データを削除\n",
    "df_all.drop(['Name','Ticket','Cabin','PassengerId'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\1836397818.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_all.fillna(df_all.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# NA埋め\n",
    "df_all.fillna(df_all.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットを01に変換\n",
    "df_all[CFG.target].fillna(0, inplace=True)\n",
    "df_all[CFG.target] = df_all[CFG.target].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリ変数をone_hot_encoding\n",
    "sex = pd.get_dummies(df_all['Sex'], drop_first=True)\n",
    "embark = pd.get_dummies(df_all['Embarked'], drop_first=True)\n",
    "df_all = pd.concat([df_all, sex, embark], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要になったカテゴリ変数を削除\n",
    "df_all.drop(['Embarked', 'Sex'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の対象とする特徴量を列挙する\n",
    "all_features = df_all.columns.tolist()\n",
    "all_features.remove(CFG.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all[:len(df_train)]\n",
    "df_test = df_all[len(df_train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thank you very much https://www.kaggle.com/mburakergenc/ttianic-minimal-pytorch-mlp\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # 後続のCrossEntropyLossでSoftMaxを掛けるので、ここでは掛けない\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO GPUで動くようにする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    # 指定した回数、lossが改善されていなければ打ち止めする\n",
    "    def __init__(self, patience=20):\n",
    "        self.partince = patience\n",
    "        self.bef_epoch = 0\n",
    "        self.min_loss = float('inf')\n",
    "\n",
    "    def step(self, epoch, loss):\n",
    "        if self.min_loss > loss:\n",
    "            self.min_loss = loss\n",
    "            self.bef_epoch = epoch\n",
    "        if epoch - self.bef_epoch > self.partince:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "early_stopping = EarlyStopping(CFG.early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 0 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.240070). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.2400695095943304\n",
      "Validation loss decreased (3.240070 ===> 1.468640). Saving the model...\n",
      "Validation loss decreased (1.468640 ===> 0.945912). Saving the model...\n",
      "Validation loss decreased (0.945912 ===> 0.921435). Saving the model...\n",
      "Validation loss decreased (0.921435 ===> 0.848913). Saving the model...\n",
      "Validation loss decreased (0.848913 ===> 0.629560). Saving the model...\n",
      "Validation loss decreased (0.629560 ===> 0.592776). Saving the model...\n",
      "Validation loss decreased (0.592776 ===> 0.560798). Saving the model...\n",
      "Validation loss decreased (0.560798 ===> 0.547668). Saving the model...\n",
      "Validation loss decreased (0.547668 ===> 0.541616). Saving the model...\n",
      "Validation loss decreased (0.541616 ===> 0.531542). Saving the model...\n",
      "Validation loss decreased (0.531542 ===> 0.513152). Saving the model...\n",
      "Validation loss decreased (0.513152 ===> 0.504513). Saving the model...\n",
      "Validation loss decreased (0.504513 ===> 0.494892). Saving the model...\n",
      "Validation loss decreased (0.494892 ===> 0.492533). Saving the model...\n",
      "Validation loss decreased (0.492533 ===> 0.467718). Saving the model...\n",
      "Epoch 00035: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.467718 ===> 0.462180). Saving the model...\n",
      "Validation loss decreased (0.462180 ===> 0.459976). Saving the model...\n",
      "Validation loss decreased (0.459976 ===> 0.456119). Saving the model...\n",
      "Validation loss decreased (0.456119 ===> 0.444561). Saving the model...\n",
      "Epoch 00059: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.444561 ===> 0.443209). Saving the model...\n",
      "Validation loss decreased (0.443209 ===> 0.443109). Saving the model...\n",
      "Validation loss decreased (0.443109 ===> 0.435634). Saving the model...\n",
      "Validation loss decreased (0.435634 ===> 0.430824). Saving the model...\n",
      "Validation loss decreased (0.430824 ===> 0.428888). Saving the model...\n",
      "Validation loss decreased (0.428888 ===> 0.425805). Saving the model...\n",
      "Validation loss decreased (0.425805 ===> 0.418370). Saving the model...\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.418370 ===> 0.413757). Saving the model...\n",
      "Validation loss decreased (0.413757 ===> 0.413644). Saving the model...\n",
      "Validation loss decreased (0.413644 ===> 0.408654). Saving the model...\n",
      "Epoch 00119: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.408654 ===> 0.408574). Saving the model...\n",
      "Validation loss decreased (0.408574 ===> 0.405771). Saving the model...\n",
      "Validation loss decreased (0.405771 ===> 0.404897). Saving the model...\n",
      "Validation loss decreased (0.404897 ===> 0.401628). Saving the model...\n",
      "Validation loss decreased (0.401628 ===> 0.397798). Saving the model...\n",
      "Validation loss decreased (0.397798 ===> 0.393961). Saving the model...\n",
      "Validation loss decreased (0.393961 ===> 0.391106). Saving the model...\n",
      "Epoch 00159: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.391106 ===> 0.389618). Saving the model...\n",
      "Validation loss decreased (0.389618 ===> 0.386432). Saving the model...\n",
      "Validation loss decreased (0.386432 ===> 0.386121). Saving the model...\n",
      "Validation loss decreased (0.386121 ===> 0.380348). Saving the model...\n",
      "Validation loss decreased (0.380348 ===> 0.378914). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.387512324065847\n",
      "Epoch 00206: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00217: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00228: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00239: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Validation loss decreased (0.378914 ===> 0.377321). Saving the model...\n",
      "Epoch 00256: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Validation loss decreased (0.377321 ===> 0.376588). Saving the model...\n",
      "Epoch 00269: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00280: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00291: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00302: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00313: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00324: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00335: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00346: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00357: reducing learning rate of group 0 to 1.9073e-07.\n",
      "STOP 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\4284312850.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[valid_index, CFG.pred] = pred.data.numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 AUC: 0.894025974025974\n",
      "----- 1 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 2.618724). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 2.618724299785205\n",
      "Validation loss decreased (2.618724 ===> 1.191547). Saving the model...\n",
      "Validation loss decreased (1.191547 ===> 0.899788). Saving the model...\n",
      "Validation loss decreased (0.899788 ===> 0.798518). Saving the model...\n",
      "Validation loss decreased (0.798518 ===> 0.575926). Saving the model...\n",
      "Validation loss decreased (0.575926 ===> 0.547625). Saving the model...\n",
      "Validation loss decreased (0.547625 ===> 0.516275). Saving the model...\n",
      "Validation loss decreased (0.516275 ===> 0.502228). Saving the model...\n",
      "Validation loss decreased (0.502228 ===> 0.502159). Saving the model...\n",
      "Validation loss decreased (0.502159 ===> 0.480927). Saving the model...\n",
      "Validation loss decreased (0.480927 ===> 0.479911). Saving the model...\n",
      "Validation loss decreased (0.479911 ===> 0.476300). Saving the model...\n",
      "Validation loss decreased (0.476300 ===> 0.468949). Saving the model...\n",
      "Validation loss decreased (0.468949 ===> 0.457581). Saving the model...\n",
      "Epoch 00035: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.457581 ===> 0.445101). Saving the model...\n",
      "Validation loss decreased (0.445101 ===> 0.444068). Saving the model...\n",
      "Validation loss decreased (0.444068 ===> 0.440728). Saving the model...\n",
      "Validation loss decreased (0.440728 ===> 0.434057). Saving the model...\n",
      "Validation loss decreased (0.434057 ===> 0.430469). Saving the model...\n",
      "Epoch 00058: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.430469 ===> 0.426561). Saving the model...\n",
      "Validation loss decreased (0.426561 ===> 0.423104). Saving the model...\n",
      "Validation loss decreased (0.423104 ===> 0.417019). Saving the model...\n",
      "Validation loss decreased (0.417019 ===> 0.411928). Saving the model...\n",
      "Validation loss decreased (0.411928 ===> 0.407672). Saving the model...\n",
      "Validation loss decreased (0.407672 ===> 0.405355). Saving the model...\n",
      "Validation loss decreased (0.405355 ===> 0.404353). Saving the model...\n",
      "Validation loss decreased (0.404353 ===> 0.402837). Saving the model...\n",
      "Validation loss decreased (0.402837 ===> 0.395832). Saving the model...\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.395832 ===> 0.394659). Saving the model...\n",
      "Validation loss decreased (0.394659 ===> 0.388884). Saving the model...\n",
      "Validation loss decreased (0.388884 ===> 0.386062). Saving the model...\n",
      "Validation loss decreased (0.386062 ===> 0.378237). Saving the model...\n",
      "Epoch 00130: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.378237 ===> 0.372637). Saving the model...\n",
      "Validation loss decreased (0.372637 ===> 0.368325). Saving the model...\n",
      "Epoch 00153: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.368325 ===> 0.365622). Saving the model...\n",
      "Validation loss decreased (0.365622 ===> 0.359317). Saving the model...\n",
      "Epoch 00171: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00182: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00193: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Validation loss decreased (0.359317 ===> 0.347996). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.36688569345846106\n",
      "Epoch 00206: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00217: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00228: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00239: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00250: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00261: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00272: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00283: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00294: reducing learning rate of group 0 to 7.6294e-07.\n",
      "STOP 1\n",
      "fold 1 AUC: 0.8508021390374332\n",
      "----- 2 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.325364). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.325363787035098\n",
      "Validation loss decreased (3.325364 ===> 1.588877). Saving the model...\n",
      "Validation loss decreased (1.588877 ===> 0.884087). Saving the model...\n",
      "Validation loss decreased (0.884087 ===> 0.698768). Saving the model...\n",
      "Validation loss decreased (0.698768 ===> 0.568295). Saving the model...\n",
      "Validation loss decreased (0.568295 ===> 0.562545). Saving the model...\n",
      "Validation loss decreased (0.562545 ===> 0.545148). Saving the model...\n",
      "Validation loss decreased (0.545148 ===> 0.531620). Saving the model...\n",
      "Validation loss decreased (0.531620 ===> 0.516012). Saving the model...\n",
      "Validation loss decreased (0.516012 ===> 0.513981). Saving the model...\n",
      "Validation loss decreased (0.513981 ===> 0.493424). Saving the model...\n",
      "Epoch 00028: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.493424 ===> 0.477589). Saving the model...\n",
      "Validation loss decreased (0.477589 ===> 0.463084). Saving the model...\n",
      "Validation loss decreased (0.463084 ===> 0.453568). Saving the model...\n",
      "Validation loss decreased (0.453568 ===> 0.450137). Saving the model...\n",
      "Validation loss decreased (0.450137 ===> 0.438704). Saving the model...\n",
      "Validation loss decreased (0.438704 ===> 0.435157). Saving the model...\n",
      "Validation loss decreased (0.435157 ===> 0.428755). Saving the model...\n",
      "Validation loss decreased (0.428755 ===> 0.427921). Saving the model...\n",
      "Epoch 00055: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.427921 ===> 0.423017). Saving the model...\n",
      "Validation loss decreased (0.423017 ===> 0.418572). Saving the model...\n",
      "Validation loss decreased (0.418572 ===> 0.415374). Saving the model...\n",
      "Validation loss decreased (0.415374 ===> 0.414658). Saving the model...\n",
      "Validation loss decreased (0.414658 ===> 0.414375). Saving the model...\n",
      "Validation loss decreased (0.414375 ===> 0.407489). Saving the model...\n",
      "Validation loss decreased (0.407489 ===> 0.403380). Saving the model...\n",
      "Epoch 00083: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.403380 ===> 0.387465). Saving the model...\n",
      "Epoch 00098: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.387465 ===> 0.386856). Saving the model...\n",
      "Validation loss decreased (0.386856 ===> 0.380813). Saving the model...\n",
      "Epoch 00122: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.380813 ===> 0.372390). Saving the model...\n",
      "Epoch 00143: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.372390 ===> 0.371383). Saving the model...\n",
      "Epoch 00158: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00169: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Validation loss decreased (0.371383 ===> 0.370341). Saving the model...\n",
      "Validation loss decreased (0.370341 ===> 0.368867). Saving the model...\n",
      "Epoch 00191: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Validation loss decreased (0.368867 ===> 0.359691). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.3635230970418495\n",
      "Epoch 00207: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00218: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00229: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00240: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00251: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00262: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00273: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00284: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00295: reducing learning rate of group 0 to 3.8147e-07.\n",
      "STOP 2\n",
      "fold 2 AUC: 0.8203208556149733\n",
      "----- 3 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.630368). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.6303678545868605\n",
      "Validation loss decreased (3.630368 ===> 1.574437). Saving the model...\n",
      "Validation loss decreased (1.574437 ===> 0.946391). Saving the model...\n",
      "Validation loss decreased (0.946391 ===> 0.695798). Saving the model...\n",
      "Validation loss decreased (0.695798 ===> 0.545710). Saving the model...\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.545710 ===> 0.523888). Saving the model...\n",
      "Validation loss decreased (0.523888 ===> 0.495836). Saving the model...\n",
      "Validation loss decreased (0.495836 ===> 0.489958). Saving the model...\n",
      "Validation loss decreased (0.489958 ===> 0.470952). Saving the model...\n",
      "Validation loss decreased (0.470952 ===> 0.456349). Saving the model...\n",
      "Epoch 00042: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.456349 ===> 0.446505). Saving the model...\n",
      "Validation loss decreased (0.446505 ===> 0.444371). Saving the model...\n",
      "Validation loss decreased (0.444371 ===> 0.439729). Saving the model...\n",
      "Validation loss decreased (0.439729 ===> 0.435251). Saving the model...\n",
      "Validation loss decreased (0.435251 ===> 0.420293). Saving the model...\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.420293 ===> 0.411653). Saving the model...\n",
      "Validation loss decreased (0.411653 ===> 0.410786). Saving the model...\n",
      "Validation loss decreased (0.410786 ===> 0.397266). Saving the model...\n",
      "Epoch 00088: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.397266 ===> 0.396862). Saving the model...\n",
      "Validation loss decreased (0.396862 ===> 0.393444). Saving the model...\n",
      "Epoch 00117: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00139: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Validation loss decreased (0.393444 ===> 0.390538). Saving the model...\n",
      "Validation loss decreased (0.390538 ===> 0.389999). Saving the model...\n",
      "Epoch 00168: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Validation loss decreased (0.389999 ===> 0.383495). Saving the model...\n",
      "Validation loss decreased (0.383495 ===> 0.383061). Saving the model...\n",
      "Epoch 00183: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00194: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Validation loss decreased (0.383061 ===> 0.382352). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.4057318973394366\n",
      "Epoch 00211: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00222: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00233: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00244: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00255: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00266: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00277: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00288: reducing learning rate of group 0 to 3.8147e-07.\n",
      "STOP 3\n",
      "fold 3 AUC: 0.8786096256684491\n",
      "----- 4 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.834553). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.8345528885610682\n",
      "Validation loss decreased (3.834553 ===> 1.358168). Saving the model...\n",
      "Validation loss decreased (1.358168 ===> 0.955782). Saving the model...\n",
      "Validation loss decreased (0.955782 ===> 0.634818). Saving the model...\n",
      "Validation loss decreased (0.634818 ===> 0.554151). Saving the model...\n",
      "Validation loss decreased (0.554151 ===> 0.531371). Saving the model...\n",
      "Validation loss decreased (0.531371 ===> 0.525648). Saving the model...\n",
      "Validation loss decreased (0.525648 ===> 0.510054). Saving the model...\n",
      "Validation loss decreased (0.510054 ===> 0.504759). Saving the model...\n",
      "Validation loss decreased (0.504759 ===> 0.491644). Saving the model...\n",
      "Validation loss decreased (0.491644 ===> 0.484181). Saving the model...\n",
      "Epoch 00037: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.484181 ===> 0.444608). Saving the model...\n",
      "Validation loss decreased (0.444608 ===> 0.444533). Saving the model...\n",
      "Validation loss decreased (0.444533 ===> 0.436281). Saving the model...\n",
      "Validation loss decreased (0.436281 ===> 0.426966). Saving the model...\n",
      "Validation loss decreased (0.426966 ===> 0.422739). Saving the model...\n",
      "Validation loss decreased (0.422739 ===> 0.420346). Saving the model...\n",
      "Validation loss decreased (0.420346 ===> 0.417524). Saving the model...\n",
      "Validation loss decreased (0.417524 ===> 0.417441). Saving the model...\n",
      "Validation loss decreased (0.417441 ===> 0.404960). Saving the model...\n",
      "Epoch 00073: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.404960 ===> 0.396650). Saving the model...\n",
      "Validation loss decreased (0.396650 ===> 0.385153). Saving the model...\n",
      "Validation loss decreased (0.385153 ===> 0.378070). Saving the model...\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.378070 ===> 0.374345). Saving the model...\n",
      "Validation loss decreased (0.374345 ===> 0.358807). Saving the model...\n",
      "Epoch 00109: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.358807 ===> 0.346496). Saving the model...\n",
      "Validation loss decreased (0.346496 ===> 0.345368). Saving the model...\n",
      "Epoch 00137: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00148: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00159: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Validation loss decreased (0.345368 ===> 0.344444). Saving the model...\n",
      "Validation loss decreased (0.344444 ===> 0.344172). Saving the model...\n",
      "Validation loss decreased (0.344172 ===> 0.336848). Saving the model...\n",
      "Epoch 00190: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00201: reducing learning rate of group 0 to 1.9531e-04.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.34957643513781317\n",
      "Validation loss decreased (0.336848 ===> 0.331809). Saving the model...\n",
      "Epoch 00223: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00234: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00245: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00256: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00267: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00278: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00289: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00300: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00311: reducing learning rate of group 0 to 3.8147e-07.\n",
      "STOP 4\n",
      "fold 4 AUC: 0.766042780748663\n",
      "----- 5 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.082328). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.082328377816445\n",
      "Validation loss decreased (3.082328 ===> 1.288508). Saving the model...\n",
      "Validation loss decreased (1.288508 ===> 0.824223). Saving the model...\n",
      "Validation loss decreased (0.824223 ===> 0.784100). Saving the model...\n",
      "Validation loss decreased (0.784100 ===> 0.644430). Saving the model...\n",
      "Validation loss decreased (0.644430 ===> 0.562347). Saving the model...\n",
      "Validation loss decreased (0.562347 ===> 0.558489). Saving the model...\n",
      "Validation loss decreased (0.558489 ===> 0.554999). Saving the model...\n",
      "Validation loss decreased (0.554999 ===> 0.548772). Saving the model...\n",
      "Validation loss decreased (0.548772 ===> 0.530167). Saving the model...\n",
      "Validation loss decreased (0.530167 ===> 0.517948). Saving the model...\n",
      "Validation loss decreased (0.517948 ===> 0.516022). Saving the model...\n",
      "Validation loss decreased (0.516022 ===> 0.512234). Saving the model...\n",
      "Validation loss decreased (0.512234 ===> 0.507071). Saving the model...\n",
      "Validation loss decreased (0.507071 ===> 0.494852). Saving the model...\n",
      "Validation loss decreased (0.494852 ===> 0.480950). Saving the model...\n",
      "Epoch 00042: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.480950 ===> 0.458375). Saving the model...\n",
      "Validation loss decreased (0.458375 ===> 0.456521). Saving the model...\n",
      "Validation loss decreased (0.456521 ===> 0.453177). Saving the model...\n",
      "Validation loss decreased (0.453177 ===> 0.445689). Saving the model...\n",
      "Validation loss decreased (0.445689 ===> 0.445418). Saving the model...\n",
      "Validation loss decreased (0.445418 ===> 0.440963). Saving the model...\n",
      "Validation loss decreased (0.440963 ===> 0.430721). Saving the model...\n",
      "Epoch 00070: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.430721 ===> 0.421765). Saving the model...\n",
      "Validation loss decreased (0.421765 ===> 0.410848). Saving the model...\n",
      "Validation loss decreased (0.410848 ===> 0.410682). Saving the model...\n",
      "Validation loss decreased (0.410682 ===> 0.402218). Saving the model...\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.402218 ===> 0.395804). Saving the model...\n",
      "Validation loss decreased (0.395804 ===> 0.385697). Saving the model...\n",
      "Epoch 00121: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.385697 ===> 0.385584). Saving the model...\n",
      "Validation loss decreased (0.385584 ===> 0.385362). Saving the model...\n",
      "Validation loss decreased (0.385362 ===> 0.377086). Saving the model...\n",
      "Validation loss decreased (0.377086 ===> 0.371034). Saving the model...\n",
      "Epoch 00152: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00163: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00174: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00185: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00196: reducing learning rate of group 0 to 1.9531e-04.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.37779928560666426\n",
      "Epoch 00207: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Validation loss decreased (0.371034 ===> 0.370505). Saving the model...\n",
      "Validation loss decreased (0.370505 ===> 0.368050). Saving the model...\n",
      "Validation loss decreased (0.368050 ===> 0.365468). Saving the model...\n",
      "Epoch 00230: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00241: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00252: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00263: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Validation loss decreased (0.365468 ===> 0.359690). Saving the model...\n",
      "Epoch 00281: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00292: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00303: reducing learning rate of group 0 to 7.6294e-07.\n",
      "STOP 5\n",
      "fold 5 AUC: 0.8636363636363636\n",
      "----- 6 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 2.905414). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 2.9054136870805163\n",
      "Validation loss decreased (2.905414 ===> 1.677320). Saving the model...\n",
      "Validation loss decreased (1.677320 ===> 0.969616). Saving the model...\n",
      "Validation loss decreased (0.969616 ===> 0.887783). Saving the model...\n",
      "Validation loss decreased (0.887783 ===> 0.720846). Saving the model...\n",
      "Validation loss decreased (0.720846 ===> 0.674657). Saving the model...\n",
      "Validation loss decreased (0.674657 ===> 0.569564). Saving the model...\n",
      "Validation loss decreased (0.569564 ===> 0.568889). Saving the model...\n",
      "Validation loss decreased (0.568889 ===> 0.556465). Saving the model...\n",
      "Validation loss decreased (0.556465 ===> 0.540512). Saving the model...\n",
      "Validation loss decreased (0.540512 ===> 0.537942). Saving the model...\n",
      "Validation loss decreased (0.537942 ===> 0.521716). Saving the model...\n",
      "Validation loss decreased (0.521716 ===> 0.507055). Saving the model...\n",
      "Validation loss decreased (0.507055 ===> 0.497229). Saving the model...\n",
      "Validation loss decreased (0.497229 ===> 0.496395). Saving the model...\n",
      "Epoch 00030: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.496395 ===> 0.495496). Saving the model...\n",
      "Validation loss decreased (0.495496 ===> 0.485158). Saving the model...\n",
      "Validation loss decreased (0.485158 ===> 0.479699). Saving the model...\n",
      "Validation loss decreased (0.479699 ===> 0.477959). Saving the model...\n",
      "Validation loss decreased (0.477959 ===> 0.472906). Saving the model...\n",
      "Validation loss decreased (0.472906 ===> 0.442593). Saving the model...\n",
      "Epoch 00049: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.442593 ===> 0.440300). Saving the model...\n",
      "Validation loss decreased (0.440300 ===> 0.439052). Saving the model...\n",
      "Validation loss decreased (0.439052 ===> 0.423367). Saving the model...\n",
      "Validation loss decreased (0.423367 ===> 0.422358). Saving the model...\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.422358 ===> 0.405852). Saving the model...\n",
      "Validation loss decreased (0.405852 ===> 0.404196). Saving the model...\n",
      "Validation loss decreased (0.404196 ===> 0.396054). Saving the model...\n",
      "Epoch 00105: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.396054 ===> 0.396023). Saving the model...\n",
      "Validation loss decreased (0.396023 ===> 0.395111). Saving the model...\n",
      "Validation loss decreased (0.395111 ===> 0.395073). Saving the model...\n",
      "Validation loss decreased (0.395073 ===> 0.394423). Saving the model...\n",
      "Validation loss decreased (0.394423 ===> 0.389487). Saving the model...\n",
      "Validation loss decreased (0.389487 ===> 0.385169). Saving the model...\n",
      "Validation loss decreased (0.385169 ===> 0.384384). Saving the model...\n",
      "Validation loss decreased (0.384384 ===> 0.380010). Saving the model...\n",
      "Validation loss decreased (0.380010 ===> 0.378138). Saving the model...\n",
      "Validation loss decreased (0.378138 ===> 0.376923). Saving the model...\n",
      "Validation loss decreased (0.376923 ===> 0.375635). Saving the model...\n",
      "Validation loss decreased (0.375635 ===> 0.373618). Saving the model...\n",
      "Epoch 00171: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.373618 ===> 0.362923). Saving the model...\n",
      "Epoch 00185: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.362923 ===> 0.360566). Saving the model...\n",
      "Validation loss decreased (0.360566 ===> 0.356468). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.369320399306861\n",
      "Validation loss decreased (0.356468 ===> 0.355887). Saving the model...\n",
      "Validation loss decreased (0.355887 ===> 0.353451). Saving the model...\n",
      "Validation loss decreased (0.353451 ===> 0.351104). Saving the model...\n",
      "Validation loss decreased (0.351104 ===> 0.349465). Saving the model...\n",
      "Validation loss decreased (0.349465 ===> 0.346272). Saving the model...\n",
      "Epoch 00242: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Validation loss decreased (0.346272 ===> 0.336917). Saving the model...\n",
      "Epoch 00264: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00275: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00286: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00297: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00308: reducing learning rate of group 0 to 2.4414e-05.\n",
      "STOP 6\n",
      "fold 6 AUC: 0.8877005347593583\n",
      "----- 7 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.219903). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.2199029090101283\n",
      "Validation loss decreased (3.219903 ===> 1.582523). Saving the model...\n",
      "Validation loss decreased (1.582523 ===> 0.952019). Saving the model...\n",
      "Validation loss decreased (0.952019 ===> 0.713688). Saving the model...\n",
      "Validation loss decreased (0.713688 ===> 0.674100). Saving the model...\n",
      "Validation loss decreased (0.674100 ===> 0.670698). Saving the model...\n",
      "Validation loss decreased (0.670698 ===> 0.607770). Saving the model...\n",
      "Validation loss decreased (0.607770 ===> 0.587847). Saving the model...\n",
      "Validation loss decreased (0.587847 ===> 0.499308). Saving the model...\n",
      "Validation loss decreased (0.499308 ===> 0.497380). Saving the model...\n",
      "Validation loss decreased (0.497380 ===> 0.495912). Saving the model...\n",
      "Validation loss decreased (0.495912 ===> 0.483915). Saving the model...\n",
      "Validation loss decreased (0.483915 ===> 0.475206). Saving the model...\n",
      "Epoch 00035: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.475206 ===> 0.464407). Saving the model...\n",
      "Validation loss decreased (0.464407 ===> 0.459542). Saving the model...\n",
      "Validation loss decreased (0.459542 ===> 0.452059). Saving the model...\n",
      "Validation loss decreased (0.452059 ===> 0.447959). Saving the model...\n",
      "Validation loss decreased (0.447959 ===> 0.446413). Saving the model...\n",
      "Validation loss decreased (0.446413 ===> 0.444876). Saving the model...\n",
      "Validation loss decreased (0.444876 ===> 0.438292). Saving the model...\n",
      "Validation loss decreased (0.438292 ===> 0.429045). Saving the model...\n",
      "Validation loss decreased (0.429045 ===> 0.424278). Saving the model...\n",
      "Epoch 00068: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.424278 ===> 0.409406). Saving the model...\n",
      "Validation loss decreased (0.409406 ===> 0.407473). Saving the model...\n",
      "Validation loss decreased (0.407473 ===> 0.405166). Saving the model...\n",
      "Validation loss decreased (0.405166 ===> 0.398387). Saving the model...\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.398387 ===> 0.398134). Saving the model...\n",
      "Validation loss decreased (0.398134 ===> 0.384295). Saving the model...\n",
      "Epoch 00119: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.384295 ===> 0.379588). Saving the model...\n",
      "Validation loss decreased (0.379588 ===> 0.379500). Saving the model...\n",
      "Validation loss decreased (0.379500 ===> 0.376937). Saving the model...\n",
      "Validation loss decreased (0.376937 ===> 0.374917). Saving the model...\n",
      "Validation loss decreased (0.374917 ===> 0.372595). Saving the model...\n",
      "Epoch 00146: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.372595 ===> 0.366757). Saving the model...\n",
      "Validation loss decreased (0.366757 ===> 0.364733). Saving the model...\n",
      "Epoch 00176: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.364733 ===> 0.358728). Saving the model...\n",
      "Epoch 00190: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00201: reducing learning rate of group 0 to 3.9063e-04.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.3767537474740488\n",
      "Epoch 00212: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00223: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00234: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00245: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00256: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00267: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Validation loss decreased (0.358728 ===> 0.356947). Saving the model...\n",
      "Epoch 00282: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00293: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00304: reducing learning rate of group 0 to 7.6294e-07.\n",
      "STOP 7\n",
      "fold 7 AUC: 0.8245989304812835\n",
      "----- 8 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 2.544991). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 2.544991186432113\n",
      "Validation loss decreased (2.544991 ===> 1.142651). Saving the model...\n",
      "Validation loss decreased (1.142651 ===> 0.733382). Saving the model...\n",
      "Validation loss decreased (0.733382 ===> 0.622247). Saving the model...\n",
      "Validation loss decreased (0.622247 ===> 0.607016). Saving the model...\n",
      "Validation loss decreased (0.607016 ===> 0.555549). Saving the model...\n",
      "Validation loss decreased (0.555549 ===> 0.537460). Saving the model...\n",
      "Validation loss decreased (0.537460 ===> 0.528512). Saving the model...\n",
      "Validation loss decreased (0.528512 ===> 0.509831). Saving the model...\n",
      "Validation loss decreased (0.509831 ===> 0.486514). Saving the model...\n",
      "Epoch 00025: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.486514 ===> 0.472050). Saving the model...\n",
      "Validation loss decreased (0.472050 ===> 0.469820). Saving the model...\n",
      "Validation loss decreased (0.469820 ===> 0.460434). Saving the model...\n",
      "Validation loss decreased (0.460434 ===> 0.444549). Saving the model...\n",
      "Validation loss decreased (0.444549 ===> 0.442581). Saving the model...\n",
      "Validation loss decreased (0.442581 ===> 0.442532). Saving the model...\n",
      "Validation loss decreased (0.442532 ===> 0.438402). Saving the model...\n",
      "Validation loss decreased (0.438402 ===> 0.434995). Saving the model...\n",
      "Epoch 00069: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.434995 ===> 0.423099). Saving the model...\n",
      "Validation loss decreased (0.423099 ===> 0.416141). Saving the model...\n",
      "Validation loss decreased (0.416141 ===> 0.414123). Saving the model...\n",
      "Validation loss decreased (0.414123 ===> 0.404449). Saving the model...\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.404449 ===> 0.401789). Saving the model...\n",
      "Epoch 00109: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.401789 ===> 0.395771). Saving the model...\n",
      "Validation loss decreased (0.395771 ===> 0.392855). Saving the model...\n",
      "Epoch 00133: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.392855 ===> 0.384443). Saving the model...\n",
      "Epoch 00145: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.384443 ===> 0.378990). Saving the model...\n",
      "Epoch 00159: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00170: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Validation loss decreased (0.378990 ===> 0.376930). Saving the model...\n",
      "Validation loss decreased (0.376930 ===> 0.370368). Saving the model...\n",
      "Epoch 00188: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00199: reducing learning rate of group 0 to 9.7656e-05.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.39075714048122084\n",
      "Epoch 00210: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Validation loss decreased (0.370368 ===> 0.368386). Saving the model...\n",
      "Epoch 00223: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00234: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00245: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00256: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00267: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Validation loss decreased (0.368386 ===> 0.366428). Saving the model...\n",
      "Epoch 00287: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00298: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00309: reducing learning rate of group 0 to 1.9073e-07.\n",
      "STOP 8\n",
      "fold 8 AUC: 0.9058823529411765\n",
      "----- 9 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.065284). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.0652844293456423\n",
      "Validation loss decreased (3.065284 ===> 1.305171). Saving the model...\n",
      "Validation loss decreased (1.305171 ===> 0.843259). Saving the model...\n",
      "Validation loss decreased (0.843259 ===> 0.673380). Saving the model...\n",
      "Validation loss decreased (0.673380 ===> 0.616218). Saving the model...\n",
      "Validation loss decreased (0.616218 ===> 0.552969). Saving the model...\n",
      "Validation loss decreased (0.552969 ===> 0.550833). Saving the model...\n",
      "Validation loss decreased (0.550833 ===> 0.534792). Saving the model...\n",
      "Validation loss decreased (0.534792 ===> 0.529986). Saving the model...\n",
      "Validation loss decreased (0.529986 ===> 0.510074). Saving the model...\n",
      "Validation loss decreased (0.510074 ===> 0.508432). Saving the model...\n",
      "Validation loss decreased (0.508432 ===> 0.497491). Saving the model...\n",
      "Validation loss decreased (0.497491 ===> 0.495342). Saving the model...\n",
      "Validation loss decreased (0.495342 ===> 0.493326). Saving the model...\n",
      "Validation loss decreased (0.493326 ===> 0.484678). Saving the model...\n",
      "Validation loss decreased (0.484678 ===> 0.476173). Saving the model...\n",
      "Validation loss decreased (0.476173 ===> 0.472941). Saving the model...\n",
      "Validation loss decreased (0.472941 ===> 0.468018). Saving the model...\n",
      "Validation loss decreased (0.468018 ===> 0.467406). Saving the model...\n",
      "Validation loss decreased (0.467406 ===> 0.453082). Saving the model...\n",
      "Epoch 00066: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.453082 ===> 0.444438). Saving the model...\n",
      "Validation loss decreased (0.444438 ===> 0.438767). Saving the model...\n",
      "Validation loss decreased (0.438767 ===> 0.430764). Saving the model...\n",
      "Validation loss decreased (0.430764 ===> 0.407707). Saving the model...\n",
      "Epoch 00086: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.407707 ===> 0.407659). Saving the model...\n",
      "Validation loss decreased (0.407659 ===> 0.406950). Saving the model...\n",
      "Validation loss decreased (0.406950 ===> 0.402128). Saving the model...\n",
      "Validation loss decreased (0.402128 ===> 0.400931). Saving the model...\n",
      "Validation loss decreased (0.400931 ===> 0.399511). Saving the model...\n",
      "Validation loss decreased (0.399511 ===> 0.397778). Saving the model...\n",
      "Epoch 00131: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.397778 ===> 0.387018). Saving the model...\n",
      "Epoch 00143: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.387018 ===> 0.379658). Saving the model...\n",
      "Epoch 00160: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00171: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.379658 ===> 0.371602). Saving the model...\n",
      "Epoch 00186: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Validation loss decreased (0.371602 ===> 0.367976). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.40068403302779937\n",
      "Epoch 00205: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00216: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Validation loss decreased (0.367976 ===> 0.363294). Saving the model...\n",
      "Validation loss decreased (0.363294 ===> 0.363080). Saving the model...\n",
      "Epoch 00238: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00249: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00260: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00271: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00282: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00293: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00304: reducing learning rate of group 0 to 1.5259e-06.\n",
      "STOP 9\n",
      "fold 9 AUC: 0.8206349206349207\n",
      "AUC 0.8520355990157544\n",
      "Training Ended! \n"
     ]
    }
   ],
   "source": [
    "#thank you very much https://www.kaggle.com/mburakergenc/ttianic-minimal-pytorch-mlp\n",
    "\n",
    "batch_size = CFG.batch_size\n",
    "batch_no = len(df_train) // batch_size\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train, df_train[CFG.target])):\n",
    "    print('-----', fold, '-----')\n",
    "    # fold毎に初期化する設定\n",
    "    # model, optimizer, scheduler, その他変数\n",
    "    model = Net(len(all_features), 512, 1)\n",
    "    print(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=CFG.lr_factor)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_loss_min = np.Inf\n",
    "\n",
    "    # データを分割する。fold毎に一回だけやる\n",
    "    X_train = df_train[all_features].iloc[train_index]\n",
    "    y_train = df_train.iloc[train_index][CFG.target]\n",
    "    X_valid = df_train[all_features].iloc[valid_index]\n",
    "    y_valid = df_train.iloc[valid_index][CFG.target]\n",
    "    X_test = df_test[all_features]\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        # TODO Variableを使うのは古いらしい\n",
    "        x0_var = Variable(torch.FloatTensor(X_train.values))\n",
    "        y0_var = Variable(torch.FloatTensor(y_train.values))\n",
    "        for i in range(batch_no):\n",
    "            # ミニバッチ学習\n",
    "            start = i * batch_size\n",
    "            end   = start + batch_size\n",
    "            x_var = x0_var[start:end]\n",
    "            y_var = y0_var[start:end]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_var).squeeze(1)\n",
    "            loss   = criterion(output, y_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()*batch_size\n",
    "\n",
    "        train_loss = train_loss / len(X_train)\n",
    "        if train_loss <= train_loss_min:\n",
    "            print(\"Validation loss decreased ({:6f} ===> {:6f}). Saving the model...\".format(train_loss_min,train_loss))\n",
    "            torch.save(model.state_dict(), CFG.model_path + \"/model\" + str(fold) + \".pt\")\n",
    "            train_loss_min = train_loss\n",
    "\n",
    "        # lrを引き下げる\n",
    "        # EarlyStoppingを使う\n",
    "        scheduler.step(train_loss)\n",
    "        if early_stopping.step(epoch, train_loss):\n",
    "            break\n",
    "\n",
    "        # log\n",
    "        if epoch % 200 == 0:\n",
    "            print('')\n",
    "            print(\"Epoch: {} \\tTrain Loss: {}\".format(epoch+1, train_loss))\n",
    "\n",
    "    print('STOP train fold=', fold)\n",
    "    x0_var = Variable(torch.FloatTensor(X_valid.values))\n",
    "    pred = torch.sigmoid(model(x0_var))\n",
    "    df_train.loc[valid_index, CFG.pred] = pred.data.numpy()\n",
    "    auc = roc_auc_score(df_train.loc[valid_index, CFG.target], df_train.loc[valid_index, CFG.pred])\n",
    "    print('fold', fold, 'AUC:', auc)\n",
    "\n",
    "auc = roc_auc_score(df_train[CFG.target], df_train[CFG.pred])\n",
    "print('-----', 'Training Ended!', '-----')\n",
    "print('AUC', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./models\\\\model0.pt' './models\\\\model1.pt' './models\\\\model2.pt'\n",
      " './models\\\\model3.pt' './models\\\\model4.pt' './models\\\\model5.pt'\n",
      " './models\\\\model6.pt' './models\\\\model7.pt' './models\\\\model8.pt'\n",
      " './models\\\\model9.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_20348\\2882358087.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.numpy()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>pred0</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>pred4</th>\n",
       "      <th>pred5</th>\n",
       "      <th>pred6</th>\n",
       "      <th>pred7</th>\n",
       "      <th>pred8</th>\n",
       "      <th>pred9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089513</td>\n",
       "      <td>0.110243</td>\n",
       "      <td>0.075552</td>\n",
       "      <td>0.071892</td>\n",
       "      <td>0.014930</td>\n",
       "      <td>0.089706</td>\n",
       "      <td>0.083010</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.159559</td>\n",
       "      <td>0.104686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.193991</td>\n",
       "      <td>0.300637</td>\n",
       "      <td>0.102730</td>\n",
       "      <td>0.310662</td>\n",
       "      <td>0.076926</td>\n",
       "      <td>0.161514</td>\n",
       "      <td>0.136091</td>\n",
       "      <td>0.070735</td>\n",
       "      <td>0.232625</td>\n",
       "      <td>0.199361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090698</td>\n",
       "      <td>0.104894</td>\n",
       "      <td>0.057204</td>\n",
       "      <td>0.144901</td>\n",
       "      <td>0.198364</td>\n",
       "      <td>0.022229</td>\n",
       "      <td>0.034553</td>\n",
       "      <td>0.092985</td>\n",
       "      <td>0.206335</td>\n",
       "      <td>0.100807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.160637</td>\n",
       "      <td>0.162516</td>\n",
       "      <td>0.133910</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.113053</td>\n",
       "      <td>0.146929</td>\n",
       "      <td>0.143314</td>\n",
       "      <td>0.065910</td>\n",
       "      <td>0.086159</td>\n",
       "      <td>0.102770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377532</td>\n",
       "      <td>0.354885</td>\n",
       "      <td>0.256344</td>\n",
       "      <td>0.288201</td>\n",
       "      <td>0.352857</td>\n",
       "      <td>0.357131</td>\n",
       "      <td>0.506915</td>\n",
       "      <td>0.254339</td>\n",
       "      <td>0.204786</td>\n",
       "      <td>0.382341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare  male  Q  S     pred0  \\\n",
       "0         0       3  34.5      0      0   7.8292     1  1  0  0.089513   \n",
       "1         0       3  47.0      1      0   7.0000     0  0  1  0.193991   \n",
       "2         0       2  62.0      0      0   9.6875     1  1  0  0.090698   \n",
       "3         0       3  27.0      0      0   8.6625     1  0  1  0.160637   \n",
       "4         0       3  22.0      1      1  12.2875     0  0  1  0.377532   \n",
       "\n",
       "      pred1     pred2     pred3     pred4     pred5     pred6     pred7  \\\n",
       "0  0.110243  0.075552  0.071892  0.014930  0.089706  0.083010  0.008107   \n",
       "1  0.300637  0.102730  0.310662  0.076926  0.161514  0.136091  0.070735   \n",
       "2  0.104894  0.057204  0.144901  0.198364  0.022229  0.034553  0.092985   \n",
       "3  0.162516  0.133910  0.097900  0.113053  0.146929  0.143314  0.065910   \n",
       "4  0.354885  0.256344  0.288201  0.352857  0.357131  0.506915  0.254339   \n",
       "\n",
       "      pred8     pred9  \n",
       "0  0.159559  0.104686  \n",
       "1  0.232625  0.199361  \n",
       "2  0.206335  0.100807  \n",
       "3  0.086159  0.102770  \n",
       "4  0.204786  0.382341  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = np.sort(glob.glob(f\"./{CFG.model_path}/*.pt\"))\n",
    "print(models)\n",
    "# fold別に作った10個のモデルをロードする\n",
    "with torch.no_grad():\n",
    "    for i, model_name in enumerate(models):\n",
    "        model = Net(len(all_features), 512, 1)\n",
    "        model.load_state_dict(torch.load(model_name,))\n",
    "        X_test = df_test[all_features]\n",
    "        x0_var = Variable(torch.FloatTensor(X_test.values))\n",
    "        pred = F.sigmoid(model(x0_var))\n",
    "        df_test[CFG.test_pred[i]] = pred.data.numpy()\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[CFG.target] = df_test[CFG.test_pred].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[CFG.target] = (df_sub[CFG.target] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f8cdbf2a96461788475085dd1e9d6dd7137de331e19aa3c17c37cb4f0963a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yourenvname')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
