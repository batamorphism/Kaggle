{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Takanori\\\\Desktop\\\\Kaggle\\\\titanic\\\\input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet & Library Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/titanic/train.csv')\n",
    "df_test  = pd.read_csv('../input/titanic/test.csv')\n",
    "df_sub   = pd.read_csv('../input/titanic/gender_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    batch_size = 96\n",
    "    epochs = 2000\n",
    "    folds = 10\n",
    "    seed = 42\n",
    "    target = 'Survived'\n",
    "    lr = 0.1\n",
    "    model_path = \"models\"\n",
    "    test_pred = ['pred' + str(i) for i in range(folds)]\n",
    "    pred = 'pred'\n",
    "    early_stopping = 100\n",
    "    lr_factor = 0.5\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CFG.model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要データを削除\n",
    "df_all.drop(['Name','Ticket','Cabin','PassengerId'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\1836397818.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_all.fillna(df_all.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# NA埋め\n",
    "df_all.fillna(df_all.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットを01に変換\n",
    "df_all[CFG.target].fillna(0, inplace=True)\n",
    "df_all[CFG.target] = df_all[CFG.target].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリ変数をone_hot_encoding\n",
    "sex = pd.get_dummies(df_all['Sex'], drop_first=True)\n",
    "embark = pd.get_dummies(df_all['Embarked'], drop_first=True)\n",
    "df_all = pd.concat([df_all, sex, embark], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要になったカテゴリ変数を削除\n",
    "df_all.drop(['Embarked', 'Sex'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の対象とする特徴量を列挙する\n",
    "all_features = df_all.columns.tolist()\n",
    "all_features.remove(CFG.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all[:len(df_train)]\n",
    "df_test = df_all[len(df_train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thank you very much https://www.kaggle.com/mburakergenc/ttianic-minimal-pytorch-mlp\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # 後続のCrossEntropyLossでSoftMaxを掛けるので、ここでは掛けない\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO GPUで動くようにする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    # 指定した回数、lossが改善されていなければ打ち止めする\n",
    "    def __init__(self, patience=20):\n",
    "        self.partince = patience\n",
    "        self.bef_epoch = 0\n",
    "        self.min_loss = float('inf')\n",
    "\n",
    "    def step(self, epoch, loss):\n",
    "        if self.min_loss > loss:\n",
    "            self.min_loss = loss\n",
    "            self.bef_epoch = epoch\n",
    "        if epoch - self.bef_epoch > self.partince:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "early_stopping = EarlyStopping(CFG.early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 0 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 4.925098). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 4.925097833411971\n",
      "Validation loss decreased (4.925098 ===> 1.360217). Saving the model...\n",
      "Validation loss decreased (1.360217 ===> 0.885273). Saving the model...\n",
      "Validation loss decreased (0.885273 ===> 0.820040). Saving the model...\n",
      "Validation loss decreased (0.820040 ===> 0.702759). Saving the model...\n",
      "Validation loss decreased (0.702759 ===> 0.650033). Saving the model...\n",
      "Validation loss decreased (0.650033 ===> 0.611978). Saving the model...\n",
      "Validation loss decreased (0.611978 ===> 0.547680). Saving the model...\n",
      "Validation loss decreased (0.547680 ===> 0.509620). Saving the model...\n",
      "Validation loss decreased (0.509620 ===> 0.501111). Saving the model...\n",
      "Validation loss decreased (0.501111 ===> 0.496826). Saving the model...\n",
      "Validation loss decreased (0.496826 ===> 0.489153). Saving the model...\n",
      "Validation loss decreased (0.489153 ===> 0.481563). Saving the model...\n",
      "Validation loss decreased (0.481563 ===> 0.465344). Saving the model...\n",
      "Validation loss decreased (0.465344 ===> 0.461554). Saving the model...\n",
      "Epoch 00042: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.461554 ===> 0.460148). Saving the model...\n",
      "Validation loss decreased (0.460148 ===> 0.434769). Saving the model...\n",
      "Validation loss decreased (0.434769 ===> 0.434208). Saving the model...\n",
      "Epoch 00057: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.434208 ===> 0.424876). Saving the model...\n",
      "Validation loss decreased (0.424876 ===> 0.415248). Saving the model...\n",
      "Validation loss decreased (0.415248 ===> 0.410650). Saving the model...\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.410650 ===> 0.408205). Saving the model...\n",
      "Validation loss decreased (0.408205 ===> 0.398375). Saving the model...\n",
      "Epoch 00089: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.398375 ===> 0.395462). Saving the model...\n",
      "Validation loss decreased (0.395462 ===> 0.388111). Saving the model...\n",
      "Validation loss decreased (0.388111 ===> 0.381797). Saving the model...\n",
      "Epoch 00117: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00139: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00150: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Validation loss decreased (0.381797 ===> 0.375931). Saving the model...\n",
      "Epoch 00164: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00175: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00186: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00197: reducing learning rate of group 0 to 2.4414e-05.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.3974685052021025\n",
      "Epoch 00208: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00219: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00230: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00241: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00252: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00263: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00274: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00285: reducing learning rate of group 0 to 9.5367e-08.\n",
      "Epoch 00296: reducing learning rate of group 0 to 4.7684e-08.\n",
      "Epoch 00307: reducing learning rate of group 0 to 2.3842e-08.\n",
      "Epoch 00318: reducing learning rate of group 0 to 1.1921e-08.\n",
      "\n",
      "Epoch: 401 \tTrain Loss: 0.38996960497243094\n",
      "STOP train fold= 0\n",
      "fold 0 AUC: 0.8825974025974027\n",
      "----- 1 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.981229). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.981229315969415\n",
      "Validation loss decreased (3.981229 ===> 1.437813). Saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\2170570300.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.loc[valid_index, CFG.pred] = pred.data.cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.437813 ===> 0.984470). Saving the model...\n",
      "Validation loss decreased (0.984470 ===> 0.861342). Saving the model...\n",
      "Validation loss decreased (0.861342 ===> 0.796001). Saving the model...\n",
      "Validation loss decreased (0.796001 ===> 0.623915). Saving the model...\n",
      "Validation loss decreased (0.623915 ===> 0.565800). Saving the model...\n",
      "Validation loss decreased (0.565800 ===> 0.559829). Saving the model...\n",
      "Validation loss decreased (0.559829 ===> 0.516211). Saving the model...\n",
      "Validation loss decreased (0.516211 ===> 0.503796). Saving the model...\n",
      "Validation loss decreased (0.503796 ===> 0.503445). Saving the model...\n",
      "Validation loss decreased (0.503445 ===> 0.488926). Saving the model...\n",
      "Validation loss decreased (0.488926 ===> 0.477426). Saving the model...\n",
      "Validation loss decreased (0.477426 ===> 0.457263). Saving the model...\n",
      "Validation loss decreased (0.457263 ===> 0.446923). Saving the model...\n",
      "Epoch 00041: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.446923 ===> 0.432712). Saving the model...\n",
      "Validation loss decreased (0.432712 ===> 0.423596). Saving the model...\n",
      "Validation loss decreased (0.423596 ===> 0.416169). Saving the model...\n",
      "Validation loss decreased (0.416169 ===> 0.415327). Saving the model...\n",
      "Validation loss decreased (0.415327 ===> 0.413578). Saving the model...\n",
      "Validation loss decreased (0.413578 ===> 0.412907). Saving the model...\n",
      "Validation loss decreased (0.412907 ===> 0.412146). Saving the model...\n",
      "Validation loss decreased (0.412146 ===> 0.409015). Saving the model...\n",
      "Validation loss decreased (0.409015 ===> 0.399967). Saving the model...\n",
      "Epoch 00096: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.399967 ===> 0.391276). Saving the model...\n",
      "Validation loss decreased (0.391276 ===> 0.389461). Saving the model...\n",
      "Validation loss decreased (0.389461 ===> 0.385919). Saving the model...\n",
      "Validation loss decreased (0.385919 ===> 0.382803). Saving the model...\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.382803 ===> 0.381107). Saving the model...\n",
      "Validation loss decreased (0.381107 ===> 0.379880). Saving the model...\n",
      "Validation loss decreased (0.379880 ===> 0.375489). Saving the model...\n",
      "Validation loss decreased (0.375489 ===> 0.375187). Saving the model...\n",
      "Validation loss decreased (0.375187 ===> 0.373430). Saving the model...\n",
      "Validation loss decreased (0.373430 ===> 0.371850). Saving the model...\n",
      "Validation loss decreased (0.371850 ===> 0.371400). Saving the model...\n",
      "Validation loss decreased (0.371400 ===> 0.366785). Saving the model...\n",
      "Validation loss decreased (0.366785 ===> 0.360651). Saving the model...\n",
      "Epoch 00164: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch 00175: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.360651 ===> 0.353650). Saving the model...\n",
      "Validation loss decreased (0.353650 ===> 0.352122). Saving the model...\n",
      "Validation loss decreased (0.352122 ===> 0.348276). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.35710498283234776\n",
      "Validation loss decreased (0.348276 ===> 0.348224). Saving the model...\n",
      "Validation loss decreased (0.348224 ===> 0.347721). Saving the model...\n",
      "Epoch 00223: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.347721 ===> 0.338941). Saving the model...\n",
      "Epoch 00241: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00252: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00263: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00274: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00285: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Validation loss decreased (0.338941 ===> 0.336321). Saving the model...\n",
      "Epoch 00300: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00322: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00333: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00344: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00355: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00366: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00377: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00388: reducing learning rate of group 0 to 9.5367e-08.\n",
      "STOP train fold= 1\n",
      "fold 1 AUC: 0.8652406417112299\n",
      "----- 2 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.820568). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.820567573395156\n",
      "Validation loss decreased (3.820568 ===> 1.724858). Saving the model...\n",
      "Validation loss decreased (1.724858 ===> 0.904564). Saving the model...\n",
      "Validation loss decreased (0.904564 ===> 0.750517). Saving the model...\n",
      "Validation loss decreased (0.750517 ===> 0.651225). Saving the model...\n",
      "Validation loss decreased (0.651225 ===> 0.627565). Saving the model...\n",
      "Validation loss decreased (0.627565 ===> 0.574025). Saving the model...\n",
      "Validation loss decreased (0.574025 ===> 0.566624). Saving the model...\n",
      "Validation loss decreased (0.566624 ===> 0.514164). Saving the model...\n",
      "Validation loss decreased (0.514164 ===> 0.485767). Saving the model...\n",
      "Validation loss decreased (0.485767 ===> 0.481394). Saving the model...\n",
      "Validation loss decreased (0.481394 ===> 0.478054). Saving the model...\n",
      "Validation loss decreased (0.478054 ===> 0.468879). Saving the model...\n",
      "Validation loss decreased (0.468879 ===> 0.461520). Saving the model...\n",
      "Validation loss decreased (0.461520 ===> 0.452387). Saving the model...\n",
      "Epoch 00041: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.452387 ===> 0.445880). Saving the model...\n",
      "Validation loss decreased (0.445880 ===> 0.438969). Saving the model...\n",
      "Validation loss decreased (0.438969 ===> 0.428179). Saving the model...\n",
      "Epoch 00058: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.428179 ===> 0.425986). Saving the model...\n",
      "Validation loss decreased (0.425986 ===> 0.422546). Saving the model...\n",
      "Validation loss decreased (0.422546 ===> 0.412615). Saving the model...\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.412615 ===> 0.412487). Saving the model...\n",
      "Validation loss decreased (0.412487 ===> 0.411260). Saving the model...\n",
      "Validation loss decreased (0.411260 ===> 0.404447). Saving the model...\n",
      "Validation loss decreased (0.404447 ===> 0.396412). Saving the model...\n",
      "Epoch 00122: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch 00133: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.396412 ===> 0.392335). Saving the model...\n",
      "Validation loss decreased (0.392335 ===> 0.391661). Saving the model...\n",
      "Validation loss decreased (0.391661 ===> 0.386142). Saving the model...\n",
      "Epoch 00162: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.386142 ===> 0.381710). Saving the model...\n",
      "Epoch 00175: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00186: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00197: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Validation loss decreased (0.381710 ===> 0.378907). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.3994309420398841\n",
      "Epoch 00209: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00220: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00231: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00242: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00253: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Validation loss decreased (0.378907 ===> 0.378451). Saving the model...\n",
      "Epoch 00270: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00281: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00292: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00303: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00314: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00325: reducing learning rate of group 0 to 9.5367e-08.\n",
      "Epoch 00336: reducing learning rate of group 0 to 4.7684e-08.\n",
      "Epoch 00347: reducing learning rate of group 0 to 2.3842e-08.\n",
      "Epoch 00358: reducing learning rate of group 0 to 1.1921e-08.\n",
      "Validation loss decreased (0.378451 ===> 0.377786). Saving the model...\n",
      "STOP train fold= 2\n",
      "fold 2 AUC: 0.795187165775401\n",
      "----- 3 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 4.308531). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 4.308531311682038\n",
      "Validation loss decreased (4.308531 ===> 1.834409). Saving the model...\n",
      "Validation loss decreased (1.834409 ===> 1.072038). Saving the model...\n",
      "Validation loss decreased (1.072038 ===> 0.658196). Saving the model...\n",
      "Validation loss decreased (0.658196 ===> 0.650869). Saving the model...\n",
      "Validation loss decreased (0.650869 ===> 0.604323). Saving the model...\n",
      "Validation loss decreased (0.604323 ===> 0.525451). Saving the model...\n",
      "Validation loss decreased (0.525451 ===> 0.507143). Saving the model...\n",
      "Validation loss decreased (0.507143 ===> 0.483089). Saving the model...\n",
      "Validation loss decreased (0.483089 ===> 0.482572). Saving the model...\n",
      "Validation loss decreased (0.482572 ===> 0.463724). Saving the model...\n",
      "Validation loss decreased (0.463724 ===> 0.463671). Saving the model...\n",
      "Validation loss decreased (0.463671 ===> 0.450633). Saving the model...\n",
      "Validation loss decreased (0.450633 ===> 0.448698). Saving the model...\n",
      "Validation loss decreased (0.448698 ===> 0.427260). Saving the model...\n",
      "Epoch 00055: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.427260 ===> 0.422283). Saving the model...\n",
      "Validation loss decreased (0.422283 ===> 0.422220). Saving the model...\n",
      "Validation loss decreased (0.422220 ===> 0.422016). Saving the model...\n",
      "Validation loss decreased (0.422016 ===> 0.419735). Saving the model...\n",
      "Validation loss decreased (0.419735 ===> 0.418623). Saving the model...\n",
      "Validation loss decreased (0.418623 ===> 0.418411). Saving the model...\n",
      "Validation loss decreased (0.418411 ===> 0.411097). Saving the model...\n",
      "Validation loss decreased (0.411097 ===> 0.403447). Saving the model...\n",
      "Epoch 00086: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.403447 ===> 0.395008). Saving the model...\n",
      "Validation loss decreased (0.395008 ===> 0.392228). Saving the model...\n",
      "Validation loss decreased (0.392228 ===> 0.389686). Saving the model...\n",
      "Validation loss decreased (0.389686 ===> 0.374461). Saving the model...\n",
      "Validation loss decreased (0.374461 ===> 0.372624). Saving the model...\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.372624 ===> 0.371428). Saving the model...\n",
      "Validation loss decreased (0.371428 ===> 0.370076). Saving the model...\n",
      "Validation loss decreased (0.370076 ===> 0.368371). Saving the model...\n",
      "Validation loss decreased (0.368371 ===> 0.367290). Saving the model...\n",
      "Validation loss decreased (0.367290 ===> 0.364166). Saving the model...\n",
      "Validation loss decreased (0.364166 ===> 0.362808). Saving the model...\n",
      "Validation loss decreased (0.362808 ===> 0.358554). Saving the model...\n",
      "Validation loss decreased (0.358554 ===> 0.357883). Saving the model...\n",
      "Validation loss decreased (0.357883 ===> 0.352961). Saving the model...\n",
      "Epoch 00174: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.352961 ===> 0.348819). Saving the model...\n",
      "Validation loss decreased (0.348819 ===> 0.348630). Saving the model...\n",
      "Validation loss decreased (0.348630 ===> 0.345879). Saving the model...\n",
      "Validation loss decreased (0.345879 ===> 0.342016). Saving the model...\n",
      "Validation loss decreased (0.342016 ===> 0.331293). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.3625057444557028\n",
      "Epoch 00208: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00219: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.331293 ===> 0.330909). Saving the model...\n",
      "Validation loss decreased (0.330909 ===> 0.330061). Saving the model...\n",
      "Validation loss decreased (0.330061 ===> 0.322405). Saving the model...\n",
      "Epoch 00247: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Validation loss decreased (0.322405 ===> 0.319692). Saving the model...\n",
      "Validation loss decreased (0.319692 ===> 0.310898). Saving the model...\n",
      "Epoch 00269: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00280: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00291: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00302: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00313: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00324: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00335: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00346: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00357: reducing learning rate of group 0 to 1.5259e-06.\n",
      "STOP train fold= 3\n",
      "fold 3 AUC: 0.8614973262032085\n",
      "----- 4 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 4.110124). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 4.110124321649794\n",
      "Validation loss decreased (4.110124 ===> 1.698793). Saving the model...\n",
      "Validation loss decreased (1.698793 ===> 1.026491). Saving the model...\n",
      "Validation loss decreased (1.026491 ===> 0.766685). Saving the model...\n",
      "Validation loss decreased (0.766685 ===> 0.724106). Saving the model...\n",
      "Validation loss decreased (0.724106 ===> 0.716196). Saving the model...\n",
      "Validation loss decreased (0.716196 ===> 0.667249). Saving the model...\n",
      "Validation loss decreased (0.667249 ===> 0.553315). Saving the model...\n",
      "Validation loss decreased (0.553315 ===> 0.499187). Saving the model...\n",
      "Validation loss decreased (0.499187 ===> 0.498156). Saving the model...\n",
      "Validation loss decreased (0.498156 ===> 0.493393). Saving the model...\n",
      "Validation loss decreased (0.493393 ===> 0.489703). Saving the model...\n",
      "Validation loss decreased (0.489703 ===> 0.476474). Saving the model...\n",
      "Validation loss decreased (0.476474 ===> 0.447924). Saving the model...\n",
      "Validation loss decreased (0.447924 ===> 0.442385). Saving the model...\n",
      "Validation loss decreased (0.442385 ===> 0.437671). Saving the model...\n",
      "Validation loss decreased (0.437671 ===> 0.427215). Saving the model...\n",
      "Validation loss decreased (0.427215 ===> 0.426299). Saving the model...\n",
      "Epoch 00055: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.426299 ===> 0.422672). Saving the model...\n",
      "Validation loss decreased (0.422672 ===> 0.419153). Saving the model...\n",
      "Validation loss decreased (0.419153 ===> 0.416112). Saving the model...\n",
      "Validation loss decreased (0.416112 ===> 0.411698). Saving the model...\n",
      "Validation loss decreased (0.411698 ===> 0.408683). Saving the model...\n",
      "Validation loss decreased (0.408683 ===> 0.406113). Saving the model...\n",
      "Validation loss decreased (0.406113 ===> 0.401065). Saving the model...\n",
      "Validation loss decreased (0.401065 ===> 0.392178). Saving the model...\n",
      "Epoch 00091: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.392178 ===> 0.389907). Saving the model...\n",
      "Validation loss decreased (0.389907 ===> 0.384579). Saving the model...\n",
      "Validation loss decreased (0.384579 ===> 0.384060). Saving the model...\n",
      "Validation loss decreased (0.384060 ===> 0.382345). Saving the model...\n",
      "Validation loss decreased (0.382345 ===> 0.378486). Saving the model...\n",
      "Epoch 00118: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Epoch 00129: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch 00140: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.378486 ===> 0.373607). Saving the model...\n",
      "Epoch 00161: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00172: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Validation loss decreased (0.373607 ===> 0.372070). Saving the model...\n",
      "Epoch 00185: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Validation loss decreased (0.372070 ===> 0.371510). Saving the model...\n",
      "Epoch 00198: reducing learning rate of group 0 to 1.9531e-04.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.3813476494115883\n",
      "Validation loss decreased (0.371510 ===> 0.370156). Saving the model...\n",
      "Epoch 00218: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00229: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00240: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Validation loss decreased (0.370156 ===> 0.369663). Saving the model...\n",
      "Validation loss decreased (0.369663 ===> 0.367973). Saving the model...\n",
      "Validation loss decreased (0.367973 ===> 0.364000). Saving the model...\n",
      "Epoch 00261: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00272: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00283: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00294: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00305: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00316: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00327: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00338: reducing learning rate of group 0 to 9.5367e-08.\n",
      "Epoch 00349: reducing learning rate of group 0 to 4.7684e-08.\n",
      "STOP train fold= 4\n",
      "fold 4 AUC: 0.7850267379679144\n",
      "----- 5 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.992190). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.992189588094887\n",
      "Validation loss decreased (3.992190 ===> 1.640581). Saving the model...\n",
      "Validation loss decreased (1.640581 ===> 1.110095). Saving the model...\n",
      "Validation loss decreased (1.110095 ===> 0.843299). Saving the model...\n",
      "Validation loss decreased (0.843299 ===> 0.814153). Saving the model...\n",
      "Validation loss decreased (0.814153 ===> 0.801818). Saving the model...\n",
      "Validation loss decreased (0.801818 ===> 0.749815). Saving the model...\n",
      "Validation loss decreased (0.749815 ===> 0.556640). Saving the model...\n",
      "Validation loss decreased (0.556640 ===> 0.536783). Saving the model...\n",
      "Validation loss decreased (0.536783 ===> 0.521202). Saving the model...\n",
      "Validation loss decreased (0.521202 ===> 0.497168). Saving the model...\n",
      "Validation loss decreased (0.497168 ===> 0.483371). Saving the model...\n",
      "Validation loss decreased (0.483371 ===> 0.465921). Saving the model...\n",
      "Validation loss decreased (0.465921 ===> 0.456310). Saving the model...\n",
      "Validation loss decreased (0.456310 ===> 0.452687). Saving the model...\n",
      "Validation loss decreased (0.452687 ===> 0.445298). Saving the model...\n",
      "Validation loss decreased (0.445298 ===> 0.443151). Saving the model...\n",
      "Validation loss decreased (0.443151 ===> 0.433501). Saving the model...\n",
      "Validation loss decreased (0.433501 ===> 0.428293). Saving the model...\n",
      "Validation loss decreased (0.428293 ===> 0.416484). Saving the model...\n",
      "Epoch 00073: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Epoch 00084: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.416484 ===> 0.414867). Saving the model...\n",
      "Validation loss decreased (0.414867 ===> 0.411994). Saving the model...\n",
      "Validation loss decreased (0.411994 ===> 0.407984). Saving the model...\n",
      "Validation loss decreased (0.407984 ===> 0.396683). Saving the model...\n",
      "Validation loss decreased (0.396683 ===> 0.396362). Saving the model...\n",
      "Validation loss decreased (0.396362 ===> 0.395993). Saving the model...\n",
      "Validation loss decreased (0.395993 ===> 0.395316). Saving the model...\n",
      "Validation loss decreased (0.395316 ===> 0.394251). Saving the model...\n",
      "Validation loss decreased (0.394251 ===> 0.391016). Saving the model...\n",
      "Validation loss decreased (0.391016 ===> 0.386583). Saving the model...\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.386583 ===> 0.371334). Saving the model...\n",
      "Epoch 00136: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch 00147: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00158: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00169: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00180: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Validation loss decreased (0.371334 ===> 0.369799). Saving the model...\n",
      "Validation loss decreased (0.369799 ===> 0.369551). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.3881402123234516\n",
      "Epoch 00211: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Validation loss decreased (0.369551 ===> 0.366730). Saving the model...\n",
      "Validation loss decreased (0.366730 ===> 0.362237). Saving the model...\n",
      "Epoch 00231: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00242: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00253: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00264: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00275: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00286: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00297: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00308: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00319: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Validation loss decreased (0.362237 ===> 0.357709). Saving the model...\n",
      "Epoch 00340: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00351: reducing learning rate of group 0 to 9.5367e-08.\n",
      "STOP train fold= 5\n",
      "fold 5 AUC: 0.8203208556149733\n",
      "----- 6 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 4.609614). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 4.60961352322167\n",
      "Validation loss decreased (4.609614 ===> 1.190880). Saving the model...\n",
      "Validation loss decreased (1.190880 ===> 0.723965). Saving the model...\n",
      "Validation loss decreased (0.723965 ===> 0.687299). Saving the model...\n",
      "Validation loss decreased (0.687299 ===> 0.663444). Saving the model...\n",
      "Validation loss decreased (0.663444 ===> 0.633837). Saving the model...\n",
      "Validation loss decreased (0.633837 ===> 0.511905). Saving the model...\n",
      "Validation loss decreased (0.511905 ===> 0.511023). Saving the model...\n",
      "Validation loss decreased (0.511023 ===> 0.484624). Saving the model...\n",
      "Epoch 00024: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.484624 ===> 0.463734). Saving the model...\n",
      "Validation loss decreased (0.463734 ===> 0.456830). Saving the model...\n",
      "Validation loss decreased (0.456830 ===> 0.451411). Saving the model...\n",
      "Validation loss decreased (0.451411 ===> 0.437137). Saving the model...\n",
      "Validation loss decreased (0.437137 ===> 0.435851). Saving the model...\n",
      "Validation loss decreased (0.435851 ===> 0.425710). Saving the model...\n",
      "Epoch 00055: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.425710 ===> 0.424447). Saving the model...\n",
      "Validation loss decreased (0.424447 ===> 0.420744). Saving the model...\n",
      "Validation loss decreased (0.420744 ===> 0.413809). Saving the model...\n",
      "Validation loss decreased (0.413809 ===> 0.411660). Saving the model...\n",
      "Validation loss decreased (0.411660 ===> 0.406273). Saving the model...\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.406273 ===> 0.399802). Saving the model...\n",
      "Validation loss decreased (0.399802 ===> 0.397019). Saving the model...\n",
      "Validation loss decreased (0.397019 ===> 0.393302). Saving the model...\n",
      "Validation loss decreased (0.393302 ===> 0.388936). Saving the model...\n",
      "Epoch 00107: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.388936 ===> 0.383660). Saving the model...\n",
      "Epoch 00122: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.383660 ===> 0.378474). Saving the model...\n",
      "Validation loss decreased (0.378474 ===> 0.372458). Saving the model...\n",
      "Validation loss decreased (0.372458 ===> 0.368380). Saving the model...\n",
      "Epoch 00151: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00162: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Validation loss decreased (0.368380 ===> 0.362536). Saving the model...\n",
      "Epoch 00178: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00189: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00200: reducing learning rate of group 0 to 9.7656e-05.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.3839123699646383\n",
      "Validation loss decreased (0.362536 ===> 0.361547). Saving the model...\n",
      "Validation loss decreased (0.361547 ===> 0.360817). Saving the model...\n",
      "Epoch 00220: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00231: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Validation loss decreased (0.360817 ===> 0.360253). Saving the model...\n",
      "Epoch 00243: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00254: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00265: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00276: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Validation loss decreased (0.360253 ===> 0.350450). Saving the model...\n",
      "Epoch 00289: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00300: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00311: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00322: reducing learning rate of group 0 to 9.5367e-08.\n",
      "Epoch 00333: reducing learning rate of group 0 to 4.7684e-08.\n",
      "Epoch 00344: reducing learning rate of group 0 to 2.3842e-08.\n",
      "Epoch 00355: reducing learning rate of group 0 to 1.1921e-08.\n",
      "STOP train fold= 6\n",
      "fold 6 AUC: 0.860427807486631\n",
      "----- 7 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 4.615903). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 4.615903435799844\n",
      "Validation loss decreased (4.615903 ===> 1.793559). Saving the model...\n",
      "Validation loss decreased (1.793559 ===> 1.161817). Saving the model...\n",
      "Validation loss decreased (1.161817 ===> 1.009532). Saving the model...\n",
      "Validation loss decreased (1.009532 ===> 0.988640). Saving the model...\n",
      "Validation loss decreased (0.988640 ===> 0.849371). Saving the model...\n",
      "Validation loss decreased (0.849371 ===> 0.642464). Saving the model...\n",
      "Validation loss decreased (0.642464 ===> 0.603933). Saving the model...\n",
      "Validation loss decreased (0.603933 ===> 0.558862). Saving the model...\n",
      "Validation loss decreased (0.558862 ===> 0.528259). Saving the model...\n",
      "Validation loss decreased (0.528259 ===> 0.517855). Saving the model...\n",
      "Validation loss decreased (0.517855 ===> 0.501403). Saving the model...\n",
      "Validation loss decreased (0.501403 ===> 0.494571). Saving the model...\n",
      "Validation loss decreased (0.494571 ===> 0.493690). Saving the model...\n",
      "Validation loss decreased (0.493690 ===> 0.455635). Saving the model...\n",
      "Validation loss decreased (0.455635 ===> 0.454882). Saving the model...\n",
      "Validation loss decreased (0.454882 ===> 0.453022). Saving the model...\n",
      "Validation loss decreased (0.453022 ===> 0.448107). Saving the model...\n",
      "Epoch 00054: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Validation loss decreased (0.448107 ===> 0.437856). Saving the model...\n",
      "Validation loss decreased (0.437856 ===> 0.421865). Saving the model...\n",
      "Validation loss decreased (0.421865 ===> 0.420578). Saving the model...\n",
      "Validation loss decreased (0.420578 ===> 0.418456). Saving the model...\n",
      "Validation loss decreased (0.418456 ===> 0.415382). Saving the model...\n",
      "Validation loss decreased (0.415382 ===> 0.411979). Saving the model...\n",
      "Validation loss decreased (0.411979 ===> 0.404631). Saving the model...\n",
      "Epoch 00118: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Epoch 00129: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.404631 ===> 0.404134). Saving the model...\n",
      "Epoch 00149: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch 00160: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00171: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch 00182: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Validation loss decreased (0.404134 ===> 0.402805). Saving the model...\n",
      "Validation loss decreased (0.402805 ===> 0.400498). Saving the model...\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.4185155737314796\n",
      "Epoch 00203: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Validation loss decreased (0.400498 ===> 0.393229). Saving the model...\n",
      "Epoch 00225: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Validation loss decreased (0.393229 ===> 0.390833). Saving the model...\n",
      "Epoch 00244: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00255: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00266: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00277: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00288: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00299: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00310: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00321: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00332: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00343: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00354: reducing learning rate of group 0 to 9.5367e-08.\n",
      "STOP train fold= 7\n",
      "fold 7 AUC: 0.8016042780748663\n",
      "----- 8 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 4.684989). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 4.684989391716936\n",
      "Validation loss decreased (4.684989 ===> 1.520749). Saving the model...\n",
      "Validation loss decreased (1.520749 ===> 0.830396). Saving the model...\n",
      "Validation loss decreased (0.830396 ===> 0.806637). Saving the model...\n",
      "Validation loss decreased (0.806637 ===> 0.757021). Saving the model...\n",
      "Validation loss decreased (0.757021 ===> 0.679505). Saving the model...\n",
      "Validation loss decreased (0.679505 ===> 0.568645). Saving the model...\n",
      "Validation loss decreased (0.568645 ===> 0.537379). Saving the model...\n",
      "Validation loss decreased (0.537379 ===> 0.499247). Saving the model...\n",
      "Validation loss decreased (0.499247 ===> 0.463002). Saving the model...\n",
      "Validation loss decreased (0.463002 ===> 0.462387). Saving the model...\n",
      "Validation loss decreased (0.462387 ===> 0.449070). Saving the model...\n",
      "Validation loss decreased (0.449070 ===> 0.446131). Saving the model...\n",
      "Validation loss decreased (0.446131 ===> 0.440543). Saving the model...\n",
      "Epoch 00059: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Epoch 00070: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Epoch 00081: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.440543 ===> 0.437605). Saving the model...\n",
      "Validation loss decreased (0.437605 ===> 0.436404). Saving the model...\n",
      "Validation loss decreased (0.436404 ===> 0.431013). Saving the model...\n",
      "Validation loss decreased (0.431013 ===> 0.428889). Saving the model...\n",
      "Validation loss decreased (0.428889 ===> 0.418825). Saving the model...\n",
      "Epoch 00117: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.418825 ===> 0.411558). Saving the model...\n",
      "Epoch 00135: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch 00146: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.411558 ===> 0.406498). Saving the model...\n",
      "Epoch 00161: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00172: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00183: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00194: reducing learning rate of group 0 to 9.7656e-05.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.41606314807143724\n",
      "Validation loss decreased (0.406498 ===> 0.404489). Saving the model...\n",
      "Epoch 00216: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch 00227: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00238: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00249: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00260: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Validation loss decreased (0.404489 ===> 0.397901). Saving the model...\n",
      "Epoch 00279: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00290: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00301: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00312: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00323: reducing learning rate of group 0 to 9.5367e-08.\n",
      "Epoch 00334: reducing learning rate of group 0 to 4.7684e-08.\n",
      "Epoch 00345: reducing learning rate of group 0 to 2.3842e-08.\n",
      "Epoch 00356: reducing learning rate of group 0 to 1.1921e-08.\n",
      "STOP train fold= 8\n",
      "fold 8 AUC: 0.9288770053475935\n",
      "----- 9 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Validation loss decreased (   inf ===> 3.977226). Saving the model...\n",
      "\n",
      "Epoch: 1 \tTrain Loss: 3.9772263381843853\n",
      "Validation loss decreased (3.977226 ===> 2.010545). Saving the model...\n",
      "Validation loss decreased (2.010545 ===> 1.002606). Saving the model...\n",
      "Validation loss decreased (1.002606 ===> 0.720254). Saving the model...\n",
      "Validation loss decreased (0.720254 ===> 0.680387). Saving the model...\n",
      "Validation loss decreased (0.680387 ===> 0.613511). Saving the model...\n",
      "Validation loss decreased (0.613511 ===> 0.568013). Saving the model...\n",
      "Validation loss decreased (0.568013 ===> 0.525288). Saving the model...\n",
      "Validation loss decreased (0.525288 ===> 0.503695). Saving the model...\n",
      "Validation loss decreased (0.503695 ===> 0.485508). Saving the model...\n",
      "Validation loss decreased (0.485508 ===> 0.477603). Saving the model...\n",
      "Validation loss decreased (0.477603 ===> 0.476880). Saving the model...\n",
      "Validation loss decreased (0.476880 ===> 0.476400). Saving the model...\n",
      "Validation loss decreased (0.476400 ===> 0.464477). Saving the model...\n",
      "Validation loss decreased (0.464477 ===> 0.464364). Saving the model...\n",
      "Validation loss decreased (0.464364 ===> 0.454695). Saving the model...\n",
      "Validation loss decreased (0.454695 ===> 0.454089). Saving the model...\n",
      "Epoch 00044: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Epoch 00055: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Validation loss decreased (0.454089 ===> 0.448563). Saving the model...\n",
      "Validation loss decreased (0.448563 ===> 0.437500). Saving the model...\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Validation loss decreased (0.437500 ===> 0.425978). Saving the model...\n",
      "Epoch 00093: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Validation loss decreased (0.425978 ===> 0.421482). Saving the model...\n",
      "Validation loss decreased (0.421482 ===> 0.417828). Saving the model...\n",
      "Epoch 00120: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Validation loss decreased (0.417828 ===> 0.413963). Saving the model...\n",
      "Epoch 00137: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Validation loss decreased (0.413963 ===> 0.402713). Saving the model...\n",
      "Epoch 00155: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch 00166: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch 00177: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch 00188: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch 00199: reducing learning rate of group 0 to 4.8828e-05.\n",
      "\n",
      "Epoch: 201 \tTrain Loss: 0.429295064617575\n",
      "Epoch 00210: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch 00221: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch 00232: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch 00243: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch 00254: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch 00265: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch 00276: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch 00287: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch 00298: reducing learning rate of group 0 to 9.5367e-08.\n",
      "Epoch 00309: reducing learning rate of group 0 to 4.7684e-08.\n",
      "Epoch 00320: reducing learning rate of group 0 to 2.3842e-08.\n",
      "Epoch 00331: reducing learning rate of group 0 to 1.1921e-08.\n",
      "STOP train fold= 9\n",
      "fold 9 AUC: 0.7978835978835979\n",
      "----- Training Ended! -----\n",
      "AUC 0.837130774720651\n"
     ]
    }
   ],
   "source": [
    "batch_size = CFG.batch_size\n",
    "batch_no = len(df_train) // batch_size\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(df_train, df_train[CFG.target])):\n",
    "    print('-----', fold, '-----')\n",
    "    # fold毎に初期化する設定\n",
    "    # model, optimizer, scheduler, その他変数\n",
    "    model = Net(len(all_features), 512, 1).to(CFG.device)\n",
    "    print(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=CFG.lr_factor, min_lr=1e-6, patience=10)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_loss_min = np.Inf\n",
    "\n",
    "    # データを分割する。fold毎に一回だけやる\n",
    "    X_train = df_train[all_features].iloc[train_index]\n",
    "    y_train = df_train.iloc[train_index][CFG.target]\n",
    "    X_valid = df_train[all_features].iloc[valid_index]\n",
    "    y_valid = df_train.iloc[valid_index][CFG.target]\n",
    "    X_test = df_test[all_features]\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        # TODO Variableを使うのは古いらしい\n",
    "        x0_var = Variable(torch.FloatTensor(X_train.values)).to(CFG.device)\n",
    "        y0_var = Variable(torch.FloatTensor(y_train.values)).to(CFG.device)\n",
    "        for i in range(batch_no):\n",
    "            # ミニバッチ学習\n",
    "            start = i * batch_size\n",
    "            end   = start + batch_size\n",
    "            x_var = x0_var[start:end]\n",
    "            y_var = y0_var[start:end]\n",
    "            if len(x_var) == 1:\n",
    "                raise \"len(x_var) == 1, cant use bn layer. please change batch_size\"\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_var).squeeze(1)\n",
    "            loss   = criterion(output, y_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()*batch_size\n",
    "\n",
    "        train_loss = train_loss / len(X_train)\n",
    "        if train_loss <= train_loss_min:\n",
    "            print(\"Validation loss decreased ({:6f} ===> {:6f}). Saving the model...\".format(train_loss_min,train_loss))\n",
    "            torch.save(model.state_dict(), CFG.model_path + \"/model\" + str(fold) + \".pt\")\n",
    "            train_loss_min = train_loss\n",
    "\n",
    "        # lrを引き下げる\n",
    "        # 暫くlossが変わってなければ、EarlyStoppingする。\n",
    "        scheduler.step(train_loss)\n",
    "        if early_stopping.step(epoch, train_loss):\n",
    "            break\n",
    "\n",
    "        # log\n",
    "        if epoch % 200 == 0:\n",
    "            print('')\n",
    "            print(\"Epoch: {} \\tTrain Loss: {}\".format(epoch+1, train_loss))\n",
    "\n",
    "    print('STOP train fold=', fold)\n",
    "    x0_var = Variable(torch.FloatTensor(X_valid.values)).to(CFG.device)\n",
    "    pred = torch.sigmoid(model(x0_var))\n",
    "    df_train.loc[valid_index, CFG.pred] = pred.data.cpu().numpy()\n",
    "    auc = roc_auc_score(df_train.loc[valid_index, CFG.target], df_train.loc[valid_index, CFG.pred])\n",
    "    print('fold', fold, 'AUC:', auc)\n",
    "\n",
    "auc = roc_auc_score(df_train[CFG.target], df_train[CFG.pred])\n",
    "print('-----', 'Training Ended!', '-----')\n",
    "print('AUC', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./models\\\\model0.pt' './models\\\\model1.pt' './models\\\\model2.pt'\n",
      " './models\\\\model3.pt' './models\\\\model4.pt' './models\\\\model5.pt'\n",
      " './models\\\\model6.pt' './models\\\\model7.pt' './models\\\\model8.pt'\n",
      " './models\\\\model9.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_5748\\3226074810.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>pred0</th>\n",
       "      <th>pred1</th>\n",
       "      <th>pred2</th>\n",
       "      <th>pred3</th>\n",
       "      <th>pred4</th>\n",
       "      <th>pred5</th>\n",
       "      <th>pred6</th>\n",
       "      <th>pred7</th>\n",
       "      <th>pred8</th>\n",
       "      <th>pred9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105576</td>\n",
       "      <td>0.091893</td>\n",
       "      <td>0.070121</td>\n",
       "      <td>0.079433</td>\n",
       "      <td>0.194460</td>\n",
       "      <td>0.090482</td>\n",
       "      <td>0.091987</td>\n",
       "      <td>0.046344</td>\n",
       "      <td>0.098540</td>\n",
       "      <td>0.061862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.283934</td>\n",
       "      <td>0.283783</td>\n",
       "      <td>0.262539</td>\n",
       "      <td>0.335459</td>\n",
       "      <td>0.209208</td>\n",
       "      <td>0.093508</td>\n",
       "      <td>0.089988</td>\n",
       "      <td>0.097999</td>\n",
       "      <td>0.214368</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180792</td>\n",
       "      <td>0.015352</td>\n",
       "      <td>0.041926</td>\n",
       "      <td>0.086274</td>\n",
       "      <td>0.098147</td>\n",
       "      <td>0.014793</td>\n",
       "      <td>0.036599</td>\n",
       "      <td>0.018911</td>\n",
       "      <td>0.148233</td>\n",
       "      <td>0.081029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.068570</td>\n",
       "      <td>0.115416</td>\n",
       "      <td>0.056804</td>\n",
       "      <td>0.160763</td>\n",
       "      <td>0.082783</td>\n",
       "      <td>0.109066</td>\n",
       "      <td>0.097097</td>\n",
       "      <td>0.086430</td>\n",
       "      <td>0.064767</td>\n",
       "      <td>0.076494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.214496</td>\n",
       "      <td>0.325254</td>\n",
       "      <td>0.362370</td>\n",
       "      <td>0.065092</td>\n",
       "      <td>0.264526</td>\n",
       "      <td>0.309451</td>\n",
       "      <td>0.177329</td>\n",
       "      <td>0.370248</td>\n",
       "      <td>0.409841</td>\n",
       "      <td>0.743845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare  male  Q  S     pred0  \\\n",
       "0         0       3  34.5      0      0   7.8292     1  1  0  0.105576   \n",
       "1         0       3  47.0      1      0   7.0000     0  0  1  0.283934   \n",
       "2         0       2  62.0      0      0   9.6875     1  1  0  0.180792   \n",
       "3         0       3  27.0      0      0   8.6625     1  0  1  0.068570   \n",
       "4         0       3  22.0      1      1  12.2875     0  0  1  0.214496   \n",
       "\n",
       "      pred1     pred2     pred3     pred4     pred5     pred6     pred7  \\\n",
       "0  0.091893  0.070121  0.079433  0.194460  0.090482  0.091987  0.046344   \n",
       "1  0.283783  0.262539  0.335459  0.209208  0.093508  0.089988  0.097999   \n",
       "2  0.015352  0.041926  0.086274  0.098147  0.014793  0.036599  0.018911   \n",
       "3  0.115416  0.056804  0.160763  0.082783  0.109066  0.097097  0.086430   \n",
       "4  0.325254  0.362370  0.065092  0.264526  0.309451  0.177329  0.370248   \n",
       "\n",
       "      pred8     pred9  \n",
       "0  0.098540  0.061862  \n",
       "1  0.214368  0.318600  \n",
       "2  0.148233  0.081029  \n",
       "3  0.064767  0.076494  \n",
       "4  0.409841  0.743845  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = np.sort(glob.glob(f\"./{CFG.model_path}/*.pt\"))\n",
    "print(models)\n",
    "# fold別に作った10個のモデルをロードする\n",
    "with torch.no_grad():\n",
    "    for i, model_name in enumerate(models):\n",
    "        model = Net(len(all_features), 512, 1).to(CFG.device)\n",
    "        model.load_state_dict(torch.load(model_name,))\n",
    "        X_test = df_test[all_features]\n",
    "        x0_var = Variable(torch.FloatTensor(X_test.values)).to(CFG.device)\n",
    "        pred = F.sigmoid(model(x0_var))\n",
    "        df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[CFG.target] = df_test[CFG.test_pred].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[CFG.target] = (df_sub[CFG.target] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f8cdbf2a96461788475085dd1e9d6dd7137de331e19aa3c17c37cb4f0963a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yourenvname')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
