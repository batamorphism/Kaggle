{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install japanize_matplotlib | tail -n 1\\n!pip install torchviz | tail -n 1\\n!pip install torchinfo | tail -n 1\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 必要ライブラリの導入\n",
    "\"\"\"\n",
    "!pip install japanize_matplotlib | tail -n 1\n",
    "!pip install torchviz | tail -n 1\n",
    "!pip install torchinfo | tail -n 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch関連ライブラリのインポート\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14  # デフォルトフォントサイズ変更\n",
    "plt.rcParams['figure.figsize'] = (6,6)  # デフォルトグラフサイズ変更\n",
    "plt.rcParams['axes.grid'] = True  # デフォルトで方眼表示ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元データ (150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "# 学習用データ準備\n",
    "\n",
    "# ライブラリのインポート\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# データ読み込み\n",
    "iris = load_iris()\n",
    "\n",
    "# 入力データと正解データ取得\n",
    "x_org, y_org = iris.data, iris.target\n",
    "\n",
    "# 結果確認\n",
    "print('元データ', x_org.shape, y_org.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対象データ (100, 2) (100,)\n"
     ]
    }
   ],
   "source": [
    "# データ絞り込み\n",
    "#   クラス0, 1のみ\n",
    "#   項目sepal_lengthとsepal_widthのみ\n",
    "\n",
    "x_data = iris.data[:100,:2]\n",
    "y_data = iris.target[:100]\n",
    "\n",
    "# 結果確認\n",
    "print('対象データ', x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n",
      "(70, 2) (30, 2) (70,) (30,)\n"
     ]
    }
   ],
   "source": [
    "# 　元データのサイズ\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "# 訓練データ、検証データに分割 (シャフルも同時に実施)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data, y_data, train_size=70, test_size=30, \n",
    "    random_state=123)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_input: 2  n_output:1\n"
     ]
    }
   ],
   "source": [
    "# 入力次元数　(今の場合2)\n",
    "n_input= x_train.shape[1]\n",
    "\n",
    "# 出力次元数\n",
    "n_output = 1\n",
    "\n",
    "# 結果確認\n",
    "print(f'n_input: {n_input}  n_output:{n_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "# 2入力1出力のロジスティック回帰モデル\n",
    "\n",
    "# これ、層を足していくの面倒だけどなんとかならん？\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_n, output_n):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_n, output_n)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    # 予測関数の定義\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンスの生成\n",
    "\n",
    "net = Net([n_input, n_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l1.weight', Parameter containing:\n",
      "tensor([[-0.0221,  0.1293]], requires_grad=True))\n",
      "('l1.bias', Parameter containing:\n",
      "tensor([-0.3022], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# モデル内のパラメータの確認\n",
    "# l1.weightとl1.biasがあることがわかる\n",
    "\n",
    "for parameter in net.named_parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (activation): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# モデルの概要表示\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数： 交差エントロピー関数\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 学習率\n",
    "lr = 0.01\n",
    "\n",
    "# 最適化関数: 勾配降下法\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力データ x_train と正解データ y_train のテンソル化\n",
    "\n",
    "inputs = torch.tensor(x_train).float()\n",
    "labels = torch.tensor(y_train).float()\n",
    "\n",
    "# 正解データはN行1列の行列に変換する\n",
    "labels1 = labels.view((-1,1))\n",
    "\n",
    "# 検証データのテンソル化\n",
    "inputs_test = torch.tensor(x_test).float()\n",
    "labels_test = torch.tensor(y_test).float()\n",
    "\n",
    "# 検証用の正解データもN行1列の行列に変換する\n",
    "labels1_test = labels_test.view((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 3.0.0 (20220226.1711)\n -->\n<!-- Pages: 1 -->\n<svg width=\"216pt\" height=\"393pt\"\n viewBox=\"0.00 0.00 216.00 393.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 389)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-389 212,-389 212,4 -4,4\"/>\n<!-- 2022986308320 -->\n<g id=\"node1\" class=\"node\">\n<title>2022986308320</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"130.5,-31 76.5,-31 76.5,0 130.5,0 130.5,-31\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 2022986332384 -->\n<g id=\"node2\" class=\"node\">\n<title>2022986332384</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"193,-86 14,-86 14,-67 193,-67 193,-86\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">BinaryCrossEntropyBackward0</text>\n</g>\n<!-- 2022986332384&#45;&gt;2022986308320 -->\n<g id=\"edge8\" class=\"edge\">\n<title>2022986332384&#45;&gt;2022986308320</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-66.79C103.5,-60.07 103.5,-50.4 103.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-41.19 103.5,-31.19 100,-41.19 107,-41.19\"/>\n</g>\n<!-- 2022986332288 -->\n<g id=\"node3\" class=\"node\">\n<title>2022986332288</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"160,-141 47,-141 47,-122 160,-122 160,-141\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">SigmoidBackward0</text>\n</g>\n<!-- 2022986332288&#45;&gt;2022986332384 -->\n<g id=\"edge1\" class=\"edge\">\n<title>2022986332288&#45;&gt;2022986332384</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-121.75C103.5,-114.8 103.5,-104.85 103.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-96.09 103.5,-86.09 100,-96.09 107,-96.09\"/>\n</g>\n<!-- 2022986332192 -->\n<g id=\"node4\" class=\"node\">\n<title>2022986332192</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154,-196 53,-196 53,-177 154,-177 154,-196\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 2022986332192&#45;&gt;2022986332288 -->\n<g id=\"edge2\" class=\"edge\">\n<title>2022986332192&#45;&gt;2022986332288</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-176.75C103.5,-169.8 103.5,-159.85 103.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-151.09 103.5,-141.09 100,-151.09 107,-151.09\"/>\n</g>\n<!-- 2022986332960 -->\n<g id=\"node5\" class=\"node\">\n<title>2022986332960</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-251 0,-251 0,-232 101,-232 101,-251\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 2022986332960&#45;&gt;2022986332192 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2022986332960&#45;&gt;2022986332192</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.25,-231.75C66.97,-224.03 78.4,-212.6 87.72,-203.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"90.31,-205.64 94.91,-196.09 85.36,-200.69 90.31,-205.64\"/>\n</g>\n<!-- 2024804060272 -->\n<g id=\"node6\" class=\"node\">\n<title>2024804060272</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-318 23.5,-318 23.5,-287 77.5,-287 77.5,-318\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 2024804060272&#45;&gt;2022986332960 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2024804060272&#45;&gt;2022986332960</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-286.92C50.5,-279.22 50.5,-269.69 50.5,-261.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-261.25 50.5,-251.25 47,-261.25 54,-261.25\"/>\n</g>\n<!-- 2022986333008 -->\n<g id=\"node7\" class=\"node\">\n<title>2022986333008</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196,-251 119,-251 119,-232 196,-232 196,-251\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 2022986333008&#45;&gt;2022986332192 -->\n<g id=\"edge5\" class=\"edge\">\n<title>2022986333008&#45;&gt;2022986332192</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.58,-231.75C140.72,-224.03 129.07,-212.6 119.58,-203.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"121.84,-200.6 112.25,-196.09 116.94,-205.59 121.84,-200.6\"/>\n</g>\n<!-- 2022986332000 -->\n<g id=\"node8\" class=\"node\">\n<title>2022986332000</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-312 107,-312 107,-293 208,-293 208,-312\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-300\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 2022986332000&#45;&gt;2022986333008 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2022986332000&#45;&gt;2022986333008</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-292.79C157.5,-284.6 157.5,-272.06 157.5,-261.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-261.24 157.5,-251.24 154,-261.24 161,-261.24\"/>\n</g>\n<!-- 2024804059872 -->\n<g id=\"node9\" class=\"node\">\n<title>2024804059872</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"187,-385 128,-385 128,-354 187,-354 187,-385\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-361\" font-family=\"monospace\" font-size=\"10.00\"> (1, 2)</text>\n</g>\n<!-- 2024804059872&#45;&gt;2022986332000 -->\n<g id=\"edge7\" class=\"edge\">\n<title>2024804059872&#45;&gt;2022986332000</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-353.75C157.5,-344.39 157.5,-332.19 157.5,-322.16\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-322.02 157.5,-312.02 154,-322.02 161,-322.02\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1d70361b730>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 予測計算\n",
    "outputs = net(inputs)\n",
    "\n",
    "# 損失計算\n",
    "loss = criterion(outputs, labels1)\n",
    "\n",
    "# 損失の計算グラフ可視化\n",
    "g = make_dot(loss, params=dict(net.named_parameters()))\n",
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率\n",
    "lr = 0.01\n",
    "\n",
    "# 初期化\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = Net(n_input, n_output).to(device)\n",
    "\n",
    "# 損失関数： 交差エントロピー関数\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 最適化関数: 勾配降下法\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 繰り返し回数\n",
    "num_epochs = 10000\n",
    "\n",
    "# 記録用リストの初期化\n",
    "history = np.zeros((0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000], loss: 0.03816 acc: 1.00000 val_loss: 0.12000, val_acc: 0.96667\n",
      "Epoch [100/10000], loss: 0.03808 acc: 1.00000 val_loss: 0.11997, val_acc: 0.96667\n",
      "Epoch [200/10000], loss: 0.03800 acc: 1.00000 val_loss: 0.11994, val_acc: 0.96667\n",
      "Epoch [300/10000], loss: 0.03791 acc: 1.00000 val_loss: 0.11992, val_acc: 0.96667\n",
      "Epoch [400/10000], loss: 0.03783 acc: 1.00000 val_loss: 0.11989, val_acc: 0.96667\n",
      "Epoch [500/10000], loss: 0.03776 acc: 1.00000 val_loss: 0.11987, val_acc: 0.96667\n",
      "Epoch [600/10000], loss: 0.03768 acc: 1.00000 val_loss: 0.11985, val_acc: 0.96667\n",
      "Epoch [700/10000], loss: 0.03760 acc: 1.00000 val_loss: 0.11982, val_acc: 0.96667\n",
      "Epoch [800/10000], loss: 0.03752 acc: 1.00000 val_loss: 0.11980, val_acc: 0.96667\n",
      "Epoch [900/10000], loss: 0.03744 acc: 1.00000 val_loss: 0.11977, val_acc: 0.96667\n",
      "Epoch [1000/10000], loss: 0.03736 acc: 1.00000 val_loss: 0.11975, val_acc: 0.96667\n",
      "Epoch [1100/10000], loss: 0.03729 acc: 1.00000 val_loss: 0.11973, val_acc: 0.96667\n",
      "Epoch [1200/10000], loss: 0.03721 acc: 1.00000 val_loss: 0.11970, val_acc: 0.96667\n",
      "Epoch [1300/10000], loss: 0.03713 acc: 1.00000 val_loss: 0.11968, val_acc: 0.96667\n",
      "Epoch [1400/10000], loss: 0.03706 acc: 1.00000 val_loss: 0.11966, val_acc: 0.96667\n",
      "Epoch [1500/10000], loss: 0.03698 acc: 1.00000 val_loss: 0.11964, val_acc: 0.96667\n",
      "Epoch [1600/10000], loss: 0.03691 acc: 1.00000 val_loss: 0.11962, val_acc: 0.96667\n",
      "Epoch [1700/10000], loss: 0.03683 acc: 1.00000 val_loss: 0.11959, val_acc: 0.96667\n",
      "Epoch [1800/10000], loss: 0.03676 acc: 1.00000 val_loss: 0.11957, val_acc: 0.96667\n",
      "Epoch [1900/10000], loss: 0.03668 acc: 1.00000 val_loss: 0.11955, val_acc: 0.96667\n",
      "Epoch [2000/10000], loss: 0.03661 acc: 1.00000 val_loss: 0.11953, val_acc: 0.96667\n",
      "Epoch [2100/10000], loss: 0.03654 acc: 1.00000 val_loss: 0.11951, val_acc: 0.96667\n",
      "Epoch [2200/10000], loss: 0.03647 acc: 1.00000 val_loss: 0.11949, val_acc: 0.96667\n",
      "Epoch [2300/10000], loss: 0.03639 acc: 1.00000 val_loss: 0.11947, val_acc: 0.96667\n",
      "Epoch [2400/10000], loss: 0.03632 acc: 1.00000 val_loss: 0.11945, val_acc: 0.96667\n",
      "Epoch [2500/10000], loss: 0.03625 acc: 1.00000 val_loss: 0.11943, val_acc: 0.96667\n",
      "Epoch [2600/10000], loss: 0.03618 acc: 1.00000 val_loss: 0.11941, val_acc: 0.96667\n",
      "Epoch [2700/10000], loss: 0.03611 acc: 1.00000 val_loss: 0.11939, val_acc: 0.96667\n",
      "Epoch [2800/10000], loss: 0.03604 acc: 1.00000 val_loss: 0.11937, val_acc: 0.96667\n",
      "Epoch [2900/10000], loss: 0.03597 acc: 1.00000 val_loss: 0.11936, val_acc: 0.96667\n",
      "Epoch [3000/10000], loss: 0.03590 acc: 1.00000 val_loss: 0.11934, val_acc: 0.96667\n",
      "Epoch [3100/10000], loss: 0.03583 acc: 1.00000 val_loss: 0.11932, val_acc: 0.96667\n",
      "Epoch [3200/10000], loss: 0.03576 acc: 1.00000 val_loss: 0.11930, val_acc: 0.96667\n",
      "Epoch [3300/10000], loss: 0.03569 acc: 1.00000 val_loss: 0.11928, val_acc: 0.96667\n",
      "Epoch [3400/10000], loss: 0.03562 acc: 1.00000 val_loss: 0.11927, val_acc: 0.96667\n",
      "Epoch [3500/10000], loss: 0.03555 acc: 1.00000 val_loss: 0.11925, val_acc: 0.96667\n",
      "Epoch [3600/10000], loss: 0.03549 acc: 1.00000 val_loss: 0.11923, val_acc: 0.96667\n",
      "Epoch [3700/10000], loss: 0.03542 acc: 1.00000 val_loss: 0.11921, val_acc: 0.96667\n",
      "Epoch [3800/10000], loss: 0.03535 acc: 1.00000 val_loss: 0.11920, val_acc: 0.96667\n",
      "Epoch [3900/10000], loss: 0.03529 acc: 1.00000 val_loss: 0.11918, val_acc: 0.96667\n",
      "Epoch [4000/10000], loss: 0.03522 acc: 1.00000 val_loss: 0.11917, val_acc: 0.96667\n",
      "Epoch [4100/10000], loss: 0.03515 acc: 1.00000 val_loss: 0.11915, val_acc: 0.96667\n",
      "Epoch [4200/10000], loss: 0.03509 acc: 1.00000 val_loss: 0.11913, val_acc: 0.96667\n",
      "Epoch [4300/10000], loss: 0.03502 acc: 1.00000 val_loss: 0.11912, val_acc: 0.96667\n",
      "Epoch [4400/10000], loss: 0.03496 acc: 1.00000 val_loss: 0.11910, val_acc: 0.96667\n",
      "Epoch [4500/10000], loss: 0.03489 acc: 1.00000 val_loss: 0.11909, val_acc: 0.96667\n",
      "Epoch [4600/10000], loss: 0.03483 acc: 1.00000 val_loss: 0.11907, val_acc: 0.96667\n",
      "Epoch [4700/10000], loss: 0.03476 acc: 1.00000 val_loss: 0.11906, val_acc: 0.96667\n",
      "Epoch [4800/10000], loss: 0.03470 acc: 1.00000 val_loss: 0.11904, val_acc: 0.96667\n",
      "Epoch [4900/10000], loss: 0.03464 acc: 1.00000 val_loss: 0.11903, val_acc: 0.96667\n",
      "Epoch [5000/10000], loss: 0.03457 acc: 1.00000 val_loss: 0.11901, val_acc: 0.96667\n",
      "Epoch [5100/10000], loss: 0.03451 acc: 1.00000 val_loss: 0.11900, val_acc: 0.96667\n",
      "Epoch [5200/10000], loss: 0.03445 acc: 1.00000 val_loss: 0.11899, val_acc: 0.96667\n",
      "Epoch [5300/10000], loss: 0.03439 acc: 1.00000 val_loss: 0.11897, val_acc: 0.96667\n",
      "Epoch [5400/10000], loss: 0.03432 acc: 1.00000 val_loss: 0.11896, val_acc: 0.96667\n",
      "Epoch [5500/10000], loss: 0.03426 acc: 1.00000 val_loss: 0.11895, val_acc: 0.96667\n",
      "Epoch [5600/10000], loss: 0.03420 acc: 1.00000 val_loss: 0.11893, val_acc: 0.96667\n",
      "Epoch [5700/10000], loss: 0.03414 acc: 1.00000 val_loss: 0.11892, val_acc: 0.96667\n",
      "Epoch [5800/10000], loss: 0.03408 acc: 1.00000 val_loss: 0.11891, val_acc: 0.96667\n",
      "Epoch [5900/10000], loss: 0.03402 acc: 1.00000 val_loss: 0.11890, val_acc: 0.96667\n",
      "Epoch [6000/10000], loss: 0.03396 acc: 1.00000 val_loss: 0.11888, val_acc: 0.96667\n",
      "Epoch [6100/10000], loss: 0.03390 acc: 1.00000 val_loss: 0.11887, val_acc: 0.96667\n",
      "Epoch [6200/10000], loss: 0.03384 acc: 1.00000 val_loss: 0.11886, val_acc: 0.96667\n",
      "Epoch [6300/10000], loss: 0.03378 acc: 1.00000 val_loss: 0.11885, val_acc: 0.96667\n",
      "Epoch [6400/10000], loss: 0.03372 acc: 1.00000 val_loss: 0.11884, val_acc: 0.96667\n",
      "Epoch [6500/10000], loss: 0.03366 acc: 1.00000 val_loss: 0.11882, val_acc: 0.96667\n",
      "Epoch [6600/10000], loss: 0.03360 acc: 1.00000 val_loss: 0.11881, val_acc: 0.96667\n",
      "Epoch [6700/10000], loss: 0.03354 acc: 1.00000 val_loss: 0.11880, val_acc: 0.96667\n",
      "Epoch [6800/10000], loss: 0.03348 acc: 1.00000 val_loss: 0.11879, val_acc: 0.96667\n",
      "Epoch [6900/10000], loss: 0.03343 acc: 1.00000 val_loss: 0.11878, val_acc: 0.96667\n",
      "Epoch [7000/10000], loss: 0.03337 acc: 1.00000 val_loss: 0.11877, val_acc: 0.96667\n",
      "Epoch [7100/10000], loss: 0.03331 acc: 1.00000 val_loss: 0.11876, val_acc: 0.96667\n",
      "Epoch [7200/10000], loss: 0.03325 acc: 1.00000 val_loss: 0.11875, val_acc: 0.96667\n",
      "Epoch [7300/10000], loss: 0.03320 acc: 1.00000 val_loss: 0.11874, val_acc: 0.96667\n",
      "Epoch [7400/10000], loss: 0.03314 acc: 1.00000 val_loss: 0.11873, val_acc: 0.96667\n",
      "Epoch [7500/10000], loss: 0.03309 acc: 1.00000 val_loss: 0.11872, val_acc: 0.96667\n",
      "Epoch [7600/10000], loss: 0.03303 acc: 1.00000 val_loss: 0.11871, val_acc: 0.96667\n",
      "Epoch [7700/10000], loss: 0.03297 acc: 1.00000 val_loss: 0.11870, val_acc: 0.96667\n",
      "Epoch [7800/10000], loss: 0.03292 acc: 1.00000 val_loss: 0.11869, val_acc: 0.96667\n",
      "Epoch [7900/10000], loss: 0.03286 acc: 1.00000 val_loss: 0.11868, val_acc: 0.96667\n",
      "Epoch [8000/10000], loss: 0.03281 acc: 1.00000 val_loss: 0.11867, val_acc: 0.96667\n",
      "Epoch [8100/10000], loss: 0.03275 acc: 1.00000 val_loss: 0.11866, val_acc: 0.96667\n",
      "Epoch [8200/10000], loss: 0.03270 acc: 1.00000 val_loss: 0.11865, val_acc: 0.96667\n",
      "Epoch [8300/10000], loss: 0.03264 acc: 1.00000 val_loss: 0.11864, val_acc: 0.96667\n",
      "Epoch [8400/10000], loss: 0.03259 acc: 1.00000 val_loss: 0.11863, val_acc: 0.96667\n",
      "Epoch [8500/10000], loss: 0.03254 acc: 1.00000 val_loss: 0.11863, val_acc: 0.96667\n",
      "Epoch [8600/10000], loss: 0.03248 acc: 1.00000 val_loss: 0.11862, val_acc: 0.96667\n",
      "Epoch [8700/10000], loss: 0.03243 acc: 1.00000 val_loss: 0.11861, val_acc: 0.96667\n",
      "Epoch [8800/10000], loss: 0.03238 acc: 1.00000 val_loss: 0.11860, val_acc: 0.96667\n",
      "Epoch [8900/10000], loss: 0.03232 acc: 1.00000 val_loss: 0.11859, val_acc: 0.96667\n",
      "Epoch [9000/10000], loss: 0.03227 acc: 1.00000 val_loss: 0.11859, val_acc: 0.96667\n",
      "Epoch [9100/10000], loss: 0.03222 acc: 1.00000 val_loss: 0.11858, val_acc: 0.96667\n",
      "Epoch [9200/10000], loss: 0.03216 acc: 1.00000 val_loss: 0.11857, val_acc: 0.96667\n",
      "Epoch [9300/10000], loss: 0.03211 acc: 1.00000 val_loss: 0.11856, val_acc: 0.96667\n",
      "Epoch [9400/10000], loss: 0.03206 acc: 1.00000 val_loss: 0.11856, val_acc: 0.96667\n",
      "Epoch [9500/10000], loss: 0.03201 acc: 1.00000 val_loss: 0.11855, val_acc: 0.96667\n",
      "Epoch [9600/10000], loss: 0.03196 acc: 1.00000 val_loss: 0.11854, val_acc: 0.96667\n",
      "Epoch [9700/10000], loss: 0.03191 acc: 1.00000 val_loss: 0.11853, val_acc: 0.96667\n",
      "Epoch [9800/10000], loss: 0.03186 acc: 1.00000 val_loss: 0.11853, val_acc: 0.96667\n",
      "Epoch [9900/10000], loss: 0.03180 acc: 1.00000 val_loss: 0.11852, val_acc: 0.96667\n"
     ]
    }
   ],
   "source": [
    "# 繰り返し計算メインループ\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練フェーズ\n",
    "    \n",
    "    #勾配値初期化\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 予測計算\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # 損失計算\n",
    "    loss = criterion(outputs, labels1)\n",
    "\n",
    "    # 勾配計算\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータ修正\n",
    "    optimizer.step()\n",
    "\n",
    "    # 損失の保存(スカラー値の取得)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    # 予測ラベル(1 or 0)計算\n",
    "    predicted = torch.where(outputs < 0.5, 0, 1)\n",
    "    \n",
    "    # 精度計算\n",
    "    train_acc = (predicted == labels1).sum() / len(y_train)\n",
    "\n",
    "    # 予測フェーズ\n",
    "\n",
    "    # 予測計算\n",
    "    outputs_test = net(inputs_test)\n",
    "\n",
    "    # 損失計算\n",
    "    loss_test = criterion(outputs_test, labels1_test)\n",
    "\n",
    "    # 損失の保存（スカラー値の取得）\n",
    "    val_loss =  loss_test.item()\n",
    "        \n",
    "    # 予測ラベル(1 or 0)計算\n",
    "    predicted_test = torch.where(outputs_test < 0.5, 0, 1)\n",
    "\n",
    "    # 精度計算\n",
    "    val_acc = (predicted_test == labels1_test).sum() / len(y_test)\n",
    "    \n",
    "    if ( epoch % 100 == 0):\n",
    "        print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')\n",
    "        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])\n",
    "        history = np.vstack((history, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初期状態: 損失: 0.63002 精度: 0.50000\n",
      "最終状態: 損失: 0.63002 精度: 0.50000\n"
     ]
    }
   ],
   "source": [
    "#損失と精度の確認\n",
    "\n",
    "print(f'初期状態: 損失: {history[0,3]:.5f} 精度: {history[0,4]:.5f}' )\n",
    "print(f'最終状態: 損失: {history[-1,3]:.5f} 精度: {history[-1,4]:.5f}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線の表示 (損失)\n",
    "\n",
    "plt.plot(history[:,0], history[:,1], 'b', label='訓練')\n",
    "plt.plot(history[:,0], history[:,3], 'k', label='検証')\n",
    "plt.xlabel('繰り返し回数')\n",
    "plt.ylabel('損失')\n",
    "plt.title('学習曲線(損失)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線の表示 (精度)\n",
    "\n",
    "plt.plot(history[:,0], history[:,2], 'b', label='訓練')\n",
    "plt.plot(history[:,0], history[:,4], 'k', label='検証')\n",
    "plt.xlabel('繰り返し回数')\n",
    "plt.ylabel('精度')\n",
    "plt.title('学習曲線(精度)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証データを散布図用に準備\n",
    "\n",
    "x_t0 = x_test[y_test==0]\n",
    "x_t1 = x_test[y_test==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの取得\n",
    "\n",
    "bias = net.l1.bias.data.numpy()\n",
    "weight = net.l1.weight.data.numpy()\n",
    "print(f'BIAS = {bias}, WEIGHT = {weight}')\n",
    "\n",
    "# 決定境界描画用 x1の値から x2の値を計算する\n",
    "def decision(x):\n",
    "    return(-(bias + weight[0,0] * x)/ weight[0,1])\n",
    "\n",
    "# 散布図のx1の最小値と最大値\n",
    "xl = np.array([x_test[:,0].min(), x_test[:,0].max()])\n",
    "yl = decision(xl)\n",
    "\n",
    "# 結果確認\n",
    "print(f'xl = {xl}  yl = {yl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散布図表示\n",
    "plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', \n",
    "        c='b', s=50, label='class 0')\n",
    "plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', \n",
    "        c='k', s=50, label='class 1')\n",
    "\n",
    "# 決定境界直線\n",
    "plt.plot(xl, yl, c='b')\n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('sepal_width')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e48ef718162ce7f151515e54bd8e7ca7421518704dce5a2232aada539837b97f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yourenvname')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
