{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QseLEIoteKKl",
        "papermill": {
          "duration": 0.034996,
          "end_time": "2022-05-10T10:45:57.032613",
          "exception": false,
          "start_time": "2022-05-10T10:45:56.997617",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "I'm a beginner, so please be gentle with me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTQ4L0HOfIVY",
        "outputId": "7d456d0a-d478-4adf-949c-3fad75eae135"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32md:\\Users\\Takanori\\Documents\\github\\myprivate\\Kaggle\\TPS2205\\tabnet_TPS2205.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/tabnet_TPS2205.ipynb#ch0000001?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/tabnet_TPS2205.ipynb#ch0000001?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:04:47.732672Z",
          "iopub.status.busy": "2022-05-12T04:04:47.732299Z",
          "iopub.status.idle": "2022-05-12T04:04:47.762807Z",
          "shell.execute_reply": "2022-05-12T04:04:47.761969Z",
          "shell.execute_reply.started": "2022-05-12T04:04:47.732569Z"
        },
        "id": "svmkuNWQeKKy",
        "papermill": {
          "duration": 0.046307,
          "end_time": "2022-05-10T10:45:57.11412",
          "exception": false,
          "start_time": "2022-05-10T10:45:57.067813",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/colab_data/TPS2205/input')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:04:47.765189Z",
          "iopub.status.busy": "2022-05-12T04:04:47.764647Z",
          "iopub.status.idle": "2022-05-12T04:04:58.113202Z",
          "shell.execute_reply": "2022-05-12T04:04:58.111958Z",
          "shell.execute_reply.started": "2022-05-12T04:04:47.765148Z"
        },
        "id": "QkhrR0ljeKK3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pytorch-tabnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCvm63hOeKK5",
        "papermill": {
          "duration": 0.036574,
          "end_time": "2022-05-10T10:45:57.186644",
          "exception": false,
          "start_time": "2022-05-10T10:45:57.15007",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# DataSet & Library Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:04:58.115273Z",
          "iopub.status.busy": "2022-05-12T04:04:58.114924Z",
          "iopub.status.idle": "2022-05-12T04:04:59.452121Z",
          "shell.execute_reply": "2022-05-12T04:04:59.451063Z",
          "shell.execute_reply.started": "2022-05-12T04:04:58.115227Z"
        },
        "id": "uNxJpHYieKK7",
        "papermill": {
          "duration": 1.221023,
          "end_time": "2022-05-10T10:45:58.442806",
          "exception": false,
          "start_time": "2022-05-10T10:45:57.221783",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time, gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:04:59.455173Z",
          "iopub.status.busy": "2022-05-12T04:04:59.454621Z",
          "iopub.status.idle": "2022-05-12T04:05:00.847747Z",
          "shell.execute_reply": "2022-05-12T04:05:00.846819Z",
          "shell.execute_reply.started": "2022-05-12T04:04:59.455133Z"
        },
        "id": "CixUqcvieKK9",
        "papermill": {
          "duration": 1.605567,
          "end_time": "2022-05-10T10:46:00.102892",
          "exception": false,
          "start_time": "2022-05-10T10:45:58.497325",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:00.849669Z",
          "iopub.status.busy": "2022-05-12T04:05:00.849265Z",
          "iopub.status.idle": "2022-05-12T04:05:00.857931Z",
          "shell.execute_reply": "2022-05-12T04:05:00.857081Z",
          "shell.execute_reply.started": "2022-05-12T04:05:00.849623Z"
        },
        "id": "FKfLBitaeKK_",
        "papermill": {
          "duration": 0.097454,
          "end_time": "2022-05-10T10:46:00.234896",
          "exception": false,
          "start_time": "2022-05-10T10:46:00.137442",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    batch_size = 8_192*8\n",
        "    virtual_batch_size = 1_024\n",
        "    epochs = 10_000  # 最大のepoch数\n",
        "    folds = 10  # クロスバリデーションにおけるfold数\n",
        "    seed = 42  # TODO 機能させる\n",
        "    target = 'target'  # 目標変数\n",
        "    model_path = \"models\"  # モデルを保存するフォルダ\n",
        "    test_pred = []  # 各foldで作ったモデル別の、testを予測した結果のカラム名\n",
        "    pred = 'pred'  # trainを予測した結果のカラム名\n",
        "    increase_batch = 9999  # 何epoch間lossが改善しなかったbatchを増やすか\n",
        "    early_stopping = 100  # 何epochでearly stoppingをするか\n",
        "    lr = 1e-1  # optimizerの学習率\n",
        "    min_lr = 1e-3  # 学習率の最低値\n",
        "    lr_patience = 10  # 学習率を引き下げる間隔\n",
        "    lr_factor = 0.5  # lrを何倍にするか  # TODO:たぶん消していい\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # TODO:たぶん消していい\n",
        "    delta = 1e-4  # AUC等がいくつ変わらなかったら学習などをやめるか"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:00.859591Z",
          "iopub.status.busy": "2022-05-12T04:05:00.859314Z",
          "iopub.status.idle": "2022-05-12T04:05:00.870285Z",
          "shell.execute_reply": "2022-05-12T04:05:00.869533Z",
          "shell.execute_reply.started": "2022-05-12T04:05:00.859541Z"
        },
        "id": "zSUIoBk4eKLE",
        "papermill": {
          "duration": 0.040032,
          "end_time": "2022-05-10T10:46:00.308537",
          "exception": false,
          "start_time": "2022-05-10T10:46:00.268505",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "os.makedirs(CFG.model_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXCUziDbeKLH",
        "papermill": {
          "duration": 0.033438,
          "end_time": "2022-05-10T10:46:00.375455",
          "exception": false,
          "start_time": "2022-05-10T10:46:00.342017",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## FE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:00.872263Z",
          "iopub.status.busy": "2022-05-12T04:05:00.871692Z",
          "iopub.status.idle": "2022-05-12T04:05:00.882531Z",
          "shell.execute_reply": "2022-05-12T04:05:00.881748Z",
          "shell.execute_reply.started": "2022-05-12T04:05:00.872226Z"
        },
        "id": "kZqMVtQueKLK",
        "outputId": "80d0495d-85b3-4455-fa5b-4d0f5d589825",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FE\n"
          ]
        }
      ],
      "source": [
        "print('FE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:00.884080Z",
          "iopub.status.busy": "2022-05-12T04:05:00.883695Z",
          "iopub.status.idle": "2022-05-12T04:05:16.371910Z",
          "shell.execute_reply": "2022-05-12T04:05:16.370590Z",
          "shell.execute_reply.started": "2022-05-12T04:05:00.884052Z"
        },
        "id": "rO-oHXA-eKLQ",
        "outputId": "97d3bf9b-e49d-47e0-e648-ccb535e85f55",
        "papermill": {
          "duration": 12.675348,
          "end_time": "2022-05-10T10:46:13.084261",
          "exception": false,
          "start_time": "2022-05-10T10:46:00.408913",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d8568798-f54a-4676-adc7-484dfe2a702f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>...</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_27</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.373246</td>\n",
              "      <td>0.238887</td>\n",
              "      <td>-0.243376</td>\n",
              "      <td>0.567405</td>\n",
              "      <td>-0.647715</td>\n",
              "      <td>0.839326</td>\n",
              "      <td>0.113133</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.540739</td>\n",
              "      <td>0.766952</td>\n",
              "      <td>-2.730628</td>\n",
              "      <td>-0.208177</td>\n",
              "      <td>1.363402</td>\n",
              "      <td>ABABDADBAB</td>\n",
              "      <td>67.609153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.697021</td>\n",
              "      <td>-1.710322</td>\n",
              "      <td>-2.230332</td>\n",
              "      <td>-0.545661</td>\n",
              "      <td>1.113173</td>\n",
              "      <td>-1.552175</td>\n",
              "      <td>0.447825</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2.278315</td>\n",
              "      <td>-0.633658</td>\n",
              "      <td>-1.217077</td>\n",
              "      <td>-3.782194</td>\n",
              "      <td>-0.058316</td>\n",
              "      <td>ACACCADCEB</td>\n",
              "      <td>377.096415</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.681726</td>\n",
              "      <td>0.616746</td>\n",
              "      <td>-1.027689</td>\n",
              "      <td>0.810492</td>\n",
              "      <td>-0.609086</td>\n",
              "      <td>0.113965</td>\n",
              "      <td>-0.708660</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.385775</td>\n",
              "      <td>-0.520558</td>\n",
              "      <td>-0.009121</td>\n",
              "      <td>2.788536</td>\n",
              "      <td>-3.703488</td>\n",
              "      <td>AAAEABCKAD</td>\n",
              "      <td>-195.599702</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.118172</td>\n",
              "      <td>-0.587835</td>\n",
              "      <td>-0.804638</td>\n",
              "      <td>2.086822</td>\n",
              "      <td>0.371005</td>\n",
              "      <td>-0.128831</td>\n",
              "      <td>-0.282575</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.572594</td>\n",
              "      <td>-1.653213</td>\n",
              "      <td>1.686035</td>\n",
              "      <td>-2.533098</td>\n",
              "      <td>-0.608601</td>\n",
              "      <td>BDBBAACBCB</td>\n",
              "      <td>210.826205</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.148481</td>\n",
              "      <td>-0.176567</td>\n",
              "      <td>-0.664871</td>\n",
              "      <td>-1.101343</td>\n",
              "      <td>0.467875</td>\n",
              "      <td>0.500117</td>\n",
              "      <td>0.407515</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.912929</td>\n",
              "      <td>-1.430366</td>\n",
              "      <td>2.127649</td>\n",
              "      <td>-3.306784</td>\n",
              "      <td>4.371371</td>\n",
              "      <td>BDBCBBCHFE</td>\n",
              "      <td>-217.211798</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8568798-f54a-4676-adc7-484dfe2a702f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8568798-f54a-4676-adc7-484dfe2a702f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8568798-f54a-4676-adc7-484dfe2a702f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id      f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
              "0   0 -1.373246  0.238887 -0.243376  0.567405 -0.647715  0.839326  0.113133   \n",
              "1   1  1.697021 -1.710322 -2.230332 -0.545661  1.113173 -1.552175  0.447825   \n",
              "2   2  1.681726  0.616746 -1.027689  0.810492 -0.609086  0.113965 -0.708660   \n",
              "3   3 -0.118172 -0.587835 -0.804638  2.086822  0.371005 -0.128831 -0.282575   \n",
              "4   4  1.148481 -0.176567 -0.664871 -1.101343  0.467875  0.500117  0.407515   \n",
              "\n",
              "   f_07  f_08  ...      f_22      f_23      f_24      f_25      f_26  \\\n",
              "0     1     5  ... -2.540739  0.766952 -2.730628 -0.208177  1.363402   \n",
              "1     1     3  ...  2.278315 -0.633658 -1.217077 -3.782194 -0.058316   \n",
              "2     1     0  ... -1.385775 -0.520558 -0.009121  2.788536 -3.703488   \n",
              "3     3     2  ...  0.572594 -1.653213  1.686035 -2.533098 -0.608601   \n",
              "4     3     3  ... -3.912929 -1.430366  2.127649 -3.306784  4.371371   \n",
              "\n",
              "         f_27        f_28  f_29  f_30  target  \n",
              "0  ABABDADBAB   67.609153     0     0       0  \n",
              "1  ACACCADCEB  377.096415     0     0       1  \n",
              "2  AAAEABCKAD -195.599702     0     2       1  \n",
              "3  BDBBAACBCB  210.826205     0     0       1  \n",
              "4  BDBCBBCHFE -217.211798     0     1       1  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-09b27195-191d-4edb-8bee-26a7f946324b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>...</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_27</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>900000</td>\n",
              "      <td>0.442517</td>\n",
              "      <td>0.174380</td>\n",
              "      <td>-0.999816</td>\n",
              "      <td>0.762741</td>\n",
              "      <td>0.186778</td>\n",
              "      <td>-1.074775</td>\n",
              "      <td>0.501888</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.006400</td>\n",
              "      <td>-1.193879</td>\n",
              "      <td>-2.435736</td>\n",
              "      <td>-2.427430</td>\n",
              "      <td>-1.966887</td>\n",
              "      <td>5.734205</td>\n",
              "      <td>BAAABADLAC</td>\n",
              "      <td>99.478419</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>900001</td>\n",
              "      <td>-0.605598</td>\n",
              "      <td>-0.305715</td>\n",
              "      <td>0.627667</td>\n",
              "      <td>-0.578898</td>\n",
              "      <td>-1.750931</td>\n",
              "      <td>1.355550</td>\n",
              "      <td>-0.190911</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2.382405</td>\n",
              "      <td>0.149442</td>\n",
              "      <td>1.883322</td>\n",
              "      <td>-2.848714</td>\n",
              "      <td>-0.725155</td>\n",
              "      <td>3.194219</td>\n",
              "      <td>AFABBAEGCB</td>\n",
              "      <td>-65.993825</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>900002</td>\n",
              "      <td>0.303990</td>\n",
              "      <td>2.445110</td>\n",
              "      <td>0.246515</td>\n",
              "      <td>0.818248</td>\n",
              "      <td>0.359731</td>\n",
              "      <td>-1.331845</td>\n",
              "      <td>1.358622</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.026098</td>\n",
              "      <td>1.312277</td>\n",
              "      <td>-5.157192</td>\n",
              "      <td>1.714005</td>\n",
              "      <td>0.585032</td>\n",
              "      <td>0.066898</td>\n",
              "      <td>BBACABBKEE</td>\n",
              "      <td>-87.405622</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>900003</td>\n",
              "      <td>0.154053</td>\n",
              "      <td>0.260126</td>\n",
              "      <td>-1.367092</td>\n",
              "      <td>-0.093175</td>\n",
              "      <td>-1.111034</td>\n",
              "      <td>-0.948481</td>\n",
              "      <td>1.119220</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.594532</td>\n",
              "      <td>-3.939475</td>\n",
              "      <td>1.754570</td>\n",
              "      <td>-2.364007</td>\n",
              "      <td>-1.003320</td>\n",
              "      <td>3.893099</td>\n",
              "      <td>AEBEAACQCC</td>\n",
              "      <td>-281.293460</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>900004</td>\n",
              "      <td>-1.651904</td>\n",
              "      <td>-0.424266</td>\n",
              "      <td>-0.667356</td>\n",
              "      <td>-0.322124</td>\n",
              "      <td>-0.089462</td>\n",
              "      <td>0.181705</td>\n",
              "      <td>1.784983</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.084906</td>\n",
              "      <td>-0.985736</td>\n",
              "      <td>-0.130467</td>\n",
              "      <td>-3.557893</td>\n",
              "      <td>1.210687</td>\n",
              "      <td>1.861884</td>\n",
              "      <td>AEBBBBDABF</td>\n",
              "      <td>25.629415</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09b27195-191d-4edb-8bee-26a7f946324b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09b27195-191d-4edb-8bee-26a7f946324b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09b27195-191d-4edb-8bee-26a7f946324b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id      f_00      f_01      f_02      f_03      f_04      f_05  \\\n",
              "0  900000  0.442517  0.174380 -0.999816  0.762741  0.186778 -1.074775   \n",
              "1  900001 -0.605598 -0.305715  0.627667 -0.578898 -1.750931  1.355550   \n",
              "2  900002  0.303990  2.445110  0.246515  0.818248  0.359731 -1.331845   \n",
              "3  900003  0.154053  0.260126 -1.367092 -0.093175 -1.111034 -0.948481   \n",
              "4  900004 -1.651904 -0.424266 -0.667356 -0.322124 -0.089462  0.181705   \n",
              "\n",
              "       f_06  f_07  f_08  ...      f_21      f_22      f_23      f_24  \\\n",
              "0  0.501888     6     6  ... -1.006400 -1.193879 -2.435736 -2.427430   \n",
              "1 -0.190911     1     3  ...  2.382405  0.149442  1.883322 -2.848714   \n",
              "2  1.358622     3     3  ... -7.026098  1.312277 -5.157192  1.714005   \n",
              "3  1.119220     0     0  ... -0.594532 -3.939475  1.754570 -2.364007   \n",
              "4  1.784983     2     2  ...  0.084906 -0.985736 -0.130467 -3.557893   \n",
              "\n",
              "       f_25      f_26        f_27        f_28  f_29  f_30  \n",
              "0 -1.966887  5.734205  BAAABADLAC   99.478419     0     0  \n",
              "1 -0.725155  3.194219  AFABBAEGCB  -65.993825     1     0  \n",
              "2  0.585032  0.066898  BBACABBKEE  -87.405622     0     1  \n",
              "3 -1.003320  3.893099  AEBEAACQCC -281.293460     0     0  \n",
              "4  1.210687  1.861884  AEBBBBDABF   25.629415     0     2  \n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2769e06c-acda-402c-b7f3-1684d658638b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>900000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>900001</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>900002</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>900003</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>900004</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2769e06c-acda-402c-b7f3-1684d658638b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2769e06c-acda-402c-b7f3-1684d658638b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2769e06c-acda-402c-b7f3-1684d658638b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id  target\n",
              "0  900000     0.5\n",
              "1  900001     0.5\n",
              "2  900002     0.5\n",
              "3  900003     0.5\n",
              "4  900004     0.5"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# データセット読み込み\n",
        "df_train = pd.read_csv(\"../input/tabular-playground-series-may-2022/train.csv\")\n",
        "df_test = pd.read_csv(\"../input/tabular-playground-series-may-2022/test.csv\")\n",
        "df_sub = pd.read_csv(\"../input/tabular-playground-series-may-2022/sample_submission.csv\")\n",
        "display(df_train.head())\n",
        "display(df_test.head())\n",
        "display(df_sub.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:16.373791Z",
          "iopub.status.busy": "2022-05-12T04:05:16.373284Z",
          "iopub.status.idle": "2022-05-12T04:05:16.382389Z",
          "shell.execute_reply": "2022-05-12T04:05:16.381475Z",
          "shell.execute_reply.started": "2022-05-12T04:05:16.373739Z"
        },
        "id": "7Oa__3skeKLS",
        "outputId": "4f048df5-1517-46c2-fec3-d2c9c5e91c0f",
        "papermill": {
          "duration": 0.046606,
          "end_time": "2022-05-10T10:46:13.167428",
          "exception": false,
          "start_time": "2022-05-10T10:46:13.120822",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(900000, 33)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(700000, 32)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df_train.shape)\n",
        "display(df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:16.385856Z",
          "iopub.status.busy": "2022-05-12T04:05:16.385244Z",
          "iopub.status.idle": "2022-05-12T04:05:16.654371Z",
          "shell.execute_reply": "2022-05-12T04:05:16.653506Z",
          "shell.execute_reply.started": "2022-05-12T04:05:16.385782Z"
        },
        "id": "SaH9ODnOeKLU",
        "papermill": {
          "duration": 0.345384,
          "end_time": "2022-05-10T10:46:41.120719",
          "exception": false,
          "start_time": "2022-05-10T10:46:40.775335",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat([df_train, df_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:16.656261Z",
          "iopub.status.busy": "2022-05-12T04:05:16.655972Z",
          "iopub.status.idle": "2022-05-12T04:05:16.729210Z",
          "shell.execute_reply": "2022-05-12T04:05:16.728463Z",
          "shell.execute_reply.started": "2022-05-12T04:05:16.656222Z"
        },
        "id": "B5mURhGHeKLV",
        "papermill": {
          "duration": 0.12841,
          "end_time": "2022-05-10T10:46:41.292354",
          "exception": false,
          "start_time": "2022-05-10T10:46:41.163944",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_all['i_02_21'] = (df_all.f_21 + df_all.f_02 > 5.2).astype(int) - (df_all.f_21 + df_all.f_02 < -5.3).astype(int)\n",
        "df_all['i_05_22'] = (df_all.f_22 + df_all.f_05 > 5.1).astype(int) - (df_all.f_22 + df_all.f_05 < -5.4).astype(int)\n",
        "i_00_01_26 = df_all.f_00 + df_all.f_01 + df_all.f_26\n",
        "df_all['i_00_01_26'] = (i_00_01_26 > 5.0).astype(int) - (i_00_01_26 < -5.0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:16.730589Z",
          "iopub.status.busy": "2022-05-12T04:05:16.730370Z",
          "iopub.status.idle": "2022-05-12T04:05:17.287219Z",
          "shell.execute_reply": "2022-05-12T04:05:17.286252Z",
          "shell.execute_reply.started": "2022-05-12T04:05:16.730565Z"
        },
        "id": "GU9m7rKyeKLW",
        "papermill": {
          "duration": 0.492669,
          "end_time": "2022-05-10T10:46:41.827176",
          "exception": false,
          "start_time": "2022-05-10T10:46:41.334507",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 不要データを削除\n",
        "df_all.drop(['id'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:17.288626Z",
          "iopub.status.busy": "2022-05-12T04:05:17.288389Z",
          "iopub.status.idle": "2022-05-12T04:05:17.430558Z",
          "shell.execute_reply": "2022-05-12T04:05:17.429836Z",
          "shell.execute_reply.started": "2022-05-12T04:05:17.288598Z"
        },
        "id": "ritWuq4IeKLX",
        "papermill": {
          "duration": 0.297427,
          "end_time": "2022-05-10T10:46:42.173145",
          "exception": false,
          "start_time": "2022-05-10T10:46:41.875718",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# NA埋め\n",
        "df_all.fillna(df_all.mean(numeric_only=True), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:05:17.431933Z",
          "iopub.status.busy": "2022-05-12T04:05:17.431701Z",
          "iopub.status.idle": "2022-05-12T04:07:14.616677Z",
          "shell.execute_reply": "2022-05-12T04:07:14.616021Z",
          "shell.execute_reply.started": "2022-05-12T04:05:17.431909Z"
        },
        "id": "9i1fcBnjeKLZ",
        "papermill": {
          "duration": 88.711742,
          "end_time": "2022-05-10T10:48:10.967544",
          "exception": false,
          "start_time": "2022-05-10T10:46:42.255802",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# f_27を分解\n",
        "def splitter(text):\n",
        "    arr = tuple(text)\n",
        "    return arr\n",
        "\n",
        "column_list = ['f_27_' + str(i) for i in range(10)]\n",
        "df_all[column_list] = df_all.apply(lambda x: splitter(x['f_27']), axis=1, result_type='expand')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:07:14.619149Z",
          "iopub.status.busy": "2022-05-12T04:07:14.618271Z",
          "iopub.status.idle": "2022-05-12T04:07:26.910297Z",
          "shell.execute_reply": "2022-05-12T04:07:26.908760Z",
          "shell.execute_reply.started": "2022-05-12T04:07:14.619106Z"
        },
        "id": "qrpyxe7XeKLa",
        "papermill": {
          "duration": 9.300861,
          "end_time": "2022-05-10T10:48:20.404086",
          "exception": false,
          "start_time": "2022-05-10T10:48:11.103225",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# f_27_0~9を数値に\n",
        "for column in column_list:\n",
        "    df_all[column] = df_all[column].apply(lambda x: ord(x)-ord('A'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:07:26.913265Z",
          "iopub.status.busy": "2022-05-12T04:07:26.912946Z",
          "iopub.status.idle": "2022-05-12T04:07:27.640798Z",
          "shell.execute_reply": "2022-05-12T04:07:27.639949Z",
          "shell.execute_reply.started": "2022-05-12T04:07:26.913227Z"
        },
        "id": "9NpfGP1weKLb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# f_27_0~9の各種統計量を求める\n",
        "\n",
        "df_all['f_27_mean'] =  df_all[column_list].mean(axis=1)\n",
        "df_all['f_27_std'] =  df_all[column_list].std(axis=1)\n",
        "df_all['f_27_sm'] = df_all['f_27_std'] / (df_all['f_27_mean']+0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:07:27.642228Z",
          "iopub.status.busy": "2022-05-12T04:07:27.641986Z",
          "iopub.status.idle": "2022-05-12T04:07:37.017150Z",
          "shell.execute_reply": "2022-05-12T04:07:37.016385Z",
          "shell.execute_reply.started": "2022-05-12T04:07:27.642201Z"
        },
        "id": "uf05RBI4eKLc",
        "papermill": {
          "duration": 10.44301,
          "end_time": "2022-05-10T11:03:08.484982",
          "exception": false,
          "start_time": "2022-05-10T11:02:58.041972",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# f_27に出てくる文字が、何種類あるかを返す\n",
        "def calc_cnt_of(txt):\n",
        "    from collections import Counter\n",
        "    cnt_of = Counter(list(txt))\n",
        "    return len(cnt_of)\n",
        "\n",
        "df_all['len_cnt_of'] = df_all['f_27'].apply(calc_cnt_of)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:07:37.018784Z",
          "iopub.status.busy": "2022-05-12T04:07:37.018533Z",
          "iopub.status.idle": "2022-05-12T04:10:07.590429Z",
          "shell.execute_reply": "2022-05-12T04:10:07.589185Z",
          "shell.execute_reply.started": "2022-05-12T04:07:37.018753Z"
        },
        "id": "CN7iK8H_eKLd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# f_27に各文字が何回出てくるかを数える\n",
        "# これは明らかにカテゴリ変数ではないので、floatで管理する\n",
        "def calc_char_cnt(txt):\n",
        "    from collections import Counter\n",
        "    import string\n",
        "    cnt_of = Counter(list(txt))\n",
        "    ret = []\n",
        "    for c in string.ascii_uppercase:\n",
        "        ret.append(float(cnt_of[c]))\n",
        "    return tuple(ret)\n",
        "\n",
        "import string\n",
        "column_list = []\n",
        "for c in string.ascii_uppercase:\n",
        "    column_list.append('f_27_cnt_' + c)\n",
        "\n",
        "df_all[column_list] = df_all.apply(lambda x: calc_char_cnt(x['f_27']), axis=1, result_type='expand')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:07.592573Z",
          "iopub.status.busy": "2022-05-12T04:10:07.592243Z",
          "iopub.status.idle": "2022-05-12T04:10:08.335193Z",
          "shell.execute_reply": "2022-05-12T04:10:08.334189Z",
          "shell.execute_reply.started": "2022-05-12T04:10:07.592529Z"
        },
        "id": "KppLCCv2eKLf",
        "papermill": {
          "duration": 2.852553,
          "end_time": "2022-05-10T11:05:11.969954",
          "exception": false,
          "start_time": "2022-05-10T11:05:09.117401",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 不要になったカテゴリ変数を削除\n",
        "df_all.drop(['f_27'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:08.337597Z",
          "iopub.status.busy": "2022-05-12T04:10:08.337252Z",
          "iopub.status.idle": "2022-05-12T04:10:11.501398Z",
          "shell.execute_reply": "2022-05-12T04:10:11.500533Z",
          "shell.execute_reply.started": "2022-05-12T04:10:08.337553Z"
        },
        "id": "MzDJDCZ1eKLf",
        "outputId": "4e7d41ad-d575-435e-b0bd-5cd3d3ec9712",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f_07 17\n",
            "f_08 16\n",
            "f_09 16\n",
            "f_10 16\n",
            "f_11 15\n",
            "f_12 17\n",
            "f_13 14\n",
            "f_14 14\n",
            "f_15 15\n",
            "f_16 16\n",
            "f_17 15\n",
            "f_18 14\n",
            "f_29 2\n",
            "f_30 3\n",
            "i_02_21 3\n",
            "i_05_22 3\n",
            "i_00_01_26 3\n",
            "f_27_0 2\n",
            "f_27_1 15\n",
            "f_27_2 2\n",
            "f_27_3 15\n",
            "f_27_4 15\n",
            "f_27_5 2\n",
            "f_27_6 15\n",
            "f_27_7 20\n",
            "f_27_8 15\n",
            "f_27_9 15\n",
            "len_cnt_of 9\n"
          ]
        }
      ],
      "source": [
        "# カテゴリ変数を0-indexedに変換\n",
        "categorical_columns = []\n",
        "categorical_dims =  {}\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "for col in df_all.columns:\n",
        "    if col == CFG.target:\n",
        "        continue\n",
        "    # if df_all[col].dtype == 'object' or len(df_all[col].unique()) < 200:\n",
        "    if df_all[col].dtype == 'object' or df_all[col].dtype == 'int64':\n",
        "        print(col, df_all[col].nunique())\n",
        "        l_enc = LabelEncoder()\n",
        "        df_all[col] = l_enc.fit_transform(df_all[col].values)\n",
        "        categorical_columns.append(col)\n",
        "        categorical_dims[col] = len(l_enc.classes_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:11.502793Z",
          "iopub.status.busy": "2022-05-12T04:10:11.502542Z",
          "iopub.status.idle": "2022-05-12T04:10:11.509309Z",
          "shell.execute_reply": "2022-05-12T04:10:11.508618Z",
          "shell.execute_reply.started": "2022-05-12T04:10:11.502767Z"
        },
        "id": "KCEsYf-7eKLh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "unused_feat = []\n",
        "\n",
        "features = [ col for col in df_all.columns if col not in unused_feat+[CFG.target]] \n",
        "\n",
        "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
        "\n",
        "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:11.511142Z",
          "iopub.status.busy": "2022-05-12T04:10:11.510757Z",
          "iopub.status.idle": "2022-05-12T04:10:17.514425Z",
          "shell.execute_reply": "2022-05-12T04:10:17.513701Z",
          "shell.execute_reply.started": "2022-05-12T04:10:11.511079Z"
        },
        "id": "LCAtqNK3eKLi",
        "outputId": "fc6a1cbd-65e5-48b5-d32d-3a7bd3786423",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f_07\n",
            "16\n",
            "f_08\n",
            "15\n",
            "f_09\n",
            "15\n",
            "f_10\n",
            "15\n",
            "f_11\n",
            "14\n",
            "f_12\n",
            "16\n",
            "f_13\n",
            "13\n",
            "f_14\n",
            "13\n",
            "f_15\n",
            "14\n",
            "f_16\n",
            "15\n",
            "f_17\n",
            "14\n",
            "f_18\n",
            "13\n",
            "f_29\n",
            "1\n",
            "f_30\n",
            "2\n",
            "i_02_21\n",
            "2\n",
            "i_05_22\n",
            "2\n",
            "i_00_01_26\n",
            "2\n",
            "f_27_0\n",
            "1\n",
            "f_27_1\n",
            "14\n",
            "f_27_2\n",
            "1\n",
            "f_27_3\n",
            "14\n",
            "f_27_4\n",
            "14\n",
            "f_27_5\n",
            "1\n",
            "f_27_6\n",
            "14\n",
            "f_27_7\n",
            "19\n",
            "f_27_8\n",
            "14\n",
            "f_27_9\n",
            "14\n",
            "len_cnt_of\n",
            "8\n"
          ]
        }
      ],
      "source": [
        "# ここまででFEは完了\n",
        "# 省メモリ化\n",
        "for col in df_all.columns:\n",
        "    if df_all[col].dtype == 'float64':\n",
        "        df_all[col] = df_all[col].astype('float32')\n",
        "    elif df_all[col].dtype == 'int64':\n",
        "        absmax = df_all[col].abs().max()\n",
        "        print(col)\n",
        "        print(absmax)\n",
        "        if absmax < (1 << 7):\n",
        "            df_all[col] = df_all[col].astype('int8')\n",
        "        elif absmax < (1 << 15):\n",
        "            df_all[col] = df_all[col].astype('int16')\n",
        "        elif absmax < (1 << 31):\n",
        "            df_all[col] = df_all[col].astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:17.517080Z",
          "iopub.status.busy": "2022-05-12T04:10:17.516500Z",
          "iopub.status.idle": "2022-05-12T04:10:17.526176Z",
          "shell.execute_reply": "2022-05-12T04:10:17.525211Z",
          "shell.execute_reply.started": "2022-05-12T04:10:17.517040Z"
        },
        "id": "LfI5ICH4eKLj",
        "outputId": "bcf794d1-ca06-4785-c8c3-d782a50007fb",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "f_00          float32\n",
              "f_01          float32\n",
              "f_02          float32\n",
              "f_03          float32\n",
              "f_04          float32\n",
              "               ...   \n",
              "f_27_cnt_V    float32\n",
              "f_27_cnt_W    float32\n",
              "f_27_cnt_X    float32\n",
              "f_27_cnt_Y    float32\n",
              "f_27_cnt_Z    float32\n",
              "Length: 74, dtype: object"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_all.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:17.527847Z",
          "iopub.status.busy": "2022-05-12T04:10:17.527558Z",
          "iopub.status.idle": "2022-05-12T04:10:18.106051Z",
          "shell.execute_reply": "2022-05-12T04:10:18.105176Z",
          "shell.execute_reply.started": "2022-05-12T04:10:17.527816Z"
        },
        "id": "rdzQ2gNjeKLk",
        "outputId": "c69c3556-e1de-4c28-a373-c56ec79ed149",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-58dcbf88-e5e0-4ea1-aebe-e20b968aa9d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>...</th>\n",
              "      <th>f_27_cnt_R</th>\n",
              "      <th>f_27_cnt_S</th>\n",
              "      <th>f_27_cnt_T</th>\n",
              "      <th>f_27_cnt_U</th>\n",
              "      <th>f_27_cnt_V</th>\n",
              "      <th>f_27_cnt_W</th>\n",
              "      <th>f_27_cnt_X</th>\n",
              "      <th>f_27_cnt_Y</th>\n",
              "      <th>f_27_cnt_Z</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.373246</td>\n",
              "      <td>0.238887</td>\n",
              "      <td>-0.243376</td>\n",
              "      <td>0.567405</td>\n",
              "      <td>-0.647715</td>\n",
              "      <td>0.839326</td>\n",
              "      <td>0.113133</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.697021</td>\n",
              "      <td>-1.710322</td>\n",
              "      <td>-2.230332</td>\n",
              "      <td>-0.545661</td>\n",
              "      <td>1.113173</td>\n",
              "      <td>-1.552175</td>\n",
              "      <td>0.447825</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.681726</td>\n",
              "      <td>0.616746</td>\n",
              "      <td>-1.027689</td>\n",
              "      <td>0.810492</td>\n",
              "      <td>-0.609086</td>\n",
              "      <td>0.113965</td>\n",
              "      <td>-0.708660</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.118172</td>\n",
              "      <td>-0.587835</td>\n",
              "      <td>-0.804638</td>\n",
              "      <td>2.086822</td>\n",
              "      <td>0.371005</td>\n",
              "      <td>-0.128831</td>\n",
              "      <td>-0.282575</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.148481</td>\n",
              "      <td>-0.176567</td>\n",
              "      <td>-0.664871</td>\n",
              "      <td>-1.101343</td>\n",
              "      <td>0.467875</td>\n",
              "      <td>0.500117</td>\n",
              "      <td>0.407515</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699995</th>\n",
              "      <td>0.640110</td>\n",
              "      <td>0.897808</td>\n",
              "      <td>-0.523956</td>\n",
              "      <td>1.563760</td>\n",
              "      <td>-0.092281</td>\n",
              "      <td>-0.610867</td>\n",
              "      <td>0.535426</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.486488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699996</th>\n",
              "      <td>-0.191771</td>\n",
              "      <td>-0.035246</td>\n",
              "      <td>-0.118533</td>\n",
              "      <td>0.584750</td>\n",
              "      <td>2.126976</td>\n",
              "      <td>0.568659</td>\n",
              "      <td>-0.052663</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.486488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699997</th>\n",
              "      <td>-0.331704</td>\n",
              "      <td>-0.328845</td>\n",
              "      <td>-1.185503</td>\n",
              "      <td>1.022128</td>\n",
              "      <td>-0.483099</td>\n",
              "      <td>-0.107146</td>\n",
              "      <td>-0.968281</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.486488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699998</th>\n",
              "      <td>-2.031073</td>\n",
              "      <td>-1.238398</td>\n",
              "      <td>0.964699</td>\n",
              "      <td>-1.045950</td>\n",
              "      <td>0.906064</td>\n",
              "      <td>0.634301</td>\n",
              "      <td>-0.707474</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.486488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699999</th>\n",
              "      <td>-0.085906</td>\n",
              "      <td>-0.002124</td>\n",
              "      <td>2.227375</td>\n",
              "      <td>0.217145</td>\n",
              "      <td>3.179153</td>\n",
              "      <td>-1.660188</td>\n",
              "      <td>0.891989</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.486488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 74 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58dcbf88-e5e0-4ea1-aebe-e20b968aa9d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58dcbf88-e5e0-4ea1-aebe-e20b968aa9d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58dcbf88-e5e0-4ea1-aebe-e20b968aa9d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
              "0      -1.373246  0.238887 -0.243376  0.567405 -0.647715  0.839326  0.113133   \n",
              "1       1.697021 -1.710322 -2.230332 -0.545661  1.113173 -1.552175  0.447825   \n",
              "2       1.681726  0.616746 -1.027689  0.810492 -0.609086  0.113965 -0.708660   \n",
              "3      -0.118172 -0.587835 -0.804638  2.086822  0.371005 -0.128831 -0.282575   \n",
              "4       1.148481 -0.176567 -0.664871 -1.101343  0.467875  0.500117  0.407515   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "699995  0.640110  0.897808 -0.523956  1.563760 -0.092281 -0.610867  0.535426   \n",
              "699996 -0.191771 -0.035246 -0.118533  0.584750  2.126976  0.568659 -0.052663   \n",
              "699997 -0.331704 -0.328845 -1.185503  1.022128 -0.483099 -0.107146 -0.968281   \n",
              "699998 -2.031073 -1.238398  0.964699 -1.045950  0.906064  0.634301 -0.707474   \n",
              "699999 -0.085906 -0.002124  2.227375  0.217145  3.179153 -1.660188  0.891989   \n",
              "\n",
              "        f_07  f_08  f_09  ...  f_27_cnt_R  f_27_cnt_S  f_27_cnt_T  f_27_cnt_U  \\\n",
              "0          1     5     1  ...         0.0         0.0         0.0         0.0   \n",
              "1          1     3     4  ...         0.0         0.0         0.0         0.0   \n",
              "2          1     0     2  ...         0.0         0.0         0.0         0.0   \n",
              "3          3     2     1  ...         0.0         0.0         0.0         0.0   \n",
              "4          3     3     0  ...         0.0         0.0         0.0         0.0   \n",
              "...      ...   ...   ...  ...         ...         ...         ...         ...   \n",
              "699995     0     1     6  ...         0.0         0.0         0.0         0.0   \n",
              "699996     4     3     4  ...         0.0         0.0         0.0         0.0   \n",
              "699997     1     1     2  ...         0.0         0.0         0.0         0.0   \n",
              "699998     5     1     1  ...         0.0         0.0         0.0         0.0   \n",
              "699999     0     3     4  ...         0.0         0.0         0.0         0.0   \n",
              "\n",
              "        f_27_cnt_V  f_27_cnt_W  f_27_cnt_X  f_27_cnt_Y  f_27_cnt_Z    target  \n",
              "0              0.0         0.0         0.0         0.0         0.0  0.000000  \n",
              "1              0.0         0.0         0.0         0.0         0.0  1.000000  \n",
              "2              0.0         0.0         0.0         0.0         0.0  1.000000  \n",
              "3              0.0         0.0         0.0         0.0         0.0  1.000000  \n",
              "4              0.0         0.0         0.0         0.0         0.0  1.000000  \n",
              "...            ...         ...         ...         ...         ...       ...  \n",
              "699995         0.0         0.0         0.0         0.0         0.0  0.486488  \n",
              "699996         0.0         0.0         0.0         0.0         0.0  0.486488  \n",
              "699997         0.0         0.0         0.0         0.0         0.0  0.486488  \n",
              "699998         0.0         0.0         0.0         0.0         0.0  0.486488  \n",
              "699999         0.0         0.0         0.0         0.0         0.0  0.486488  \n",
              "\n",
              "[1600000 rows x 74 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "all_features = df_all.columns.tolist()\n",
        "all_features.remove(CFG.target)\n",
        "df_all.reindex(all_features + [CFG.target], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:18.108232Z",
          "iopub.status.busy": "2022-05-12T04:10:18.107879Z",
          "iopub.status.idle": "2022-05-12T04:10:18.168715Z",
          "shell.execute_reply": "2022-05-12T04:10:18.167441Z",
          "shell.execute_reply.started": "2022-05-12T04:10:18.108184Z"
        },
        "id": "5kG24xo5eKLl",
        "papermill": {
          "duration": 2.160102,
          "end_time": "2022-05-10T11:05:27.581844",
          "exception": false,
          "start_time": "2022-05-10T11:05:25.421742",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_train = df_all[:len(df_train)]\n",
        "df_test = df_all[len(df_train):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:18.171574Z",
          "iopub.status.busy": "2022-05-12T04:10:18.171225Z",
          "iopub.status.idle": "2022-05-12T04:10:19.663078Z",
          "shell.execute_reply": "2022-05-12T04:10:19.662030Z",
          "shell.execute_reply.started": "2022-05-12T04:10:18.171523Z"
        },
        "id": "RrbKwOjIeKLm",
        "papermill": {
          "duration": 3.969914,
          "end_time": "2022-05-10T11:05:33.707122",
          "exception": false,
          "start_time": "2022-05-10T11:05:29.737208",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_train.to_pickle('train_fe.pkl')\n",
        "df_test.to_pickle('test_fe.pkl')\n",
        "df_all.to_pickle('all_fe.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1syemr5eKLm",
        "papermill": {
          "duration": 2.115464,
          "end_time": "2022-05-10T11:05:42.927539",
          "exception": false,
          "start_time": "2022-05-10T11:05:40.812075",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:19.665419Z",
          "iopub.status.busy": "2022-05-12T04:10:19.665127Z",
          "iopub.status.idle": "2022-05-12T04:10:19.671711Z",
          "shell.execute_reply": "2022-05-12T04:10:19.670657Z",
          "shell.execute_reply.started": "2022-05-12T04:10:19.665390Z"
        },
        "id": "xKTMC2-peKLn",
        "outputId": "28a28363-32f0-4972-f55d-e9c41dd52edb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TabNet\n"
          ]
        }
      ],
      "source": [
        "print('TabNet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:19.677109Z",
          "iopub.status.busy": "2022-05-12T04:10:19.676760Z",
          "iopub.status.idle": "2022-05-12T04:10:20.056445Z",
          "shell.execute_reply": "2022-05-12T04:10:20.055414Z",
          "shell.execute_reply.started": "2022-05-12T04:10:19.677075Z"
        },
        "id": "pCi3VE7reKLn",
        "papermill": {
          "duration": 3.302225,
          "end_time": "2022-05-10T11:05:48.423437",
          "exception": false,
          "start_time": "2022-05-10T11:05:45.121212",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_pickle('train_fe.pkl')\n",
        "df_test = pd.read_pickle('test_fe.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqGDtvbjeKLo",
        "papermill": {
          "duration": 2.121208,
          "end_time": "2022-05-10T11:05:52.630053",
          "exception": false,
          "start_time": "2022-05-10T11:05:50.508845",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Define Pytorch Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd7etW_9eKLp",
        "papermill": {
          "duration": 2.11413,
          "end_time": "2022-05-10T11:06:01.323176",
          "exception": false,
          "start_time": "2022-05-10T11:05:59.209046",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Pytorch Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:20.058026Z",
          "iopub.status.busy": "2022-05-12T04:10:20.057768Z",
          "iopub.status.idle": "2022-05-12T04:10:20.062758Z",
          "shell.execute_reply": "2022-05-12T04:10:20.061804Z",
          "shell.execute_reply.started": "2022-05-12T04:10:20.057997Z"
        },
        "id": "TA4Km8MWeKLp",
        "papermill": {
          "duration": 2.795336,
          "end_time": "2022-05-10T11:06:06.248996",
          "exception": false,
          "start_time": "2022-05-10T11:06:03.45366",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-oEAsQdeKLq",
        "papermill": {
          "duration": 2.175418,
          "end_time": "2022-05-10T11:06:10.80194",
          "exception": false,
          "start_time": "2022-05-10T11:06:08.626522",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Pytorch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:20.064684Z",
          "iopub.status.busy": "2022-05-12T04:10:20.064424Z",
          "iopub.status.idle": "2022-05-12T04:10:20.081928Z",
          "shell.execute_reply": "2022-05-12T04:10:20.081077Z",
          "shell.execute_reply.started": "2022-05-12T04:10:20.064647Z"
        },
        "id": "kJ9si5tqeKLr",
        "papermill": {
          "duration": 2.114609,
          "end_time": "2022-05-10T11:06:15.015486",
          "exception": false,
          "start_time": "2022-05-10T11:06:12.900877",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class LossChecker:\n",
        "    # 指定した回数、loss等が改善されていなければTrueを返す\n",
        "    def __init__(self, patience=20, strategy=\"min\", delta=1e-6):\n",
        "        self.patience = patience\n",
        "        self.bef_epoch = 0\n",
        "        self.strategy = strategy\n",
        "        self.delta = delta\n",
        "        if strategy == \"max\":\n",
        "            self.val = -float('inf')\n",
        "        else:\n",
        "            self.val = float('inf')\n",
        "\n",
        "    def step(self, epoch, loss):\n",
        "        if self.strategy == \"max\":\n",
        "            if self.val < loss:\n",
        "                self.val = loss + self.delta\n",
        "                self.bef_epoch = epoch\n",
        "            if epoch - self.bef_epoch > self.patience:\n",
        "                self.bef_epoch = epoch\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            if self.val > loss:\n",
        "                self.val = loss - self.delta\n",
        "                self.bef_epoch = epoch\n",
        "            if epoch - self.bef_epoch > self.patience:\n",
        "                self.bef_epoch = epoch\n",
        "                return True\n",
        "            else:\n",
        "                return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:20.086361Z",
          "iopub.status.busy": "2022-05-12T04:10:20.083834Z",
          "iopub.status.idle": "2022-05-12T04:10:20.101693Z",
          "shell.execute_reply": "2022-05-12T04:10:20.100036Z",
          "shell.execute_reply.started": "2022-05-12T04:10:20.086291Z"
        },
        "id": "GvigcsggeKLr",
        "papermill": {
          "duration": 2.228949,
          "end_time": "2022-05-10T11:06:19.421623",
          "exception": false,
          "start_time": "2022-05-10T11:06:17.192674",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 学習の対象とする特徴量を列挙する\n",
        "all_features = df_train.columns.tolist()\n",
        "all_features.remove(CFG.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZgiALnmeKLs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:20.104619Z",
          "iopub.status.busy": "2022-05-12T04:10:20.103743Z",
          "iopub.status.idle": "2022-05-12T04:10:20.116486Z",
          "shell.execute_reply": "2022-05-12T04:10:20.115616Z",
          "shell.execute_reply.started": "2022-05-12T04:10:20.104554Z"
        },
        "id": "MI5KYvkyeKLt",
        "outputId": "555e6b03-b33a-42d5-bf14-41055a708f3f",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(900000, 74)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:20.119055Z",
          "iopub.status.busy": "2022-05-12T04:10:20.118250Z",
          "iopub.status.idle": "2022-05-12T04:10:20.143335Z",
          "shell.execute_reply": "2022-05-12T04:10:20.141747Z",
          "shell.execute_reply.started": "2022-05-12T04:10:20.119009Z"
        },
        "id": "tVTaQxvOeKLt",
        "outputId": "e01bb5b6-b131-4660-8cbb-0b0f65738ef8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7 17\n",
            "8 16\n",
            "9 16\n",
            "10 16\n",
            "11 15\n",
            "12 17\n",
            "13 14\n",
            "14 14\n",
            "15 15\n",
            "16 16\n",
            "17 15\n",
            "18 14\n",
            "28 2\n",
            "29 3\n",
            "30 3\n",
            "31 3\n",
            "32 3\n",
            "33 2\n",
            "34 15\n",
            "35 2\n",
            "36 15\n",
            "37 15\n",
            "38 2\n",
            "39 15\n",
            "40 20\n",
            "41 15\n",
            "42 15\n",
            "46 9\n"
          ]
        }
      ],
      "source": [
        "for i, d in zip(cat_idxs, cat_dims):\n",
        "    print(i, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:20.146954Z",
          "iopub.status.busy": "2022-05-12T04:10:20.146537Z",
          "iopub.status.idle": "2022-05-12T04:10:20.168390Z",
          "shell.execute_reply": "2022-05-12T04:10:20.167146Z",
          "shell.execute_reply.started": "2022-05-12T04:10:20.146905Z"
        },
        "id": "xlA23go-eKLu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:20.170320Z",
          "iopub.status.busy": "2022-05-12T04:10:20.169970Z",
          "iopub.status.idle": "2022-05-12T04:10:20.180816Z",
          "shell.execute_reply": "2022-05-12T04:10:20.179887Z",
          "shell.execute_reply.started": "2022-05-12T04:10:20.170275Z"
        },
        "id": "aLOqpf4deKLu",
        "outputId": "2bf4b713-a5f2-448d-e5f4-48ac0b386ff3",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65536"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CFG.batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-05-12T04:10:20.183301Z",
          "iopub.status.busy": "2022-05-12T04:10:20.182796Z",
          "iopub.status.idle": "2022-05-12T04:10:48.249475Z",
          "shell.execute_reply": "2022-05-12T04:10:48.248252Z",
          "shell.execute_reply.started": "2022-05-12T04:10:20.183258Z"
        },
        "id": "3vRopY3eeKLv",
        "outputId": "02248f89-9196-4822-9fe6-37d84f75c9c3",
        "papermill": {
          "duration": 2.146747,
          "end_time": "2022-05-10T11:06:28.036655",
          "exception": false,
          "start_time": "2022-05-10T11:06:25.889908",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- 0 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 17474242.26211| val_0_unsup_loss: 8379.45605|  0:00:05s\n",
            "epoch 1  | loss: 5896.30275| val_0_unsup_loss: 2.85878 |  0:00:10s\n",
            "epoch 2  | loss: 61.46923| val_0_unsup_loss: 2.06963 |  0:00:15s\n",
            "epoch 3  | loss: 3.09758 | val_0_unsup_loss: 1.70161 |  0:00:21s\n",
            "epoch 4  | loss: 3.03843 | val_0_unsup_loss: 1.52393 |  0:00:26s\n",
            "epoch 5  | loss: 1.50813 | val_0_unsup_loss: 1.42851 |  0:00:31s\n",
            "epoch 6  | loss: 1.43492 | val_0_unsup_loss: 1.38388 |  0:00:36s\n",
            "epoch 7  | loss: 1.37347 | val_0_unsup_loss: 1.36601 |  0:00:41s\n",
            "epoch 8  | loss: 2.09621 | val_0_unsup_loss: 1.36214 |  0:00:47s\n",
            "epoch 9  | loss: 1.37853 | val_0_unsup_loss: 1.3592  |  0:00:52s\n",
            "epoch 10 | loss: 2.17099 | val_0_unsup_loss: 1.35871 |  0:00:57s\n",
            "epoch 11 | loss: 14.1903 | val_0_unsup_loss: 1.35858 |  0:01:02s\n",
            "epoch 12 | loss: 1.35724 | val_0_unsup_loss: 1.35633 |  0:01:07s\n",
            "epoch 13 | loss: 1.87749 | val_0_unsup_loss: 1.35551 |  0:01:12s\n",
            "epoch 14 | loss: 1.35524 | val_0_unsup_loss: 1.35491 |  0:01:17s\n",
            "epoch 15 | loss: 1.37656 | val_0_unsup_loss: 1.35459 |  0:01:22s\n",
            "epoch 16 | loss: 1.35453 | val_0_unsup_loss: 1.35442 |  0:01:28s\n",
            "epoch 17 | loss: 1.35419 | val_0_unsup_loss: 1.35433 |  0:01:33s\n",
            "epoch 18 | loss: 1.35651 | val_0_unsup_loss: 1.35429 |  0:01:38s\n",
            "epoch 19 | loss: 5.28598 | val_0_unsup_loss: 1.35425 |  0:01:44s\n",
            "epoch 20 | loss: 1.35408 | val_0_unsup_loss: 1.35423 |  0:01:49s\n",
            "epoch 21 | loss: 1.3542  | val_0_unsup_loss: 1.35422 |  0:01:54s\n",
            "epoch 22 | loss: 1.35397 | val_0_unsup_loss: 1.35421 |  0:02:00s\n",
            "epoch 23 | loss: 1.35404 | val_0_unsup_loss: 1.3542  |  0:02:05s\n",
            "epoch 24 | loss: 1.35425 | val_0_unsup_loss: 1.3542  |  0:02:10s\n",
            "epoch 25 | loss: 1.3543  | val_0_unsup_loss: 1.35419 |  0:02:16s\n",
            "epoch 26 | loss: 1.35413 | val_0_unsup_loss: 1.35419 |  0:02:21s\n",
            "epoch 27 | loss: 1.35405 | val_0_unsup_loss: 1.35419 |  0:02:26s\n",
            "epoch 28 | loss: 1.35428 | val_0_unsup_loss: 1.35419 |  0:02:32s\n",
            "epoch 29 | loss: 1.35442 | val_0_unsup_loss: 1.35419 |  0:02:37s\n",
            "epoch 30 | loss: 1.36231 | val_0_unsup_loss: 1.35419 |  0:02:43s\n",
            "epoch 31 | loss: 1.35405 | val_0_unsup_loss: 1.35419 |  0:02:48s\n",
            "epoch 32 | loss: 1.35422 | val_0_unsup_loss: 1.35419 |  0:02:53s\n",
            "epoch 33 | loss: 1.35403 | val_0_unsup_loss: 1.35419 |  0:02:58s\n",
            "epoch 34 | loss: 1.35417 | val_0_unsup_loss: 1.35419 |  0:03:04s\n",
            "epoch 35 | loss: 1.35427 | val_0_unsup_loss: 1.35419 |  0:03:09s\n",
            "epoch 36 | loss: 1.35432 | val_0_unsup_loss: 1.35419 |  0:03:14s\n",
            "epoch 37 | loss: 1.35434 | val_0_unsup_loss: 1.35419 |  0:03:20s\n",
            "epoch 38 | loss: 1.35436 | val_0_unsup_loss: 1.35419 |  0:03:25s\n",
            "epoch 39 | loss: 1.35433 | val_0_unsup_loss: 1.35419 |  0:03:30s\n",
            "epoch 40 | loss: 1.35414 | val_0_unsup_loss: 1.35419 |  0:03:36s\n",
            "epoch 41 | loss: 1.35396 | val_0_unsup_loss: 1.35419 |  0:03:41s\n",
            "epoch 42 | loss: 1.3542  | val_0_unsup_loss: 1.35419 |  0:03:46s\n",
            "epoch 43 | loss: 1.35382 | val_0_unsup_loss: 1.35419 |  0:03:52s\n",
            "epoch 44 | loss: 1.35428 | val_0_unsup_loss: 1.35419 |  0:03:57s\n",
            "epoch 45 | loss: 2.23298 | val_0_unsup_loss: 1.35419 |  0:04:02s\n",
            "epoch 46 | loss: 1.35394 | val_0_unsup_loss: 1.35419 |  0:04:08s\n",
            "epoch 47 | loss: 1.35425 | val_0_unsup_loss: 1.35419 |  0:04:13s\n",
            "epoch 48 | loss: 1.35414 | val_0_unsup_loss: 1.35419 |  0:04:18s\n",
            "epoch 49 | loss: 1.35406 | val_0_unsup_loss: 1.35419 |  0:04:24s\n",
            "epoch 50 | loss: 1.62309 | val_0_unsup_loss: 1.35436 |  0:04:29s\n",
            "epoch 51 | loss: 1.35441 | val_0_unsup_loss: 1.35434 |  0:04:35s\n",
            "epoch 52 | loss: 1.35421 | val_0_unsup_loss: 1.35426 |  0:04:40s\n",
            "epoch 53 | loss: 1.35399 | val_0_unsup_loss: 1.35421 |  0:04:45s\n",
            "epoch 54 | loss: 1.35392 | val_0_unsup_loss: 1.35419 |  0:04:50s\n",
            "epoch 55 | loss: 1.3542  | val_0_unsup_loss: 1.35419 |  0:04:56s\n",
            "epoch 56 | loss: 1.3582  | val_0_unsup_loss: 1.35424 |  0:05:01s\n",
            "epoch 57 | loss: 1.35395 | val_0_unsup_loss: 1.35423 |  0:05:06s\n",
            "epoch 58 | loss: 1.3538  | val_0_unsup_loss: 1.3542  |  0:05:12s\n",
            "epoch 59 | loss: 1.35411 | val_0_unsup_loss: 1.35419 |  0:05:17s\n",
            "epoch 60 | loss: 1.35432 | val_0_unsup_loss: 1.35419 |  0:05:22s\n",
            "epoch 61 | loss: 1.35386 | val_0_unsup_loss: 1.35419 |  0:05:27s\n",
            "epoch 62 | loss: 1.35407 | val_0_unsup_loss: 1.35419 |  0:05:33s\n",
            "epoch 63 | loss: 1.3539  | val_0_unsup_loss: 1.35419 |  0:05:38s\n",
            "epoch 64 | loss: 1.35427 | val_0_unsup_loss: 1.35419 |  0:05:44s\n",
            "epoch 65 | loss: 1.35421 | val_0_unsup_loss: 1.35419 |  0:05:49s\n",
            "epoch 66 | loss: 1.35393 | val_0_unsup_loss: 1.35419 |  0:05:54s\n",
            "epoch 67 | loss: 1.35411 | val_0_unsup_loss: 1.35419 |  0:06:00s\n",
            "epoch 68 | loss: 1.35407 | val_0_unsup_loss: 1.35419 |  0:06:05s\n",
            "epoch 69 | loss: 1.35406 | val_0_unsup_loss: 1.35419 |  0:06:10s\n",
            "epoch 70 | loss: 1.35401 | val_0_unsup_loss: 1.35419 |  0:06:16s\n",
            "epoch 71 | loss: 1.35412 | val_0_unsup_loss: 1.35419 |  0:06:21s\n",
            "epoch 72 | loss: 1.35412 | val_0_unsup_loss: 1.35419 |  0:06:26s\n",
            "epoch 73 | loss: 1.35415 | val_0_unsup_loss: 1.35419 |  0:06:31s\n",
            "epoch 74 | loss: 1.35398 | val_0_unsup_loss: 1.35419 |  0:06:37s\n",
            "epoch 75 | loss: 1.35418 | val_0_unsup_loss: 1.35419 |  0:06:42s\n",
            "epoch 76 | loss: 1.3541  | val_0_unsup_loss: 1.35419 |  0:06:48s\n",
            "epoch 77 | loss: 1.3932  | val_0_unsup_loss: 1.35435 |  0:06:53s\n",
            "epoch 78 | loss: 1.3542  | val_0_unsup_loss: 1.35427 |  0:06:58s\n",
            "epoch 79 | loss: 1.35401 | val_0_unsup_loss: 1.3542  |  0:07:04s\n",
            "epoch 80 | loss: 1.35427 | val_0_unsup_loss: 1.35419 |  0:07:09s\n",
            "epoch 81 | loss: 1.35428 | val_0_unsup_loss: 1.35419 |  0:07:14s\n",
            "epoch 82 | loss: 1.35405 | val_0_unsup_loss: 1.35419 |  0:07:20s\n",
            "epoch 83 | loss: 1.35409 | val_0_unsup_loss: 1.35419 |  0:07:25s\n",
            "epoch 84 | loss: 1.35398 | val_0_unsup_loss: 1.35419 |  0:07:30s\n",
            "epoch 85 | loss: 1.35437 | val_0_unsup_loss: 1.35419 |  0:07:36s\n",
            "epoch 86 | loss: 1.35455 | val_0_unsup_loss: 1.35419 |  0:07:41s\n",
            "epoch 87 | loss: 1.35411 | val_0_unsup_loss: 1.35419 |  0:07:47s\n",
            "epoch 88 | loss: 1.35427 | val_0_unsup_loss: 1.35419 |  0:07:52s\n",
            "\n",
            "Early stopping occurred at epoch 88 with best_epoch = 68 and best_val_0_unsup_loss = 1.35419\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.69238 | train_auc: 0.53416 | valid_auc: 0.53311 |  0:00:08s\n",
            "epoch 1  | loss: 0.61898 | train_auc: 0.76628 | valid_auc: 0.76981 |  0:00:16s\n",
            "epoch 2  | loss: 0.37685 | train_auc: 0.87033 | valid_auc: 0.86924 |  0:00:24s\n",
            "epoch 3  | loss: 0.26003 | train_auc: 0.91625 | valid_auc: 0.91602 |  0:00:32s\n",
            "epoch 4  | loss: 0.22038 | train_auc: 0.93364 | valid_auc: 0.93262 |  0:00:40s\n",
            "epoch 5  | loss: 0.20148 | train_auc: 0.94829 | valid_auc: 0.94748 |  0:00:48s\n",
            "epoch 6  | loss: 0.18666 | train_auc: 0.96193 | valid_auc: 0.96132 |  0:00:56s\n",
            "epoch 7  | loss: 0.174   | train_auc: 0.969   | valid_auc: 0.96835 |  0:01:04s\n",
            "epoch 8  | loss: 0.15412 | train_auc: 0.97501 | valid_auc: 0.97494 |  0:01:12s\n",
            "epoch 9  | loss: 0.13776 | train_auc: 0.98022 | valid_auc: 0.97992 |  0:01:20s\n",
            "epoch 10 | loss: 0.12783 | train_auc: 0.98741 | valid_auc: 0.98725 |  0:01:28s\n",
            "epoch 11 | loss: 0.12362 | train_auc: 0.98794 | valid_auc: 0.98777 |  0:01:37s\n",
            "epoch 12 | loss: 0.12093 | train_auc: 0.98903 | valid_auc: 0.98882 |  0:01:45s\n",
            "epoch 13 | loss: 0.11815 | train_auc: 0.98937 | valid_auc: 0.98906 |  0:01:53s\n",
            "epoch 14 | loss: 0.11488 | train_auc: 0.9887  | valid_auc: 0.98838 |  0:02:01s\n",
            "epoch 15 | loss: 0.11308 | train_auc: 0.99006 | valid_auc: 0.98968 |  0:02:09s\n",
            "epoch 16 | loss: 0.11005 | train_auc: 0.99113 | valid_auc: 0.99083 |  0:02:17s\n",
            "epoch 17 | loss: 0.10869 | train_auc: 0.99101 | valid_auc: 0.99088 |  0:02:25s\n",
            "epoch 18 | loss: 0.10595 | train_auc: 0.99169 | valid_auc: 0.99155 |  0:02:33s\n",
            "epoch 19 | loss: 0.10404 | train_auc: 0.99127 | valid_auc: 0.99109 |  0:02:41s\n",
            "epoch 20 | loss: 0.10152 | train_auc: 0.99262 | valid_auc: 0.9925  |  0:02:50s\n",
            "epoch 21 | loss: 0.10247 | train_auc: 0.98965 | valid_auc: 0.9894  |  0:02:58s\n",
            "epoch 22 | loss: 0.10195 | train_auc: 0.9936  | valid_auc: 0.99351 |  0:03:06s\n",
            "epoch 23 | loss: 0.09954 | train_auc: 0.99339 | valid_auc: 0.9933  |  0:03:14s\n",
            "epoch 24 | loss: 0.09835 | train_auc: 0.99399 | valid_auc: 0.99377 |  0:03:23s\n",
            "epoch 25 | loss: 0.09617 | train_auc: 0.9941  | valid_auc: 0.99392 |  0:03:31s\n",
            "epoch 26 | loss: 0.09467 | train_auc: 0.99443 | valid_auc: 0.99412 |  0:03:39s\n",
            "epoch 27 | loss: 0.09515 | train_auc: 0.99399 | valid_auc: 0.99379 |  0:03:47s\n",
            "epoch 28 | loss: 0.09441 | train_auc: 0.99391 | valid_auc: 0.99376 |  0:03:56s\n",
            "epoch 29 | loss: 0.09402 | train_auc: 0.99436 | valid_auc: 0.99411 |  0:04:04s\n",
            "epoch 30 | loss: 0.09213 | train_auc: 0.99439 | valid_auc: 0.9941  |  0:04:12s\n",
            "epoch 31 | loss: 0.09145 | train_auc: 0.99479 | valid_auc: 0.99453 |  0:04:20s\n",
            "epoch 32 | loss: 0.0907  | train_auc: 0.99476 | valid_auc: 0.99454 |  0:04:28s\n",
            "epoch 33 | loss: 0.09074 | train_auc: 0.99491 | valid_auc: 0.99461 |  0:04:36s\n",
            "epoch 34 | loss: 0.09081 | train_auc: 0.99502 | valid_auc: 0.99482 |  0:04:44s\n",
            "epoch 35 | loss: 0.08957 | train_auc: 0.99524 | valid_auc: 0.99507 |  0:04:52s\n",
            "epoch 36 | loss: 0.08885 | train_auc: 0.99532 | valid_auc: 0.99509 |  0:05:00s\n",
            "epoch 37 | loss: 0.08834 | train_auc: 0.9953  | valid_auc: 0.99503 |  0:05:08s\n",
            "epoch 38 | loss: 0.08792 | train_auc: 0.99519 | valid_auc: 0.9949  |  0:05:16s\n",
            "epoch 39 | loss: 0.0881  | train_auc: 0.99533 | valid_auc: 0.99507 |  0:05:25s\n",
            "epoch 40 | loss: 0.08739 | train_auc: 0.99549 | valid_auc: 0.99527 |  0:05:33s\n",
            "epoch 41 | loss: 0.08652 | train_auc: 0.99565 | valid_auc: 0.99541 |  0:05:41s\n",
            "epoch 42 | loss: 0.08637 | train_auc: 0.99551 | valid_auc: 0.99523 |  0:05:49s\n",
            "epoch 43 | loss: 0.08686 | train_auc: 0.99565 | valid_auc: 0.99534 |  0:05:57s\n",
            "epoch 44 | loss: 0.08588 | train_auc: 0.99548 | valid_auc: 0.99528 |  0:06:06s\n",
            "epoch 45 | loss: 0.0851  | train_auc: 0.99449 | valid_auc: 0.99417 |  0:06:14s\n",
            "epoch 46 | loss: 0.08492 | train_auc: 0.99522 | valid_auc: 0.99486 |  0:06:22s\n",
            "epoch 47 | loss: 0.0851  | train_auc: 0.99536 | valid_auc: 0.99507 |  0:06:30s\n",
            "epoch 48 | loss: 0.08681 | train_auc: 0.99571 | valid_auc: 0.99542 |  0:06:38s\n",
            "epoch 49 | loss: 0.08549 | train_auc: 0.99574 | valid_auc: 0.99551 |  0:06:47s\n",
            "epoch 50 | loss: 0.08493 | train_auc: 0.9955  | valid_auc: 0.99524 |  0:06:55s\n",
            "epoch 51 | loss: 0.08557 | train_auc: 0.9956  | valid_auc: 0.99544 |  0:07:03s\n",
            "epoch 52 | loss: 0.08441 | train_auc: 0.99581 | valid_auc: 0.99556 |  0:07:11s\n",
            "epoch 53 | loss: 0.08366 | train_auc: 0.99593 | valid_auc: 0.99569 |  0:07:19s\n",
            "epoch 54 | loss: 0.083   | train_auc: 0.9959  | valid_auc: 0.99564 |  0:07:27s\n",
            "epoch 55 | loss: 0.08322 | train_auc: 0.99563 | valid_auc: 0.99528 |  0:07:35s\n",
            "epoch 56 | loss: 0.08397 | train_auc: 0.99566 | valid_auc: 0.99545 |  0:07:43s\n",
            "epoch 57 | loss: 0.08384 | train_auc: 0.99505 | valid_auc: 0.99469 |  0:07:51s\n",
            "epoch 58 | loss: 0.08811 | train_auc: 0.99496 | valid_auc: 0.9947  |  0:07:59s\n",
            "epoch 59 | loss: 0.09004 | train_auc: 0.99477 | valid_auc: 0.99464 |  0:08:08s\n",
            "epoch 60 | loss: 0.08756 | train_auc: 0.99544 | valid_auc: 0.99527 |  0:08:16s\n",
            "epoch 61 | loss: 0.08448 | train_auc: 0.99564 | valid_auc: 0.9955  |  0:08:24s\n",
            "epoch 62 | loss: 0.0839  | train_auc: 0.99581 | valid_auc: 0.99557 |  0:08:32s\n",
            "epoch 63 | loss: 0.08247 | train_auc: 0.9956  | valid_auc: 0.99542 |  0:08:40s\n",
            "epoch 64 | loss: 0.0817  | train_auc: 0.99606 | valid_auc: 0.99583 |  0:08:48s\n",
            "epoch 65 | loss: 0.08224 | train_auc: 0.99605 | valid_auc: 0.99573 |  0:08:56s\n",
            "epoch 66 | loss: 0.08182 | train_auc: 0.9958  | valid_auc: 0.99556 |  0:09:05s\n",
            "epoch 67 | loss: 0.08063 | train_auc: 0.99596 | valid_auc: 0.99571 |  0:09:13s\n",
            "epoch 68 | loss: 0.08117 | train_auc: 0.99607 | valid_auc: 0.99581 |  0:09:21s\n",
            "epoch 69 | loss: 0.08094 | train_auc: 0.99619 | valid_auc: 0.996   |  0:09:29s\n",
            "epoch 70 | loss: 0.0793  | train_auc: 0.99617 | valid_auc: 0.99594 |  0:09:37s\n",
            "epoch 71 | loss: 0.07916 | train_auc: 0.99589 | valid_auc: 0.99562 |  0:09:45s\n",
            "epoch 72 | loss: 0.07912 | train_auc: 0.99642 | valid_auc: 0.99617 |  0:09:53s\n",
            "epoch 73 | loss: 0.07874 | train_auc: 0.99622 | valid_auc: 0.99597 |  0:10:02s\n",
            "epoch 74 | loss: 0.07837 | train_auc: 0.99622 | valid_auc: 0.99604 |  0:10:09s\n",
            "epoch 75 | loss: 0.07896 | train_auc: 0.99604 | valid_auc: 0.99576 |  0:10:18s\n",
            "epoch 76 | loss: 0.07917 | train_auc: 0.99637 | valid_auc: 0.99614 |  0:10:26s\n",
            "epoch 77 | loss: 0.07953 | train_auc: 0.9963  | valid_auc: 0.99604 |  0:10:34s\n",
            "epoch 78 | loss: 0.07841 | train_auc: 0.99653 | valid_auc: 0.99622 |  0:10:42s\n",
            "epoch 79 | loss: 0.07837 | train_auc: 0.99642 | valid_auc: 0.99617 |  0:10:50s\n",
            "epoch 80 | loss: 0.07707 | train_auc: 0.99625 | valid_auc: 0.99604 |  0:10:58s\n",
            "epoch 81 | loss: 0.0774  | train_auc: 0.99485 | valid_auc: 0.99451 |  0:11:07s\n",
            "epoch 82 | loss: 0.07761 | train_auc: 0.99573 | valid_auc: 0.9954  |  0:11:14s\n",
            "epoch 83 | loss: 0.07713 | train_auc: 0.99624 | valid_auc: 0.99596 |  0:11:23s\n",
            "epoch 84 | loss: 0.07676 | train_auc: 0.99663 | valid_auc: 0.99636 |  0:11:31s\n",
            "epoch 85 | loss: 0.07661 | train_auc: 0.9967  | valid_auc: 0.99644 |  0:11:39s\n",
            "epoch 86 | loss: 0.07584 | train_auc: 0.99631 | valid_auc: 0.99603 |  0:11:47s\n",
            "epoch 87 | loss: 0.07586 | train_auc: 0.99636 | valid_auc: 0.99601 |  0:11:55s\n",
            "epoch 88 | loss: 0.0763  | train_auc: 0.99656 | valid_auc: 0.9963  |  0:12:04s\n",
            "epoch 89 | loss: 0.0768  | train_auc: 0.99669 | valid_auc: 0.99641 |  0:12:12s\n",
            "epoch 90 | loss: 0.07553 | train_auc: 0.99641 | valid_auc: 0.99612 |  0:12:20s\n",
            "epoch 91 | loss: 0.07586 | train_auc: 0.99649 | valid_auc: 0.99623 |  0:12:28s\n",
            "epoch 92 | loss: 0.07612 | train_auc: 0.9961  | valid_auc: 0.99575 |  0:12:36s\n",
            "epoch 93 | loss: 0.07586 | train_auc: 0.99641 | valid_auc: 0.99606 |  0:12:44s\n",
            "epoch 94 | loss: 0.0751  | train_auc: 0.99666 | valid_auc: 0.99634 |  0:12:52s\n",
            "epoch 95 | loss: 0.0758  | train_auc: 0.99663 | valid_auc: 0.99625 |  0:13:01s\n",
            "epoch 96 | loss: 0.07478 | train_auc: 0.99664 | valid_auc: 0.99635 |  0:13:09s\n",
            "epoch 97 | loss: 0.07428 | train_auc: 0.99651 | valid_auc: 0.99622 |  0:13:17s\n",
            "epoch 98 | loss: 0.07474 | train_auc: 0.99653 | valid_auc: 0.99627 |  0:13:25s\n",
            "epoch 99 | loss: 0.07422 | train_auc: 0.9969  | valid_auc: 0.99661 |  0:13:33s\n",
            "epoch 100| loss: 0.07534 | train_auc: 0.9967  | valid_auc: 0.99637 |  0:13:41s\n",
            "epoch 101| loss: 0.07439 | train_auc: 0.99684 | valid_auc: 0.99655 |  0:13:50s\n",
            "epoch 102| loss: 0.07471 | train_auc: 0.99668 | valid_auc: 0.99637 |  0:13:58s\n",
            "epoch 103| loss: 0.07617 | train_auc: 0.99667 | valid_auc: 0.9964  |  0:14:06s\n",
            "epoch 104| loss: 0.07446 | train_auc: 0.99671 | valid_auc: 0.99639 |  0:14:14s\n",
            "epoch 105| loss: 0.074   | train_auc: 0.99688 | valid_auc: 0.99658 |  0:14:23s\n",
            "epoch 106| loss: 0.07392 | train_auc: 0.99677 | valid_auc: 0.99641 |  0:14:31s\n",
            "epoch 107| loss: 0.07374 | train_auc: 0.99619 | valid_auc: 0.99587 |  0:14:39s\n",
            "epoch 108| loss: 0.07348 | train_auc: 0.99614 | valid_auc: 0.99573 |  0:14:47s\n",
            "epoch 109| loss: 0.07387 | train_auc: 0.99646 | valid_auc: 0.99614 |  0:14:55s\n",
            "epoch 110| loss: 0.07307 | train_auc: 0.99695 | valid_auc: 0.99661 |  0:15:04s\n",
            "epoch 111| loss: 0.0728  | train_auc: 0.99702 | valid_auc: 0.99669 |  0:15:12s\n",
            "epoch 112| loss: 0.07246 | train_auc: 0.99687 | valid_auc: 0.9965  |  0:15:20s\n",
            "epoch 113| loss: 0.07291 | train_auc: 0.99673 | valid_auc: 0.9964  |  0:15:28s\n",
            "epoch 114| loss: 0.07234 | train_auc: 0.99684 | valid_auc: 0.99648 |  0:15:36s\n",
            "epoch 115| loss: 0.07236 | train_auc: 0.99684 | valid_auc: 0.9965  |  0:15:45s\n",
            "epoch 116| loss: 0.0719  | train_auc: 0.997   | valid_auc: 0.99667 |  0:15:53s\n",
            "epoch 117| loss: 0.07245 | train_auc: 0.99705 | valid_auc: 0.9967  |  0:16:01s\n",
            "epoch 118| loss: 0.07227 | train_auc: 0.99699 | valid_auc: 0.99667 |  0:16:09s\n",
            "epoch 119| loss: 0.07293 | train_auc: 0.99687 | valid_auc: 0.99655 |  0:16:17s\n",
            "epoch 120| loss: 0.07377 | train_auc: 0.99673 | valid_auc: 0.9964  |  0:16:25s\n",
            "epoch 121| loss: 0.07276 | train_auc: 0.99693 | valid_auc: 0.99661 |  0:16:33s\n",
            "epoch 122| loss: 0.0717  | train_auc: 0.99703 | valid_auc: 0.99668 |  0:16:42s\n",
            "epoch 123| loss: 0.07153 | train_auc: 0.99705 | valid_auc: 0.99668 |  0:16:50s\n",
            "epoch 124| loss: 0.0717  | train_auc: 0.99707 | valid_auc: 0.99676 |  0:16:58s\n",
            "epoch 125| loss: 0.07193 | train_auc: 0.99707 | valid_auc: 0.99674 |  0:17:06s\n",
            "epoch 126| loss: 0.07186 | train_auc: 0.997   | valid_auc: 0.99667 |  0:17:15s\n",
            "epoch 127| loss: 0.07136 | train_auc: 0.99704 | valid_auc: 0.99669 |  0:17:23s\n",
            "epoch 128| loss: 0.07181 | train_auc: 0.99712 | valid_auc: 0.99679 |  0:17:31s\n",
            "epoch 129| loss: 0.07127 | train_auc: 0.99707 | valid_auc: 0.9967  |  0:17:39s\n",
            "epoch 130| loss: 0.07173 | train_auc: 0.99708 | valid_auc: 0.99672 |  0:17:47s\n",
            "epoch 131| loss: 0.07182 | train_auc: 0.99709 | valid_auc: 0.99671 |  0:17:56s\n",
            "epoch 132| loss: 0.07128 | train_auc: 0.997   | valid_auc: 0.99661 |  0:18:04s\n",
            "epoch 133| loss: 0.07164 | train_auc: 0.99698 | valid_auc: 0.99664 |  0:18:12s\n",
            "epoch 134| loss: 0.07097 | train_auc: 0.99707 | valid_auc: 0.9967  |  0:18:20s\n",
            "epoch 135| loss: 0.07162 | train_auc: 0.9971  | valid_auc: 0.99676 |  0:18:28s\n",
            "epoch 136| loss: 0.07074 | train_auc: 0.99711 | valid_auc: 0.9968  |  0:18:36s\n",
            "epoch 137| loss: 0.07095 | train_auc: 0.99687 | valid_auc: 0.99653 |  0:18:45s\n",
            "epoch 138| loss: 0.0707  | train_auc: 0.99697 | valid_auc: 0.99661 |  0:18:53s\n",
            "epoch 139| loss: 0.07079 | train_auc: 0.99702 | valid_auc: 0.99666 |  0:19:01s\n",
            "epoch 140| loss: 0.07047 | train_auc: 0.99723 | valid_auc: 0.99689 |  0:19:09s\n",
            "epoch 141| loss: 0.06969 | train_auc: 0.99719 | valid_auc: 0.99684 |  0:19:17s\n",
            "epoch 142| loss: 0.07014 | train_auc: 0.99699 | valid_auc: 0.99665 |  0:19:26s\n",
            "epoch 143| loss: 0.07019 | train_auc: 0.99701 | valid_auc: 0.99671 |  0:19:34s\n",
            "epoch 144| loss: 0.07006 | train_auc: 0.99711 | valid_auc: 0.99677 |  0:19:42s\n",
            "epoch 145| loss: 0.07022 | train_auc: 0.99584 | valid_auc: 0.9954  |  0:19:50s\n",
            "epoch 146| loss: 0.07113 | train_auc: 0.99632 | valid_auc: 0.99591 |  0:19:58s\n",
            "epoch 147| loss: 0.07067 | train_auc: 0.99698 | valid_auc: 0.99658 |  0:20:07s\n",
            "epoch 148| loss: 0.07065 | train_auc: 0.99714 | valid_auc: 0.99677 |  0:20:15s\n",
            "epoch 149| loss: 0.07055 | train_auc: 0.99715 | valid_auc: 0.99681 |  0:20:23s\n",
            "epoch 150| loss: 0.07013 | train_auc: 0.99709 | valid_auc: 0.99676 |  0:20:32s\n",
            "epoch 151| loss: 0.0709  | train_auc: 0.99707 | valid_auc: 0.99676 |  0:20:40s\n",
            "epoch 152| loss: 0.07044 | train_auc: 0.99699 | valid_auc: 0.99667 |  0:20:48s\n",
            "epoch 153| loss: 0.06937 | train_auc: 0.99697 | valid_auc: 0.99665 |  0:20:56s\n",
            "epoch 154| loss: 0.06936 | train_auc: 0.99675 | valid_auc: 0.9964  |  0:21:05s\n",
            "epoch 155| loss: 0.06944 | train_auc: 0.99699 | valid_auc: 0.99669 |  0:21:13s\n",
            "epoch 156| loss: 0.06967 | train_auc: 0.9972  | valid_auc: 0.99684 |  0:21:21s\n",
            "epoch 157| loss: 0.06969 | train_auc: 0.99708 | valid_auc: 0.99673 |  0:21:29s\n",
            "epoch 158| loss: 0.06955 | train_auc: 0.99694 | valid_auc: 0.99657 |  0:21:37s\n",
            "epoch 159| loss: 0.06921 | train_auc: 0.99699 | valid_auc: 0.99665 |  0:21:46s\n",
            "epoch 160| loss: 0.06947 | train_auc: 0.99535 | valid_auc: 0.99493 |  0:21:54s\n",
            "\n",
            "Early stopping occurred at epoch 160 with best_epoch = 140 and best_valid_auc = 0.99689\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9968855967253886\n",
            "----- 1 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 17702011.14433| val_0_unsup_loss: 3571.20776|  0:00:05s\n",
            "epoch 1  | loss: 5554.13142| val_0_unsup_loss: 2.66453 |  0:00:10s\n",
            "epoch 2  | loss: 10.89608| val_0_unsup_loss: 1.92766 |  0:00:16s\n",
            "epoch 3  | loss: 1.76007 | val_0_unsup_loss: 1.57887 |  0:00:21s\n",
            "epoch 4  | loss: 1.54441 | val_0_unsup_loss: 1.44147 |  0:00:27s\n",
            "epoch 5  | loss: 4.48896 | val_0_unsup_loss: 1.38847 |  0:00:32s\n",
            "epoch 6  | loss: 1.65903 | val_0_unsup_loss: 1.37092 |  0:00:37s\n",
            "epoch 7  | loss: 1.55481 | val_0_unsup_loss: 1.36578 |  0:00:43s\n",
            "epoch 8  | loss: 1.98449 | val_0_unsup_loss: 1.36427 |  0:00:48s\n",
            "epoch 9  | loss: 1.74163 | val_0_unsup_loss: 1.36324 |  0:00:53s\n",
            "epoch 10 | loss: 1.85545 | val_0_unsup_loss: 1.36142 |  0:00:59s\n",
            "epoch 11 | loss: 1.38126 | val_0_unsup_loss: 1.35971 |  0:01:04s\n",
            "epoch 12 | loss: 1.75191 | val_0_unsup_loss: 1.35887 |  0:01:09s\n",
            "epoch 13 | loss: 5.3408  | val_0_unsup_loss: 1.35846 |  0:01:15s\n",
            "epoch 14 | loss: 1.35868 | val_0_unsup_loss: 1.35771 |  0:01:20s\n",
            "epoch 15 | loss: 1.35847 | val_0_unsup_loss: 1.35702 |  0:01:26s\n",
            "epoch 16 | loss: 1.42254 | val_0_unsup_loss: 1.3566  |  0:01:31s\n",
            "epoch 17 | loss: 1.41963 | val_0_unsup_loss: 1.35611 |  0:01:37s\n",
            "epoch 18 | loss: 1.35576 | val_0_unsup_loss: 1.35563 |  0:01:42s\n",
            "epoch 19 | loss: 1.35575 | val_0_unsup_loss: 1.35522 |  0:01:47s\n",
            "epoch 20 | loss: 1.35494 | val_0_unsup_loss: 1.35492 |  0:01:53s\n",
            "epoch 21 | loss: 1.35477 | val_0_unsup_loss: 1.35469 |  0:01:58s\n",
            "epoch 22 | loss: 1.35448 | val_0_unsup_loss: 1.35451 |  0:02:04s\n",
            "epoch 23 | loss: 1.35469 | val_0_unsup_loss: 1.35438 |  0:02:09s\n",
            "epoch 24 | loss: 1.35438 | val_0_unsup_loss: 1.35429 |  0:02:14s\n",
            "epoch 25 | loss: 1.35457 | val_0_unsup_loss: 1.35423 |  0:02:20s\n",
            "epoch 26 | loss: 1.35453 | val_0_unsup_loss: 1.3542  |  0:02:25s\n",
            "epoch 27 | loss: 1.35438 | val_0_unsup_loss: 1.35416 |  0:02:31s\n",
            "epoch 28 | loss: 1.35432 | val_0_unsup_loss: 1.35414 |  0:02:36s\n",
            "epoch 29 | loss: 1.35417 | val_0_unsup_loss: 1.35412 |  0:02:42s\n",
            "epoch 30 | loss: 1.35395 | val_0_unsup_loss: 1.35411 |  0:02:47s\n",
            "epoch 31 | loss: 1.35425 | val_0_unsup_loss: 1.35411 |  0:02:52s\n",
            "epoch 32 | loss: 1.38449 | val_0_unsup_loss: 1.35427 |  0:02:58s\n",
            "epoch 33 | loss: 1.35427 | val_0_unsup_loss: 1.35416 |  0:03:03s\n",
            "epoch 34 | loss: 1.35398 | val_0_unsup_loss: 1.35412 |  0:03:09s\n",
            "epoch 35 | loss: 1.35424 | val_0_unsup_loss: 1.3541  |  0:03:14s\n",
            "epoch 36 | loss: 1.35427 | val_0_unsup_loss: 1.3541  |  0:03:19s\n",
            "epoch 37 | loss: 1.35425 | val_0_unsup_loss: 1.3541  |  0:03:25s\n",
            "epoch 38 | loss: 1.35418 | val_0_unsup_loss: 1.3541  |  0:03:30s\n",
            "epoch 39 | loss: 1.35418 | val_0_unsup_loss: 1.3541  |  0:03:36s\n",
            "epoch 40 | loss: 1.35404 | val_0_unsup_loss: 1.3541  |  0:03:41s\n",
            "epoch 41 | loss: 1.35394 | val_0_unsup_loss: 1.3541  |  0:03:47s\n",
            "epoch 42 | loss: 1.35391 | val_0_unsup_loss: 1.3541  |  0:03:52s\n",
            "epoch 43 | loss: 1.35406 | val_0_unsup_loss: 1.3541  |  0:03:58s\n",
            "epoch 44 | loss: 1.35433 | val_0_unsup_loss: 1.3541  |  0:04:03s\n",
            "epoch 45 | loss: 1.35406 | val_0_unsup_loss: 1.3541  |  0:04:09s\n",
            "epoch 46 | loss: 1.3538  | val_0_unsup_loss: 1.3541  |  0:04:14s\n",
            "epoch 47 | loss: 1.35384 | val_0_unsup_loss: 1.3541  |  0:04:19s\n",
            "epoch 48 | loss: 1.35406 | val_0_unsup_loss: 1.3541  |  0:04:25s\n",
            "epoch 49 | loss: 1.35416 | val_0_unsup_loss: 1.3541  |  0:04:30s\n",
            "epoch 50 | loss: 1.35406 | val_0_unsup_loss: 1.3541  |  0:04:35s\n",
            "epoch 51 | loss: 1.35385 | val_0_unsup_loss: 1.3541  |  0:04:41s\n",
            "epoch 52 | loss: 1.35392 | val_0_unsup_loss: 1.3541  |  0:04:46s\n",
            "epoch 53 | loss: 1.35388 | val_0_unsup_loss: 1.3541  |  0:04:52s\n",
            "epoch 54 | loss: 1.35405 | val_0_unsup_loss: 1.3541  |  0:04:57s\n",
            "epoch 55 | loss: 1.35412 | val_0_unsup_loss: 1.3541  |  0:05:02s\n",
            "epoch 56 | loss: 1.35421 | val_0_unsup_loss: 1.3541  |  0:05:08s\n",
            "epoch 57 | loss: 1.35413 | val_0_unsup_loss: 1.3541  |  0:05:13s\n",
            "epoch 58 | loss: 1.35411 | val_0_unsup_loss: 1.3541  |  0:05:19s\n",
            "epoch 59 | loss: 1.35394 | val_0_unsup_loss: 1.3541  |  0:05:24s\n",
            "epoch 60 | loss: 1.35435 | val_0_unsup_loss: 1.3541  |  0:05:29s\n",
            "epoch 61 | loss: 1.35389 | val_0_unsup_loss: 1.3541  |  0:05:35s\n",
            "epoch 62 | loss: 1.35444 | val_0_unsup_loss: 1.3541  |  0:05:40s\n",
            "epoch 63 | loss: 1.35418 | val_0_unsup_loss: 1.3541  |  0:05:46s\n",
            "epoch 64 | loss: 1.35411 | val_0_unsup_loss: 1.3541  |  0:05:51s\n",
            "epoch 65 | loss: 1.35415 | val_0_unsup_loss: 1.3541  |  0:05:56s\n",
            "epoch 66 | loss: 1.35383 | val_0_unsup_loss: 1.3541  |  0:06:02s\n",
            "epoch 67 | loss: 1.35439 | val_0_unsup_loss: 1.3541  |  0:06:07s\n",
            "epoch 68 | loss: 1.35399 | val_0_unsup_loss: 1.3541  |  0:06:13s\n",
            "epoch 69 | loss: 1.354   | val_0_unsup_loss: 1.3541  |  0:06:18s\n",
            "epoch 70 | loss: 1.35406 | val_0_unsup_loss: 1.3541  |  0:06:23s\n",
            "epoch 71 | loss: 1.35407 | val_0_unsup_loss: 1.3541  |  0:06:29s\n",
            "\n",
            "Early stopping occurred at epoch 71 with best_epoch = 51 and best_val_0_unsup_loss = 1.3541\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.68584 | train_auc: 0.604   | valid_auc: 0.60458 |  0:00:08s\n",
            "epoch 1  | loss: 0.59354 | train_auc: 0.63542 | valid_auc: 0.63287 |  0:00:16s\n",
            "epoch 2  | loss: 0.36986 | train_auc: 0.803   | valid_auc: 0.80184 |  0:00:24s\n",
            "epoch 3  | loss: 0.27294 | train_auc: 0.77004 | valid_auc: 0.76823 |  0:00:32s\n",
            "epoch 4  | loss: 0.22549 | train_auc: 0.76099 | valid_auc: 0.75735 |  0:00:41s\n",
            "epoch 5  | loss: 0.20411 | train_auc: 0.72955 | valid_auc: 0.7265  |  0:00:49s\n",
            "epoch 6  | loss: 0.18706 | train_auc: 0.6782  | valid_auc: 0.67422 |  0:00:57s\n",
            "epoch 7  | loss: 0.17945 | train_auc: 0.665   | valid_auc: 0.6601  |  0:01:06s\n",
            "epoch 8  | loss: 0.17348 | train_auc: 0.71416 | valid_auc: 0.70991 |  0:01:14s\n",
            "epoch 9  | loss: 0.16784 | train_auc: 0.83809 | valid_auc: 0.83387 |  0:01:22s\n",
            "epoch 10 | loss: 0.16424 | train_auc: 0.90202 | valid_auc: 0.89952 |  0:01:30s\n",
            "epoch 11 | loss: 0.16027 | train_auc: 0.95975 | valid_auc: 0.95819 |  0:01:39s\n",
            "epoch 12 | loss: 0.15481 | train_auc: 0.96193 | valid_auc: 0.96106 |  0:01:47s\n",
            "epoch 13 | loss: 0.15024 | train_auc: 0.96736 | valid_auc: 0.96654 |  0:01:55s\n",
            "epoch 14 | loss: 0.14627 | train_auc: 0.97494 | valid_auc: 0.97417 |  0:02:03s\n",
            "epoch 15 | loss: 0.14491 | train_auc: 0.9706  | valid_auc: 0.96987 |  0:02:12s\n",
            "epoch 16 | loss: 0.13917 | train_auc: 0.97847 | valid_auc: 0.97787 |  0:02:20s\n",
            "epoch 17 | loss: 0.13343 | train_auc: 0.98607 | valid_auc: 0.98534 |  0:02:28s\n",
            "epoch 18 | loss: 0.13032 | train_auc: 0.98731 | valid_auc: 0.98656 |  0:02:37s\n",
            "epoch 19 | loss: 0.12698 | train_auc: 0.98642 | valid_auc: 0.98566 |  0:02:45s\n",
            "epoch 20 | loss: 0.12254 | train_auc: 0.99004 | valid_auc: 0.9893  |  0:02:53s\n",
            "epoch 21 | loss: 0.11511 | train_auc: 0.99116 | valid_auc: 0.99035 |  0:03:02s\n",
            "epoch 22 | loss: 0.1106  | train_auc: 0.98794 | valid_auc: 0.9876  |  0:03:10s\n",
            "epoch 23 | loss: 0.10511 | train_auc: 0.99102 | valid_auc: 0.99064 |  0:03:18s\n",
            "epoch 24 | loss: 0.10182 | train_auc: 0.99246 | valid_auc: 0.992   |  0:03:27s\n",
            "epoch 25 | loss: 0.0996  | train_auc: 0.99342 | valid_auc: 0.99287 |  0:03:35s\n",
            "epoch 26 | loss: 0.09853 | train_auc: 0.99399 | valid_auc: 0.99342 |  0:03:43s\n",
            "epoch 27 | loss: 0.0973  | train_auc: 0.99419 | valid_auc: 0.99361 |  0:03:52s\n",
            "epoch 28 | loss: 0.09632 | train_auc: 0.99356 | valid_auc: 0.99278 |  0:04:00s\n",
            "epoch 29 | loss: 0.095   | train_auc: 0.99435 | valid_auc: 0.99362 |  0:04:08s\n",
            "epoch 30 | loss: 0.09496 | train_auc: 0.99374 | valid_auc: 0.99299 |  0:04:17s\n",
            "epoch 31 | loss: 0.09438 | train_auc: 0.9944  | valid_auc: 0.99372 |  0:04:25s\n",
            "epoch 32 | loss: 0.09391 | train_auc: 0.99456 | valid_auc: 0.99382 |  0:04:33s\n",
            "epoch 33 | loss: 0.09345 | train_auc: 0.99455 | valid_auc: 0.99386 |  0:04:42s\n",
            "epoch 34 | loss: 0.09294 | train_auc: 0.99499 | valid_auc: 0.99428 |  0:04:50s\n",
            "epoch 35 | loss: 0.09083 | train_auc: 0.99473 | valid_auc: 0.99402 |  0:04:58s\n",
            "epoch 36 | loss: 0.09038 | train_auc: 0.9945  | valid_auc: 0.99366 |  0:05:07s\n",
            "epoch 37 | loss: 0.09016 | train_auc: 0.99495 | valid_auc: 0.99419 |  0:05:15s\n",
            "epoch 38 | loss: 0.0889  | train_auc: 0.99495 | valid_auc: 0.99423 |  0:05:23s\n",
            "epoch 39 | loss: 0.08835 | train_auc: 0.99528 | valid_auc: 0.99463 |  0:05:32s\n",
            "epoch 40 | loss: 0.08747 | train_auc: 0.99553 | valid_auc: 0.99488 |  0:05:40s\n",
            "epoch 41 | loss: 0.08673 | train_auc: 0.99554 | valid_auc: 0.99482 |  0:05:48s\n",
            "epoch 42 | loss: 0.08577 | train_auc: 0.99558 | valid_auc: 0.99485 |  0:05:56s\n",
            "epoch 43 | loss: 0.0857  | train_auc: 0.99549 | valid_auc: 0.99473 |  0:06:05s\n",
            "epoch 44 | loss: 0.08542 | train_auc: 0.99564 | valid_auc: 0.99507 |  0:06:13s\n",
            "epoch 45 | loss: 0.08587 | train_auc: 0.99552 | valid_auc: 0.99492 |  0:06:22s\n",
            "epoch 46 | loss: 0.08466 | train_auc: 0.99572 | valid_auc: 0.99506 |  0:06:30s\n",
            "epoch 47 | loss: 0.08387 | train_auc: 0.99595 | valid_auc: 0.99529 |  0:06:38s\n",
            "epoch 48 | loss: 0.0833  | train_auc: 0.99574 | valid_auc: 0.99503 |  0:06:46s\n",
            "epoch 49 | loss: 0.08383 | train_auc: 0.99589 | valid_auc: 0.99519 |  0:06:55s\n",
            "epoch 50 | loss: 0.0837  | train_auc: 0.9958  | valid_auc: 0.99512 |  0:07:03s\n",
            "epoch 51 | loss: 0.08301 | train_auc: 0.996   | valid_auc: 0.99532 |  0:07:11s\n",
            "epoch 52 | loss: 0.0827  | train_auc: 0.99607 | valid_auc: 0.99536 |  0:07:19s\n",
            "epoch 53 | loss: 0.08251 | train_auc: 0.99586 | valid_auc: 0.99511 |  0:07:27s\n",
            "epoch 54 | loss: 0.08344 | train_auc: 0.99613 | valid_auc: 0.99544 |  0:07:36s\n",
            "epoch 55 | loss: 0.08219 | train_auc: 0.99605 | valid_auc: 0.99538 |  0:07:44s\n",
            "epoch 56 | loss: 0.08179 | train_auc: 0.99606 | valid_auc: 0.99535 |  0:07:52s\n",
            "epoch 57 | loss: 0.08215 | train_auc: 0.99619 | valid_auc: 0.99549 |  0:08:00s\n",
            "epoch 58 | loss: 0.08167 | train_auc: 0.99609 | valid_auc: 0.99532 |  0:08:09s\n",
            "epoch 59 | loss: 0.08092 | train_auc: 0.99617 | valid_auc: 0.99545 |  0:08:17s\n",
            "epoch 60 | loss: 0.08113 | train_auc: 0.99613 | valid_auc: 0.99545 |  0:08:26s\n",
            "epoch 61 | loss: 0.08209 | train_auc: 0.9961  | valid_auc: 0.99533 |  0:08:34s\n",
            "epoch 62 | loss: 0.08184 | train_auc: 0.99618 | valid_auc: 0.9954  |  0:08:42s\n",
            "epoch 63 | loss: 0.08104 | train_auc: 0.99438 | valid_auc: 0.99377 |  0:08:50s\n",
            "epoch 64 | loss: 0.08081 | train_auc: 0.98921 | valid_auc: 0.98851 |  0:08:59s\n",
            "epoch 65 | loss: 0.08004 | train_auc: 0.99381 | valid_auc: 0.99312 |  0:09:07s\n",
            "epoch 66 | loss: 0.08005 | train_auc: 0.99567 | valid_auc: 0.99494 |  0:09:15s\n",
            "epoch 67 | loss: 0.07959 | train_auc: 0.99636 | valid_auc: 0.99567 |  0:09:24s\n",
            "epoch 68 | loss: 0.07932 | train_auc: 0.99631 | valid_auc: 0.99553 |  0:09:32s\n",
            "epoch 69 | loss: 0.07857 | train_auc: 0.99618 | valid_auc: 0.99537 |  0:09:41s\n",
            "epoch 70 | loss: 0.07837 | train_auc: 0.99611 | valid_auc: 0.99534 |  0:09:49s\n",
            "epoch 71 | loss: 0.07806 | train_auc: 0.99603 | valid_auc: 0.99522 |  0:09:58s\n",
            "epoch 72 | loss: 0.07819 | train_auc: 0.99594 | valid_auc: 0.99511 |  0:10:06s\n",
            "epoch 73 | loss: 0.07818 | train_auc: 0.9961  | valid_auc: 0.99532 |  0:10:14s\n",
            "epoch 74 | loss: 0.07844 | train_auc: 0.99616 | valid_auc: 0.99541 |  0:10:22s\n",
            "epoch 75 | loss: 0.0786  | train_auc: 0.99608 | valid_auc: 0.99529 |  0:10:30s\n",
            "epoch 76 | loss: 0.07862 | train_auc: 0.99653 | valid_auc: 0.99576 |  0:10:38s\n",
            "epoch 77 | loss: 0.07802 | train_auc: 0.99644 | valid_auc: 0.99566 |  0:10:47s\n",
            "epoch 78 | loss: 0.0781  | train_auc: 0.99649 | valid_auc: 0.99577 |  0:10:55s\n",
            "epoch 79 | loss: 0.07792 | train_auc: 0.99657 | valid_auc: 0.99582 |  0:11:04s\n",
            "epoch 80 | loss: 0.07789 | train_auc: 0.99648 | valid_auc: 0.99569 |  0:11:12s\n",
            "epoch 81 | loss: 0.07705 | train_auc: 0.99643 | valid_auc: 0.99566 |  0:11:21s\n",
            "epoch 82 | loss: 0.07669 | train_auc: 0.99653 | valid_auc: 0.99573 |  0:11:29s\n",
            "epoch 83 | loss: 0.0766  | train_auc: 0.99652 | valid_auc: 0.99577 |  0:11:37s\n",
            "epoch 84 | loss: 0.07635 | train_auc: 0.99647 | valid_auc: 0.9957  |  0:11:45s\n",
            "epoch 85 | loss: 0.07657 | train_auc: 0.99651 | valid_auc: 0.99571 |  0:11:54s\n",
            "epoch 86 | loss: 0.07607 | train_auc: 0.99648 | valid_auc: 0.99568 |  0:12:02s\n",
            "epoch 87 | loss: 0.07627 | train_auc: 0.99647 | valid_auc: 0.99573 |  0:12:10s\n",
            "epoch 88 | loss: 0.07603 | train_auc: 0.99605 | valid_auc: 0.99527 |  0:12:19s\n",
            "epoch 89 | loss: 0.07596 | train_auc: 0.99644 | valid_auc: 0.99565 |  0:12:27s\n",
            "epoch 90 | loss: 0.07588 | train_auc: 0.99667 | valid_auc: 0.99584 |  0:12:35s\n",
            "epoch 91 | loss: 0.07659 | train_auc: 0.99666 | valid_auc: 0.99591 |  0:12:44s\n",
            "epoch 92 | loss: 0.07611 | train_auc: 0.99652 | valid_auc: 0.99574 |  0:12:52s\n",
            "epoch 93 | loss: 0.07675 | train_auc: 0.99669 | valid_auc: 0.99596 |  0:13:00s\n",
            "epoch 94 | loss: 0.07578 | train_auc: 0.99676 | valid_auc: 0.99598 |  0:13:08s\n",
            "epoch 95 | loss: 0.07544 | train_auc: 0.99663 | valid_auc: 0.9958  |  0:13:17s\n",
            "epoch 96 | loss: 0.07579 | train_auc: 0.99681 | valid_auc: 0.99606 |  0:13:25s\n",
            "epoch 97 | loss: 0.07576 | train_auc: 0.9967  | valid_auc: 0.99595 |  0:13:33s\n",
            "epoch 98 | loss: 0.07559 | train_auc: 0.99676 | valid_auc: 0.99597 |  0:13:41s\n",
            "epoch 99 | loss: 0.07504 | train_auc: 0.99678 | valid_auc: 0.99605 |  0:13:50s\n",
            "epoch 100| loss: 0.07511 | train_auc: 0.99684 | valid_auc: 0.99608 |  0:13:58s\n",
            "epoch 101| loss: 0.07439 | train_auc: 0.99679 | valid_auc: 0.99601 |  0:14:06s\n",
            "epoch 102| loss: 0.07459 | train_auc: 0.99678 | valid_auc: 0.99599 |  0:14:14s\n",
            "epoch 103| loss: 0.07476 | train_auc: 0.99684 | valid_auc: 0.99614 |  0:14:23s\n",
            "epoch 104| loss: 0.0754  | train_auc: 0.99683 | valid_auc: 0.9961  |  0:14:31s\n",
            "epoch 105| loss: 0.07512 | train_auc: 0.9968  | valid_auc: 0.99612 |  0:14:39s\n",
            "epoch 106| loss: 0.07448 | train_auc: 0.99664 | valid_auc: 0.99588 |  0:14:48s\n",
            "epoch 107| loss: 0.07459 | train_auc: 0.99673 | valid_auc: 0.996   |  0:14:56s\n",
            "epoch 108| loss: 0.0756  | train_auc: 0.99684 | valid_auc: 0.99615 |  0:15:05s\n",
            "epoch 109| loss: 0.07526 | train_auc: 0.99682 | valid_auc: 0.9961  |  0:15:13s\n",
            "epoch 110| loss: 0.07526 | train_auc: 0.99663 | valid_auc: 0.99583 |  0:15:21s\n",
            "epoch 111| loss: 0.07536 | train_auc: 0.9968  | valid_auc: 0.99609 |  0:15:30s\n",
            "epoch 112| loss: 0.0748  | train_auc: 0.99687 | valid_auc: 0.99617 |  0:15:38s\n",
            "epoch 113| loss: 0.07384 | train_auc: 0.99682 | valid_auc: 0.99604 |  0:15:46s\n",
            "epoch 114| loss: 0.07387 | train_auc: 0.99696 | valid_auc: 0.99621 |  0:15:54s\n",
            "epoch 115| loss: 0.07398 | train_auc: 0.9969  | valid_auc: 0.99615 |  0:16:03s\n",
            "epoch 116| loss: 0.07443 | train_auc: 0.99685 | valid_auc: 0.99607 |  0:16:11s\n",
            "epoch 117| loss: 0.07417 | train_auc: 0.99686 | valid_auc: 0.99611 |  0:16:19s\n",
            "epoch 118| loss: 0.07431 | train_auc: 0.99684 | valid_auc: 0.99615 |  0:16:28s\n",
            "epoch 119| loss: 0.07416 | train_auc: 0.99689 | valid_auc: 0.99611 |  0:16:36s\n",
            "epoch 120| loss: 0.07396 | train_auc: 0.99692 | valid_auc: 0.9962  |  0:16:44s\n",
            "epoch 121| loss: 0.07373 | train_auc: 0.99689 | valid_auc: 0.99621 |  0:16:53s\n",
            "epoch 122| loss: 0.07429 | train_auc: 0.9968  | valid_auc: 0.99603 |  0:17:01s\n",
            "epoch 123| loss: 0.07396 | train_auc: 0.99684 | valid_auc: 0.9961  |  0:17:10s\n",
            "epoch 124| loss: 0.07391 | train_auc: 0.99692 | valid_auc: 0.99618 |  0:17:18s\n",
            "epoch 125| loss: 0.07381 | train_auc: 0.99701 | valid_auc: 0.99631 |  0:17:26s\n",
            "epoch 126| loss: 0.07363 | train_auc: 0.99688 | valid_auc: 0.99617 |  0:17:34s\n",
            "epoch 127| loss: 0.07334 | train_auc: 0.997   | valid_auc: 0.99625 |  0:17:43s\n",
            "epoch 128| loss: 0.0736  | train_auc: 0.99695 | valid_auc: 0.9962  |  0:17:51s\n",
            "epoch 129| loss: 0.07331 | train_auc: 0.99691 | valid_auc: 0.99621 |  0:17:59s\n",
            "epoch 130| loss: 0.07307 | train_auc: 0.9969  | valid_auc: 0.99617 |  0:18:08s\n",
            "epoch 131| loss: 0.07338 | train_auc: 0.99658 | valid_auc: 0.99583 |  0:18:16s\n",
            "epoch 132| loss: 0.07384 | train_auc: 0.99672 | valid_auc: 0.99605 |  0:18:25s\n",
            "epoch 133| loss: 0.07294 | train_auc: 0.99697 | valid_auc: 0.99623 |  0:18:33s\n",
            "epoch 134| loss: 0.07301 | train_auc: 0.99703 | valid_auc: 0.99627 |  0:18:41s\n",
            "epoch 135| loss: 0.07274 | train_auc: 0.99697 | valid_auc: 0.99626 |  0:18:50s\n",
            "epoch 136| loss: 0.07332 | train_auc: 0.99689 | valid_auc: 0.99615 |  0:18:58s\n",
            "epoch 137| loss: 0.07275 | train_auc: 0.997   | valid_auc: 0.99627 |  0:19:07s\n",
            "epoch 138| loss: 0.07293 | train_auc: 0.99668 | valid_auc: 0.99596 |  0:19:15s\n",
            "epoch 139| loss: 0.0728  | train_auc: 0.9967  | valid_auc: 0.99597 |  0:19:23s\n",
            "epoch 140| loss: 0.07264 | train_auc: 0.99686 | valid_auc: 0.99613 |  0:19:31s\n",
            "epoch 141| loss: 0.07275 | train_auc: 0.99689 | valid_auc: 0.99615 |  0:19:40s\n",
            "epoch 142| loss: 0.07294 | train_auc: 0.99698 | valid_auc: 0.99625 |  0:19:48s\n",
            "epoch 143| loss: 0.07288 | train_auc: 0.99698 | valid_auc: 0.99624 |  0:19:57s\n",
            "epoch 144| loss: 0.07266 | train_auc: 0.99698 | valid_auc: 0.99617 |  0:20:05s\n",
            "epoch 145| loss: 0.07173 | train_auc: 0.99697 | valid_auc: 0.99622 |  0:20:13s\n",
            "\n",
            "Early stopping occurred at epoch 145 with best_epoch = 125 and best_valid_auc = 0.99631\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9963102054632936\n",
            "----- 2 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 17819799.23466| val_0_unsup_loss: 1639.02502|  0:00:05s\n",
            "epoch 1  | loss: 4311.23117| val_0_unsup_loss: 2.41144 |  0:00:10s\n",
            "epoch 2  | loss: 8.12063 | val_0_unsup_loss: 1.9299  |  0:00:16s\n",
            "epoch 3  | loss: 2.53567 | val_0_unsup_loss: 1.65893 |  0:00:21s\n",
            "epoch 4  | loss: 3.96455 | val_0_unsup_loss: 1.5131  |  0:00:27s\n",
            "epoch 5  | loss: 1.59993 | val_0_unsup_loss: 1.42389 |  0:00:32s\n",
            "epoch 6  | loss: 1.84204 | val_0_unsup_loss: 1.38285 |  0:00:37s\n",
            "epoch 7  | loss: 3.13939 | val_0_unsup_loss: 1.36637 |  0:00:43s\n",
            "epoch 8  | loss: 1.36566 | val_0_unsup_loss: 1.36258 |  0:00:48s\n",
            "epoch 9  | loss: 1.36355 | val_0_unsup_loss: 1.3587  |  0:00:54s\n",
            "epoch 10 | loss: 2.68735 | val_0_unsup_loss: 1.35631 |  0:00:59s\n",
            "epoch 11 | loss: 3.13948 | val_0_unsup_loss: 1.35609 |  0:01:04s\n",
            "epoch 12 | loss: 1.35599 | val_0_unsup_loss: 1.35569 |  0:01:10s\n",
            "epoch 13 | loss: 1.36209 | val_0_unsup_loss: 1.35524 |  0:01:15s\n",
            "epoch 14 | loss: 1.41924 | val_0_unsup_loss: 1.35508 |  0:01:21s\n",
            "epoch 15 | loss: 1.36112 | val_0_unsup_loss: 1.35471 |  0:01:26s\n",
            "epoch 16 | loss: 2.56576 | val_0_unsup_loss: 1.35458 |  0:01:31s\n",
            "epoch 17 | loss: 1.35421 | val_0_unsup_loss: 1.35448 |  0:01:37s\n",
            "epoch 18 | loss: 1.35409 | val_0_unsup_loss: 1.3544  |  0:01:42s\n",
            "epoch 19 | loss: 1.35433 | val_0_unsup_loss: 1.35437 |  0:01:47s\n",
            "epoch 20 | loss: 1.35434 | val_0_unsup_loss: 1.35436 |  0:01:53s\n",
            "epoch 21 | loss: 1.35486 | val_0_unsup_loss: 1.35437 |  0:01:58s\n",
            "epoch 22 | loss: 2.30825 | val_0_unsup_loss: 1.35447 |  0:02:04s\n",
            "epoch 23 | loss: 1.39672 | val_0_unsup_loss: 1.35448 |  0:02:09s\n",
            "epoch 24 | loss: 1.40388 | val_0_unsup_loss: 1.35458 |  0:02:14s\n",
            "epoch 25 | loss: 2.00585 | val_0_unsup_loss: 1.35461 |  0:02:20s\n",
            "epoch 26 | loss: 1.36177 | val_0_unsup_loss: 1.35516 |  0:02:25s\n",
            "epoch 27 | loss: 1.63871 | val_0_unsup_loss: 1.35508 |  0:02:30s\n",
            "epoch 28 | loss: 1.35481 | val_0_unsup_loss: 1.35452 |  0:02:36s\n",
            "epoch 29 | loss: 1.43715 | val_0_unsup_loss: 1.39418 |  0:02:41s\n",
            "epoch 30 | loss: 1.56125 | val_0_unsup_loss: 1.35522 |  0:02:47s\n",
            "epoch 31 | loss: 1.35435 | val_0_unsup_loss: 1.35438 |  0:02:52s\n",
            "epoch 32 | loss: 1.49038 | val_0_unsup_loss: 1.35439 |  0:02:58s\n",
            "epoch 33 | loss: 1.35397 | val_0_unsup_loss: 1.35436 |  0:03:03s\n",
            "epoch 34 | loss: 1.35457 | val_0_unsup_loss: 1.35435 |  0:03:08s\n",
            "epoch 35 | loss: 1.35427 | val_0_unsup_loss: 1.35435 |  0:03:14s\n",
            "epoch 36 | loss: 1.35455 | val_0_unsup_loss: 1.35434 |  0:03:19s\n",
            "epoch 37 | loss: 1.35422 | val_0_unsup_loss: 1.35434 |  0:03:24s\n",
            "epoch 38 | loss: 1.35404 | val_0_unsup_loss: 1.35434 |  0:03:30s\n",
            "epoch 39 | loss: 1.35412 | val_0_unsup_loss: 1.35434 |  0:03:35s\n",
            "epoch 40 | loss: 1.35408 | val_0_unsup_loss: 1.35434 |  0:03:40s\n",
            "epoch 41 | loss: 1.35387 | val_0_unsup_loss: 1.35434 |  0:03:46s\n",
            "epoch 42 | loss: 1.35389 | val_0_unsup_loss: 1.35434 |  0:03:51s\n",
            "epoch 43 | loss: 1.35964 | val_0_unsup_loss: 1.35437 |  0:03:57s\n",
            "epoch 44 | loss: 1.3541  | val_0_unsup_loss: 1.35436 |  0:04:02s\n",
            "epoch 45 | loss: 1.35449 | val_0_unsup_loss: 1.35434 |  0:04:07s\n",
            "epoch 46 | loss: 1.35402 | val_0_unsup_loss: 1.35434 |  0:04:13s\n",
            "epoch 47 | loss: 6.08705 | val_0_unsup_loss: 1.35436 |  0:04:18s\n",
            "epoch 48 | loss: 1.35405 | val_0_unsup_loss: 1.35434 |  0:04:24s\n",
            "epoch 49 | loss: 1.35426 | val_0_unsup_loss: 1.35434 |  0:04:29s\n",
            "epoch 50 | loss: 1.3542  | val_0_unsup_loss: 1.35434 |  0:04:34s\n",
            "epoch 51 | loss: 1.3541  | val_0_unsup_loss: 1.35434 |  0:04:40s\n",
            "epoch 52 | loss: 1.3541  | val_0_unsup_loss: 1.35434 |  0:04:45s\n",
            "epoch 53 | loss: 1.35397 | val_0_unsup_loss: 1.35434 |  0:04:51s\n",
            "epoch 54 | loss: 1.35401 | val_0_unsup_loss: 1.35434 |  0:04:56s\n",
            "epoch 55 | loss: 1.35417 | val_0_unsup_loss: 1.35434 |  0:05:01s\n",
            "epoch 56 | loss: 1.35429 | val_0_unsup_loss: 1.35434 |  0:05:07s\n",
            "epoch 57 | loss: 1.35416 | val_0_unsup_loss: 1.35434 |  0:05:12s\n",
            "epoch 58 | loss: 1.35394 | val_0_unsup_loss: 1.35434 |  0:05:17s\n",
            "epoch 59 | loss: 1.35408 | val_0_unsup_loss: 1.35434 |  0:05:23s\n",
            "epoch 60 | loss: 1.35398 | val_0_unsup_loss: 1.35434 |  0:05:28s\n",
            "epoch 61 | loss: 1.35402 | val_0_unsup_loss: 1.35434 |  0:05:33s\n",
            "epoch 62 | loss: 1.35426 | val_0_unsup_loss: 1.35434 |  0:05:39s\n",
            "epoch 63 | loss: 1.35429 | val_0_unsup_loss: 1.35434 |  0:05:44s\n",
            "epoch 64 | loss: 1.35417 | val_0_unsup_loss: 1.35434 |  0:05:50s\n",
            "epoch 65 | loss: 1.35402 | val_0_unsup_loss: 1.35434 |  0:05:55s\n",
            "epoch 66 | loss: 1.35417 | val_0_unsup_loss: 1.35434 |  0:06:00s\n",
            "epoch 67 | loss: 1.35398 | val_0_unsup_loss: 1.35434 |  0:06:06s\n",
            "epoch 68 | loss: 1.3538  | val_0_unsup_loss: 1.35434 |  0:06:11s\n",
            "epoch 69 | loss: 1.35417 | val_0_unsup_loss: 1.35434 |  0:06:16s\n",
            "epoch 70 | loss: 1.35443 | val_0_unsup_loss: 1.35434 |  0:06:22s\n",
            "epoch 71 | loss: 1.35387 | val_0_unsup_loss: 1.35434 |  0:06:27s\n",
            "epoch 72 | loss: 1.35393 | val_0_unsup_loss: 1.35434 |  0:06:33s\n",
            "epoch 73 | loss: 1.35435 | val_0_unsup_loss: 1.35434 |  0:06:38s\n",
            "epoch 74 | loss: 1.35391 | val_0_unsup_loss: 1.35434 |  0:06:43s\n",
            "epoch 75 | loss: 1.35407 | val_0_unsup_loss: 1.35434 |  0:06:48s\n",
            "epoch 76 | loss: 1.35423 | val_0_unsup_loss: 1.35434 |  0:06:54s\n",
            "epoch 77 | loss: 1.35416 | val_0_unsup_loss: 1.35434 |  0:06:59s\n",
            "epoch 78 | loss: 1.35422 | val_0_unsup_loss: 1.35434 |  0:07:05s\n",
            "epoch 79 | loss: 1.35426 | val_0_unsup_loss: 1.35434 |  0:07:10s\n",
            "epoch 80 | loss: 1.35406 | val_0_unsup_loss: 1.35434 |  0:07:15s\n",
            "epoch 81 | loss: 1.354   | val_0_unsup_loss: 1.35434 |  0:07:21s\n",
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 61 and best_val_0_unsup_loss = 1.35434\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.68963 | train_auc: 0.56124 | valid_auc: 0.55821 |  0:00:08s\n",
            "epoch 1  | loss: 0.60342 | train_auc: 0.65568 | valid_auc: 0.65687 |  0:00:16s\n",
            "epoch 2  | loss: 0.406   | train_auc: 0.58737 | valid_auc: 0.58889 |  0:00:25s\n",
            "epoch 3  | loss: 0.26836 | train_auc: 0.56289 | valid_auc: 0.56435 |  0:00:33s\n",
            "epoch 4  | loss: 0.21269 | train_auc: 0.57573 | valid_auc: 0.57663 |  0:00:42s\n",
            "epoch 5  | loss: 0.18844 | train_auc: 0.62747 | valid_auc: 0.6259  |  0:00:50s\n",
            "epoch 6  | loss: 0.17803 | train_auc: 0.60279 | valid_auc: 0.60249 |  0:00:59s\n",
            "epoch 7  | loss: 0.17136 | train_auc: 0.63777 | valid_auc: 0.63781 |  0:01:07s\n",
            "epoch 8  | loss: 0.16714 | train_auc: 0.66124 | valid_auc: 0.66024 |  0:01:15s\n",
            "epoch 9  | loss: 0.16374 | train_auc: 0.70376 | valid_auc: 0.7037  |  0:01:24s\n",
            "epoch 10 | loss: 0.16148 | train_auc: 0.74434 | valid_auc: 0.74403 |  0:01:32s\n",
            "epoch 11 | loss: 0.15667 | train_auc: 0.8571  | valid_auc: 0.85752 |  0:01:41s\n",
            "epoch 12 | loss: 0.15153 | train_auc: 0.92624 | valid_auc: 0.9275  |  0:01:49s\n",
            "epoch 13 | loss: 0.14199 | train_auc: 0.96017 | valid_auc: 0.96099 |  0:01:57s\n",
            "epoch 14 | loss: 0.13458 | train_auc: 0.97873 | valid_auc: 0.97901 |  0:02:06s\n",
            "epoch 15 | loss: 0.13069 | train_auc: 0.98427 | valid_auc: 0.98434 |  0:02:15s\n",
            "epoch 16 | loss: 0.12683 | train_auc: 0.98692 | valid_auc: 0.98696 |  0:02:23s\n",
            "epoch 17 | loss: 0.12472 | train_auc: 0.98755 | valid_auc: 0.98761 |  0:02:31s\n",
            "epoch 18 | loss: 0.12304 | train_auc: 0.98931 | valid_auc: 0.98934 |  0:02:40s\n",
            "epoch 19 | loss: 0.12173 | train_auc: 0.9869  | valid_auc: 0.98698 |  0:02:48s\n",
            "epoch 20 | loss: 0.12016 | train_auc: 0.98838 | valid_auc: 0.98834 |  0:02:57s\n",
            "epoch 21 | loss: 0.11898 | train_auc: 0.99025 | valid_auc: 0.99012 |  0:03:05s\n",
            "epoch 22 | loss: 0.11773 | train_auc: 0.98972 | valid_auc: 0.98961 |  0:03:13s\n",
            "epoch 23 | loss: 0.11707 | train_auc: 0.98957 | valid_auc: 0.98953 |  0:03:22s\n",
            "epoch 24 | loss: 0.11579 | train_auc: 0.9902  | valid_auc: 0.99017 |  0:03:30s\n",
            "epoch 25 | loss: 0.11424 | train_auc: 0.99077 | valid_auc: 0.99069 |  0:03:39s\n",
            "epoch 26 | loss: 0.114   | train_auc: 0.99069 | valid_auc: 0.99063 |  0:03:47s\n",
            "epoch 27 | loss: 0.11311 | train_auc: 0.98976 | valid_auc: 0.98963 |  0:03:56s\n",
            "epoch 28 | loss: 0.11254 | train_auc: 0.99197 | valid_auc: 0.99186 |  0:04:04s\n",
            "epoch 29 | loss: 0.11083 | train_auc: 0.99155 | valid_auc: 0.99141 |  0:04:13s\n",
            "epoch 30 | loss: 0.10944 | train_auc: 0.99233 | valid_auc: 0.99225 |  0:04:21s\n",
            "epoch 31 | loss: 0.10887 | train_auc: 0.99199 | valid_auc: 0.99188 |  0:04:29s\n",
            "epoch 32 | loss: 0.10834 | train_auc: 0.99165 | valid_auc: 0.99151 |  0:04:38s\n",
            "epoch 33 | loss: 0.10974 | train_auc: 0.99283 | valid_auc: 0.99265 |  0:04:46s\n",
            "epoch 34 | loss: 0.10866 | train_auc: 0.99295 | valid_auc: 0.99276 |  0:04:54s\n",
            "epoch 35 | loss: 0.10743 | train_auc: 0.99308 | valid_auc: 0.99295 |  0:05:03s\n",
            "epoch 36 | loss: 0.1059  | train_auc: 0.99325 | valid_auc: 0.99306 |  0:05:11s\n",
            "epoch 37 | loss: 0.10496 | train_auc: 0.9932  | valid_auc: 0.99307 |  0:05:20s\n",
            "epoch 38 | loss: 0.10398 | train_auc: 0.99359 | valid_auc: 0.9935  |  0:05:28s\n",
            "epoch 39 | loss: 0.1035  | train_auc: 0.99339 | valid_auc: 0.99329 |  0:05:37s\n",
            "epoch 40 | loss: 0.10302 | train_auc: 0.99303 | valid_auc: 0.99279 |  0:05:45s\n",
            "epoch 41 | loss: 0.10248 | train_auc: 0.99238 | valid_auc: 0.99217 |  0:05:53s\n",
            "epoch 42 | loss: 0.09946 | train_auc: 0.99372 | valid_auc: 0.99358 |  0:06:02s\n",
            "epoch 43 | loss: 0.09876 | train_auc: 0.9942  | valid_auc: 0.99408 |  0:06:10s\n",
            "epoch 44 | loss: 0.09783 | train_auc: 0.99445 | valid_auc: 0.99434 |  0:06:19s\n",
            "epoch 45 | loss: 0.09708 | train_auc: 0.99401 | valid_auc: 0.99396 |  0:06:27s\n",
            "epoch 46 | loss: 0.09694 | train_auc: 0.99394 | valid_auc: 0.9938  |  0:06:36s\n",
            "epoch 47 | loss: 0.0968  | train_auc: 0.99452 | valid_auc: 0.9944  |  0:06:44s\n",
            "epoch 48 | loss: 0.09636 | train_auc: 0.99446 | valid_auc: 0.99436 |  0:06:53s\n",
            "epoch 49 | loss: 0.09547 | train_auc: 0.9947  | valid_auc: 0.99455 |  0:07:01s\n",
            "epoch 50 | loss: 0.09582 | train_auc: 0.99449 | valid_auc: 0.99438 |  0:07:10s\n",
            "epoch 51 | loss: 0.09488 | train_auc: 0.99447 | valid_auc: 0.99437 |  0:07:18s\n",
            "epoch 52 | loss: 0.0938  | train_auc: 0.99439 | valid_auc: 0.99426 |  0:07:27s\n",
            "epoch 53 | loss: 0.09334 | train_auc: 0.99446 | valid_auc: 0.99438 |  0:07:35s\n",
            "epoch 54 | loss: 0.0948  | train_auc: 0.99475 | valid_auc: 0.9946  |  0:07:43s\n",
            "epoch 55 | loss: 0.09331 | train_auc: 0.9948  | valid_auc: 0.9946  |  0:07:52s\n",
            "epoch 56 | loss: 0.09309 | train_auc: 0.99495 | valid_auc: 0.99481 |  0:08:00s\n",
            "epoch 57 | loss: 0.09308 | train_auc: 0.99502 | valid_auc: 0.99483 |  0:08:09s\n",
            "epoch 58 | loss: 0.09205 | train_auc: 0.99482 | valid_auc: 0.99465 |  0:08:17s\n",
            "epoch 59 | loss: 0.09199 | train_auc: 0.99492 | valid_auc: 0.99475 |  0:08:26s\n",
            "epoch 60 | loss: 0.09122 | train_auc: 0.99524 | valid_auc: 0.99507 |  0:08:34s\n",
            "epoch 61 | loss: 0.09076 | train_auc: 0.99522 | valid_auc: 0.99505 |  0:08:43s\n",
            "epoch 62 | loss: 0.08957 | train_auc: 0.99526 | valid_auc: 0.99506 |  0:08:51s\n",
            "epoch 63 | loss: 0.08948 | train_auc: 0.99531 | valid_auc: 0.9951  |  0:09:00s\n",
            "epoch 64 | loss: 0.08924 | train_auc: 0.99556 | valid_auc: 0.99533 |  0:09:08s\n",
            "epoch 65 | loss: 0.08857 | train_auc: 0.99541 | valid_auc: 0.99513 |  0:09:17s\n",
            "epoch 66 | loss: 0.08807 | train_auc: 0.99542 | valid_auc: 0.99518 |  0:09:26s\n",
            "epoch 67 | loss: 0.08773 | train_auc: 0.99531 | valid_auc: 0.99496 |  0:09:34s\n",
            "epoch 68 | loss: 0.08708 | train_auc: 0.99545 | valid_auc: 0.99513 |  0:09:42s\n",
            "epoch 69 | loss: 0.08709 | train_auc: 0.99585 | valid_auc: 0.99551 |  0:09:51s\n",
            "epoch 70 | loss: 0.08624 | train_auc: 0.99566 | valid_auc: 0.9953  |  0:09:59s\n",
            "epoch 71 | loss: 0.08683 | train_auc: 0.99548 | valid_auc: 0.99524 |  0:10:08s\n",
            "epoch 72 | loss: 0.0862  | train_auc: 0.99573 | valid_auc: 0.99547 |  0:10:16s\n",
            "epoch 73 | loss: 0.08575 | train_auc: 0.99544 | valid_auc: 0.995   |  0:10:25s\n",
            "epoch 74 | loss: 0.08595 | train_auc: 0.9955  | valid_auc: 0.99511 |  0:10:34s\n",
            "epoch 75 | loss: 0.08575 | train_auc: 0.99566 | valid_auc: 0.99532 |  0:10:42s\n",
            "epoch 76 | loss: 0.08574 | train_auc: 0.99592 | valid_auc: 0.99556 |  0:10:51s\n",
            "epoch 77 | loss: 0.08506 | train_auc: 0.99566 | valid_auc: 0.99535 |  0:10:59s\n",
            "epoch 78 | loss: 0.08522 | train_auc: 0.99588 | valid_auc: 0.99556 |  0:11:08s\n",
            "epoch 79 | loss: 0.08441 | train_auc: 0.99596 | valid_auc: 0.99559 |  0:11:16s\n",
            "epoch 80 | loss: 0.08471 | train_auc: 0.99588 | valid_auc: 0.99556 |  0:11:25s\n",
            "epoch 81 | loss: 0.08403 | train_auc: 0.99591 | valid_auc: 0.99553 |  0:11:33s\n",
            "epoch 82 | loss: 0.08416 | train_auc: 0.99564 | valid_auc: 0.99526 |  0:11:42s\n",
            "epoch 83 | loss: 0.08335 | train_auc: 0.99588 | valid_auc: 0.99555 |  0:11:50s\n",
            "epoch 84 | loss: 0.08242 | train_auc: 0.99604 | valid_auc: 0.99568 |  0:11:59s\n",
            "epoch 85 | loss: 0.08241 | train_auc: 0.99556 | valid_auc: 0.99516 |  0:12:07s\n",
            "epoch 86 | loss: 0.08288 | train_auc: 0.99601 | valid_auc: 0.99569 |  0:12:15s\n",
            "epoch 87 | loss: 0.08364 | train_auc: 0.99589 | valid_auc: 0.99555 |  0:12:24s\n",
            "epoch 88 | loss: 0.08275 | train_auc: 0.9962  | valid_auc: 0.99586 |  0:12:32s\n",
            "epoch 89 | loss: 0.08171 | train_auc: 0.99616 | valid_auc: 0.99583 |  0:12:41s\n",
            "epoch 90 | loss: 0.08215 | train_auc: 0.99613 | valid_auc: 0.9957  |  0:12:50s\n",
            "epoch 91 | loss: 0.08147 | train_auc: 0.99613 | valid_auc: 0.99581 |  0:12:58s\n",
            "epoch 92 | loss: 0.08194 | train_auc: 0.99626 | valid_auc: 0.99594 |  0:13:06s\n",
            "epoch 93 | loss: 0.08247 | train_auc: 0.99588 | valid_auc: 0.99551 |  0:13:15s\n",
            "epoch 94 | loss: 0.08187 | train_auc: 0.99499 | valid_auc: 0.99461 |  0:13:23s\n",
            "epoch 95 | loss: 0.08201 | train_auc: 0.99612 | valid_auc: 0.99579 |  0:13:32s\n",
            "epoch 96 | loss: 0.08089 | train_auc: 0.99624 | valid_auc: 0.99589 |  0:13:40s\n",
            "epoch 97 | loss: 0.08081 | train_auc: 0.99624 | valid_auc: 0.99586 |  0:13:49s\n",
            "epoch 98 | loss: 0.08133 | train_auc: 0.99634 | valid_auc: 0.996   |  0:13:57s\n",
            "epoch 99 | loss: 0.0803  | train_auc: 0.9962  | valid_auc: 0.99588 |  0:14:06s\n",
            "epoch 100| loss: 0.08102 | train_auc: 0.99538 | valid_auc: 0.99511 |  0:14:14s\n",
            "epoch 101| loss: 0.08064 | train_auc: 0.99609 | valid_auc: 0.99578 |  0:14:23s\n",
            "epoch 102| loss: 0.08056 | train_auc: 0.99626 | valid_auc: 0.9959  |  0:14:31s\n",
            "epoch 103| loss: 0.08039 | train_auc: 0.99625 | valid_auc: 0.99588 |  0:14:39s\n",
            "epoch 104| loss: 0.0804  | train_auc: 0.9962  | valid_auc: 0.99577 |  0:14:48s\n",
            "epoch 105| loss: 0.0804  | train_auc: 0.9962  | valid_auc: 0.9958  |  0:14:57s\n",
            "epoch 106| loss: 0.08066 | train_auc: 0.99631 | valid_auc: 0.99593 |  0:15:05s\n",
            "epoch 107| loss: 0.07975 | train_auc: 0.99643 | valid_auc: 0.99605 |  0:15:14s\n",
            "epoch 108| loss: 0.08059 | train_auc: 0.99639 | valid_auc: 0.99602 |  0:15:22s\n",
            "epoch 109| loss: 0.07993 | train_auc: 0.99634 | valid_auc: 0.9959  |  0:15:31s\n",
            "epoch 110| loss: 0.07937 | train_auc: 0.99637 | valid_auc: 0.99598 |  0:15:40s\n",
            "epoch 111| loss: 0.07895 | train_auc: 0.99631 | valid_auc: 0.99592 |  0:15:48s\n",
            "epoch 112| loss: 0.07916 | train_auc: 0.99637 | valid_auc: 0.99604 |  0:15:57s\n",
            "epoch 113| loss: 0.07887 | train_auc: 0.99637 | valid_auc: 0.99596 |  0:16:05s\n",
            "epoch 114| loss: 0.07931 | train_auc: 0.99628 | valid_auc: 0.99595 |  0:16:14s\n",
            "epoch 115| loss: 0.07896 | train_auc: 0.9964  | valid_auc: 0.996   |  0:16:22s\n",
            "epoch 116| loss: 0.07873 | train_auc: 0.99631 | valid_auc: 0.99589 |  0:16:31s\n",
            "epoch 117| loss: 0.07862 | train_auc: 0.99639 | valid_auc: 0.99598 |  0:16:39s\n",
            "epoch 118| loss: 0.07904 | train_auc: 0.99654 | valid_auc: 0.99615 |  0:16:48s\n",
            "epoch 119| loss: 0.07821 | train_auc: 0.99652 | valid_auc: 0.99607 |  0:16:57s\n",
            "epoch 120| loss: 0.07816 | train_auc: 0.99609 | valid_auc: 0.99571 |  0:17:05s\n",
            "epoch 121| loss: 0.07859 | train_auc: 0.9964  | valid_auc: 0.99604 |  0:17:14s\n",
            "epoch 122| loss: 0.07896 | train_auc: 0.99643 | valid_auc: 0.99606 |  0:17:22s\n",
            "epoch 123| loss: 0.07812 | train_auc: 0.9965  | valid_auc: 0.99616 |  0:17:31s\n",
            "epoch 124| loss: 0.07837 | train_auc: 0.99647 | valid_auc: 0.99612 |  0:17:39s\n",
            "epoch 125| loss: 0.07829 | train_auc: 0.99647 | valid_auc: 0.9961  |  0:17:48s\n",
            "epoch 126| loss: 0.07816 | train_auc: 0.99654 | valid_auc: 0.99615 |  0:17:56s\n",
            "epoch 127| loss: 0.07782 | train_auc: 0.99644 | valid_auc: 0.99605 |  0:18:05s\n",
            "epoch 128| loss: 0.07814 | train_auc: 0.99641 | valid_auc: 0.99598 |  0:18:13s\n",
            "epoch 129| loss: 0.07797 | train_auc: 0.9965  | valid_auc: 0.99612 |  0:18:21s\n",
            "epoch 130| loss: 0.07761 | train_auc: 0.99657 | valid_auc: 0.99622 |  0:18:30s\n",
            "epoch 131| loss: 0.07694 | train_auc: 0.99656 | valid_auc: 0.99614 |  0:18:39s\n",
            "epoch 132| loss: 0.07719 | train_auc: 0.99663 | valid_auc: 0.99625 |  0:18:47s\n",
            "epoch 133| loss: 0.07679 | train_auc: 0.99652 | valid_auc: 0.99613 |  0:18:56s\n",
            "epoch 134| loss: 0.07703 | train_auc: 0.99639 | valid_auc: 0.996   |  0:19:04s\n",
            "epoch 135| loss: 0.07766 | train_auc: 0.99635 | valid_auc: 0.99599 |  0:19:13s\n",
            "epoch 136| loss: 0.0771  | train_auc: 0.9964  | valid_auc: 0.99599 |  0:19:22s\n",
            "epoch 137| loss: 0.0775  | train_auc: 0.99626 | valid_auc: 0.99583 |  0:19:30s\n",
            "epoch 138| loss: 0.07735 | train_auc: 0.99641 | valid_auc: 0.996   |  0:19:39s\n",
            "epoch 139| loss: 0.07698 | train_auc: 0.99661 | valid_auc: 0.99619 |  0:19:47s\n",
            "epoch 140| loss: 0.07652 | train_auc: 0.99651 | valid_auc: 0.99605 |  0:19:56s\n",
            "epoch 141| loss: 0.07648 | train_auc: 0.99665 | valid_auc: 0.99621 |  0:20:04s\n",
            "epoch 142| loss: 0.07631 | train_auc: 0.99662 | valid_auc: 0.99625 |  0:20:13s\n",
            "epoch 143| loss: 0.0766  | train_auc: 0.99661 | valid_auc: 0.99619 |  0:20:21s\n",
            "epoch 144| loss: 0.07637 | train_auc: 0.99666 | valid_auc: 0.99621 |  0:20:30s\n",
            "epoch 145| loss: 0.07581 | train_auc: 0.99679 | valid_auc: 0.99641 |  0:20:39s\n",
            "epoch 146| loss: 0.07585 | train_auc: 0.99661 | valid_auc: 0.99618 |  0:20:47s\n",
            "epoch 147| loss: 0.07577 | train_auc: 0.99665 | valid_auc: 0.99622 |  0:20:56s\n",
            "epoch 148| loss: 0.07613 | train_auc: 0.99671 | valid_auc: 0.99628 |  0:21:04s\n",
            "epoch 149| loss: 0.07571 | train_auc: 0.99671 | valid_auc: 0.99629 |  0:21:13s\n",
            "epoch 150| loss: 0.07529 | train_auc: 0.99672 | valid_auc: 0.9963  |  0:21:22s\n",
            "epoch 151| loss: 0.07581 | train_auc: 0.99681 | valid_auc: 0.99635 |  0:21:30s\n",
            "epoch 152| loss: 0.07552 | train_auc: 0.99673 | valid_auc: 0.9963  |  0:21:38s\n",
            "epoch 153| loss: 0.07572 | train_auc: 0.99637 | valid_auc: 0.99599 |  0:21:47s\n",
            "epoch 154| loss: 0.07583 | train_auc: 0.99648 | valid_auc: 0.99609 |  0:21:55s\n",
            "epoch 155| loss: 0.0756  | train_auc: 0.99676 | valid_auc: 0.99635 |  0:22:04s\n",
            "epoch 156| loss: 0.07543 | train_auc: 0.99663 | valid_auc: 0.99616 |  0:22:12s\n",
            "epoch 157| loss: 0.07516 | train_auc: 0.9968  | valid_auc: 0.99636 |  0:22:21s\n",
            "epoch 158| loss: 0.07446 | train_auc: 0.99675 | valid_auc: 0.99633 |  0:22:29s\n",
            "epoch 159| loss: 0.07488 | train_auc: 0.9968  | valid_auc: 0.9964  |  0:22:38s\n",
            "epoch 160| loss: 0.07494 | train_auc: 0.99672 | valid_auc: 0.9963  |  0:22:46s\n",
            "epoch 161| loss: 0.07482 | train_auc: 0.99683 | valid_auc: 0.9964  |  0:22:55s\n",
            "epoch 162| loss: 0.07427 | train_auc: 0.99679 | valid_auc: 0.99633 |  0:23:04s\n",
            "epoch 163| loss: 0.07501 | train_auc: 0.9967  | valid_auc: 0.99627 |  0:23:12s\n",
            "epoch 164| loss: 0.07442 | train_auc: 0.99687 | valid_auc: 0.99644 |  0:23:21s\n",
            "epoch 165| loss: 0.0749  | train_auc: 0.99681 | valid_auc: 0.99637 |  0:23:29s\n",
            "epoch 166| loss: 0.07435 | train_auc: 0.99677 | valid_auc: 0.99637 |  0:23:37s\n",
            "epoch 167| loss: 0.0749  | train_auc: 0.99676 | valid_auc: 0.99637 |  0:23:46s\n",
            "epoch 168| loss: 0.07464 | train_auc: 0.99683 | valid_auc: 0.99643 |  0:23:54s\n",
            "epoch 169| loss: 0.07482 | train_auc: 0.99681 | valid_auc: 0.9964  |  0:24:03s\n",
            "epoch 170| loss: 0.07403 | train_auc: 0.9969  | valid_auc: 0.99651 |  0:24:11s\n",
            "epoch 171| loss: 0.07403 | train_auc: 0.99688 | valid_auc: 0.99642 |  0:24:20s\n",
            "epoch 172| loss: 0.07431 | train_auc: 0.99654 | valid_auc: 0.99607 |  0:24:28s\n",
            "epoch 173| loss: 0.07442 | train_auc: 0.99689 | valid_auc: 0.9965  |  0:24:37s\n",
            "epoch 174| loss: 0.07364 | train_auc: 0.99695 | valid_auc: 0.99652 |  0:24:45s\n",
            "epoch 175| loss: 0.07405 | train_auc: 0.99692 | valid_auc: 0.99646 |  0:24:54s\n",
            "epoch 176| loss: 0.07405 | train_auc: 0.9969  | valid_auc: 0.99648 |  0:25:03s\n",
            "epoch 177| loss: 0.07353 | train_auc: 0.99684 | valid_auc: 0.99641 |  0:25:11s\n",
            "epoch 178| loss: 0.07385 | train_auc: 0.99682 | valid_auc: 0.99638 |  0:25:20s\n",
            "epoch 179| loss: 0.07364 | train_auc: 0.99671 | valid_auc: 0.99628 |  0:25:28s\n",
            "epoch 180| loss: 0.07444 | train_auc: 0.99647 | valid_auc: 0.99603 |  0:25:37s\n",
            "epoch 181| loss: 0.07396 | train_auc: 0.99659 | valid_auc: 0.99617 |  0:25:45s\n",
            "epoch 182| loss: 0.07303 | train_auc: 0.99687 | valid_auc: 0.9964  |  0:25:54s\n",
            "epoch 183| loss: 0.07345 | train_auc: 0.99679 | valid_auc: 0.99637 |  0:26:02s\n",
            "epoch 184| loss: 0.07384 | train_auc: 0.99698 | valid_auc: 0.99655 |  0:26:10s\n",
            "epoch 185| loss: 0.07338 | train_auc: 0.99696 | valid_auc: 0.99654 |  0:26:19s\n",
            "epoch 186| loss: 0.07319 | train_auc: 0.99685 | valid_auc: 0.99646 |  0:26:28s\n",
            "epoch 187| loss: 0.0732  | train_auc: 0.99687 | valid_auc: 0.99638 |  0:26:37s\n",
            "epoch 188| loss: 0.0728  | train_auc: 0.99694 | valid_auc: 0.99646 |  0:26:45s\n",
            "epoch 189| loss: 0.07242 | train_auc: 0.99698 | valid_auc: 0.99654 |  0:26:54s\n",
            "epoch 190| loss: 0.07261 | train_auc: 0.99695 | valid_auc: 0.99644 |  0:27:03s\n",
            "epoch 191| loss: 0.0727  | train_auc: 0.99692 | valid_auc: 0.99646 |  0:27:11s\n",
            "epoch 192| loss: 0.07279 | train_auc: 0.99697 | valid_auc: 0.99652 |  0:27:20s\n",
            "epoch 193| loss: 0.07268 | train_auc: 0.99699 | valid_auc: 0.99655 |  0:27:29s\n",
            "epoch 194| loss: 0.07242 | train_auc: 0.99701 | valid_auc: 0.99655 |  0:27:37s\n",
            "epoch 195| loss: 0.07268 | train_auc: 0.99693 | valid_auc: 0.99642 |  0:27:46s\n",
            "epoch 196| loss: 0.07257 | train_auc: 0.99697 | valid_auc: 0.99652 |  0:27:54s\n",
            "epoch 197| loss: 0.07217 | train_auc: 0.99702 | valid_auc: 0.99654 |  0:28:03s\n",
            "epoch 198| loss: 0.07261 | train_auc: 0.99705 | valid_auc: 0.99661 |  0:28:12s\n",
            "epoch 199| loss: 0.07202 | train_auc: 0.99704 | valid_auc: 0.99654 |  0:28:20s\n",
            "epoch 200| loss: 0.07213 | train_auc: 0.99684 | valid_auc: 0.99637 |  0:28:29s\n",
            "epoch 201| loss: 0.07245 | train_auc: 0.99697 | valid_auc: 0.99651 |  0:28:38s\n",
            "epoch 202| loss: 0.07196 | train_auc: 0.99704 | valid_auc: 0.99656 |  0:28:46s\n",
            "epoch 203| loss: 0.07215 | train_auc: 0.99703 | valid_auc: 0.99657 |  0:28:55s\n",
            "epoch 204| loss: 0.0716  | train_auc: 0.99709 | valid_auc: 0.9966  |  0:29:04s\n",
            "epoch 205| loss: 0.07187 | train_auc: 0.99704 | valid_auc: 0.99657 |  0:29:12s\n",
            "epoch 206| loss: 0.07195 | train_auc: 0.99708 | valid_auc: 0.99663 |  0:29:21s\n",
            "epoch 207| loss: 0.07152 | train_auc: 0.99705 | valid_auc: 0.99659 |  0:29:30s\n",
            "epoch 208| loss: 0.07159 | train_auc: 0.99707 | valid_auc: 0.99661 |  0:29:38s\n",
            "epoch 209| loss: 0.07153 | train_auc: 0.99709 | valid_auc: 0.99662 |  0:29:47s\n",
            "epoch 210| loss: 0.07125 | train_auc: 0.9971  | valid_auc: 0.99663 |  0:29:56s\n",
            "epoch 211| loss: 0.07152 | train_auc: 0.99708 | valid_auc: 0.99663 |  0:30:04s\n",
            "epoch 212| loss: 0.07149 | train_auc: 0.99712 | valid_auc: 0.99669 |  0:30:13s\n",
            "epoch 213| loss: 0.07174 | train_auc: 0.99704 | valid_auc: 0.9966  |  0:30:22s\n",
            "epoch 214| loss: 0.0718  | train_auc: 0.99705 | valid_auc: 0.99661 |  0:30:31s\n",
            "epoch 215| loss: 0.07178 | train_auc: 0.99706 | valid_auc: 0.99659 |  0:30:39s\n",
            "epoch 216| loss: 0.07158 | train_auc: 0.99708 | valid_auc: 0.99661 |  0:30:48s\n",
            "epoch 217| loss: 0.07132 | train_auc: 0.99701 | valid_auc: 0.99654 |  0:30:56s\n",
            "epoch 218| loss: 0.07133 | train_auc: 0.99714 | valid_auc: 0.99668 |  0:31:04s\n",
            "epoch 219| loss: 0.07084 | train_auc: 0.99715 | valid_auc: 0.99669 |  0:31:13s\n",
            "epoch 220| loss: 0.07068 | train_auc: 0.99718 | valid_auc: 0.99672 |  0:31:22s\n",
            "epoch 221| loss: 0.07115 | train_auc: 0.99713 | valid_auc: 0.99668 |  0:31:30s\n",
            "epoch 222| loss: 0.07115 | train_auc: 0.99714 | valid_auc: 0.99671 |  0:31:39s\n",
            "epoch 223| loss: 0.07064 | train_auc: 0.99709 | valid_auc: 0.99661 |  0:31:47s\n",
            "epoch 224| loss: 0.07091 | train_auc: 0.99711 | valid_auc: 0.99666 |  0:31:56s\n",
            "epoch 225| loss: 0.07066 | train_auc: 0.99714 | valid_auc: 0.9967  |  0:32:05s\n",
            "epoch 226| loss: 0.07094 | train_auc: 0.99711 | valid_auc: 0.99664 |  0:32:14s\n",
            "epoch 227| loss: 0.07073 | train_auc: 0.99709 | valid_auc: 0.99662 |  0:32:22s\n",
            "epoch 228| loss: 0.07078 | train_auc: 0.99709 | valid_auc: 0.99659 |  0:32:31s\n",
            "epoch 229| loss: 0.07095 | train_auc: 0.9972  | valid_auc: 0.99676 |  0:32:39s\n",
            "epoch 230| loss: 0.07098 | train_auc: 0.99715 | valid_auc: 0.9967  |  0:32:48s\n",
            "epoch 231| loss: 0.07074 | train_auc: 0.9972  | valid_auc: 0.99674 |  0:32:56s\n",
            "epoch 232| loss: 0.07048 | train_auc: 0.99716 | valid_auc: 0.9967  |  0:33:05s\n",
            "epoch 233| loss: 0.07039 | train_auc: 0.99718 | valid_auc: 0.99672 |  0:33:14s\n",
            "epoch 234| loss: 0.07054 | train_auc: 0.99713 | valid_auc: 0.99666 |  0:33:23s\n",
            "epoch 235| loss: 0.07087 | train_auc: 0.99717 | valid_auc: 0.99672 |  0:33:31s\n",
            "epoch 236| loss: 0.07202 | train_auc: 0.99716 | valid_auc: 0.99672 |  0:33:40s\n",
            "epoch 237| loss: 0.07095 | train_auc: 0.99718 | valid_auc: 0.99673 |  0:33:49s\n",
            "epoch 238| loss: 0.07043 | train_auc: 0.99712 | valid_auc: 0.99668 |  0:33:58s\n",
            "epoch 239| loss: 0.07059 | train_auc: 0.99716 | valid_auc: 0.99674 |  0:34:06s\n",
            "epoch 240| loss: 0.07086 | train_auc: 0.99722 | valid_auc: 0.99676 |  0:34:15s\n",
            "epoch 241| loss: 0.07046 | train_auc: 0.99716 | valid_auc: 0.99672 |  0:34:24s\n",
            "epoch 242| loss: 0.0707  | train_auc: 0.99717 | valid_auc: 0.99669 |  0:34:32s\n",
            "epoch 243| loss: 0.07018 | train_auc: 0.99717 | valid_auc: 0.99672 |  0:34:41s\n",
            "epoch 244| loss: 0.07041 | train_auc: 0.99721 | valid_auc: 0.99678 |  0:34:50s\n",
            "epoch 245| loss: 0.07006 | train_auc: 0.99718 | valid_auc: 0.99673 |  0:34:58s\n",
            "epoch 246| loss: 0.06986 | train_auc: 0.99719 | valid_auc: 0.99676 |  0:35:06s\n",
            "epoch 247| loss: 0.0701  | train_auc: 0.99723 | valid_auc: 0.99677 |  0:35:15s\n",
            "epoch 248| loss: 0.06993 | train_auc: 0.99718 | valid_auc: 0.99669 |  0:35:24s\n",
            "epoch 249| loss: 0.06977 | train_auc: 0.99719 | valid_auc: 0.99674 |  0:35:32s\n",
            "epoch 250| loss: 0.07006 | train_auc: 0.99723 | valid_auc: 0.9968  |  0:35:41s\n",
            "epoch 251| loss: 0.07028 | train_auc: 0.99724 | valid_auc: 0.99683 |  0:35:50s\n",
            "epoch 252| loss: 0.06967 | train_auc: 0.99721 | valid_auc: 0.99679 |  0:35:58s\n",
            "epoch 253| loss: 0.06965 | train_auc: 0.99723 | valid_auc: 0.99678 |  0:36:07s\n",
            "epoch 254| loss: 0.06979 | train_auc: 0.99725 | valid_auc: 0.99679 |  0:36:16s\n",
            "epoch 255| loss: 0.06952 | train_auc: 0.99719 | valid_auc: 0.99669 |  0:36:25s\n",
            "epoch 256| loss: 0.06981 | train_auc: 0.9972  | valid_auc: 0.99674 |  0:36:33s\n",
            "epoch 257| loss: 0.06953 | train_auc: 0.99725 | valid_auc: 0.99677 |  0:36:42s\n",
            "epoch 258| loss: 0.07007 | train_auc: 0.99724 | valid_auc: 0.99678 |  0:36:50s\n",
            "epoch 259| loss: 0.06975 | train_auc: 0.99723 | valid_auc: 0.99679 |  0:36:59s\n",
            "epoch 260| loss: 0.06956 | train_auc: 0.99726 | valid_auc: 0.99677 |  0:37:08s\n",
            "epoch 261| loss: 0.06941 | train_auc: 0.99727 | valid_auc: 0.9968  |  0:37:16s\n",
            "epoch 262| loss: 0.06922 | train_auc: 0.99727 | valid_auc: 0.9968  |  0:37:25s\n",
            "epoch 263| loss: 0.0693  | train_auc: 0.99726 | valid_auc: 0.99681 |  0:37:34s\n",
            "epoch 264| loss: 0.06996 | train_auc: 0.9972  | valid_auc: 0.99677 |  0:37:42s\n",
            "epoch 265| loss: 0.06944 | train_auc: 0.99725 | valid_auc: 0.99682 |  0:37:51s\n",
            "epoch 266| loss: 0.06971 | train_auc: 0.99727 | valid_auc: 0.99682 |  0:38:00s\n",
            "epoch 267| loss: 0.06945 | train_auc: 0.99728 | valid_auc: 0.99684 |  0:38:09s\n",
            "epoch 268| loss: 0.06957 | train_auc: 0.9972  | valid_auc: 0.99675 |  0:38:18s\n",
            "epoch 269| loss: 0.06937 | train_auc: 0.99724 | valid_auc: 0.99679 |  0:38:26s\n",
            "epoch 270| loss: 0.06941 | train_auc: 0.99729 | valid_auc: 0.99686 |  0:38:35s\n",
            "epoch 271| loss: 0.06994 | train_auc: 0.99725 | valid_auc: 0.99684 |  0:38:44s\n",
            "epoch 272| loss: 0.06976 | train_auc: 0.99724 | valid_auc: 0.99681 |  0:38:52s\n",
            "epoch 273| loss: 0.06926 | train_auc: 0.99728 | valid_auc: 0.99683 |  0:39:01s\n",
            "epoch 274| loss: 0.06927 | train_auc: 0.99731 | valid_auc: 0.99684 |  0:39:09s\n",
            "epoch 275| loss: 0.0688  | train_auc: 0.99731 | valid_auc: 0.99686 |  0:39:18s\n",
            "epoch 276| loss: 0.0687  | train_auc: 0.9973  | valid_auc: 0.99683 |  0:39:27s\n",
            "epoch 277| loss: 0.06898 | train_auc: 0.99729 | valid_auc: 0.99682 |  0:39:36s\n",
            "epoch 278| loss: 0.06882 | train_auc: 0.99729 | valid_auc: 0.99684 |  0:39:45s\n",
            "epoch 279| loss: 0.06909 | train_auc: 0.99729 | valid_auc: 0.99683 |  0:39:53s\n",
            "epoch 280| loss: 0.06918 | train_auc: 0.99731 | valid_auc: 0.99686 |  0:40:02s\n",
            "epoch 281| loss: 0.06877 | train_auc: 0.9973  | valid_auc: 0.99688 |  0:40:11s\n",
            "epoch 282| loss: 0.069   | train_auc: 0.99733 | valid_auc: 0.99689 |  0:40:20s\n",
            "epoch 283| loss: 0.069   | train_auc: 0.99732 | valid_auc: 0.99686 |  0:40:28s\n",
            "epoch 284| loss: 0.06867 | train_auc: 0.99734 | valid_auc: 0.99687 |  0:40:37s\n",
            "epoch 285| loss: 0.06889 | train_auc: 0.99729 | valid_auc: 0.99683 |  0:40:45s\n",
            "epoch 286| loss: 0.069   | train_auc: 0.99731 | valid_auc: 0.99685 |  0:40:54s\n",
            "epoch 287| loss: 0.06878 | train_auc: 0.99725 | valid_auc: 0.99682 |  0:41:02s\n",
            "epoch 288| loss: 0.06912 | train_auc: 0.9973  | valid_auc: 0.99684 |  0:41:11s\n",
            "epoch 289| loss: 0.06865 | train_auc: 0.99733 | valid_auc: 0.99686 |  0:41:20s\n",
            "epoch 290| loss: 0.06881 | train_auc: 0.99729 | valid_auc: 0.99681 |  0:41:29s\n",
            "epoch 291| loss: 0.06879 | train_auc: 0.9973  | valid_auc: 0.99685 |  0:41:37s\n",
            "epoch 292| loss: 0.06876 | train_auc: 0.99732 | valid_auc: 0.99685 |  0:41:46s\n",
            "epoch 293| loss: 0.06873 | train_auc: 0.99732 | valid_auc: 0.99687 |  0:41:55s\n",
            "epoch 294| loss: 0.06845 | train_auc: 0.99732 | valid_auc: 0.99688 |  0:42:04s\n",
            "epoch 295| loss: 0.06868 | train_auc: 0.99735 | valid_auc: 0.9969  |  0:42:12s\n",
            "epoch 296| loss: 0.06861 | train_auc: 0.58371 | valid_auc: 0.58131 |  0:42:21s\n",
            "epoch 297| loss: 0.06844 | train_auc: 0.99733 | valid_auc: 0.99688 |  0:42:30s\n",
            "epoch 298| loss: 0.06861 | train_auc: 0.99733 | valid_auc: 0.99686 |  0:42:38s\n",
            "epoch 299| loss: 0.06862 | train_auc: 0.99734 | valid_auc: 0.99688 |  0:42:47s\n",
            "epoch 300| loss: 0.06869 | train_auc: 0.99734 | valid_auc: 0.99689 |  0:42:56s\n",
            "epoch 301| loss: 0.06858 | train_auc: 0.99735 | valid_auc: 0.99691 |  0:43:05s\n",
            "epoch 302| loss: 0.06851 | train_auc: 0.99736 | valid_auc: 0.99692 |  0:43:13s\n",
            "epoch 303| loss: 0.06843 | train_auc: 0.99735 | valid_auc: 0.99689 |  0:43:22s\n",
            "epoch 304| loss: 0.06851 | train_auc: 0.99735 | valid_auc: 0.9969  |  0:43:31s\n",
            "epoch 305| loss: 0.06837 | train_auc: 0.99736 | valid_auc: 0.99687 |  0:43:39s\n",
            "epoch 306| loss: 0.06829 | train_auc: 0.99735 | valid_auc: 0.99688 |  0:43:48s\n",
            "epoch 307| loss: 0.06848 | train_auc: 0.99728 | valid_auc: 0.9968  |  0:43:57s\n",
            "epoch 308| loss: 0.06855 | train_auc: 0.99735 | valid_auc: 0.9969  |  0:44:05s\n",
            "epoch 309| loss: 0.06856 | train_auc: 0.99735 | valid_auc: 0.99686 |  0:44:14s\n",
            "epoch 310| loss: 0.06856 | train_auc: 0.99734 | valid_auc: 0.99686 |  0:44:23s\n",
            "epoch 311| loss: 0.06845 | train_auc: 0.99733 | valid_auc: 0.99688 |  0:44:31s\n",
            "epoch 312| loss: 0.0682  | train_auc: 0.99735 | valid_auc: 0.99688 |  0:44:40s\n",
            "epoch 313| loss: 0.06866 | train_auc: 0.99733 | valid_auc: 0.99686 |  0:44:49s\n",
            "epoch 314| loss: 0.06821 | train_auc: 0.99735 | valid_auc: 0.99689 |  0:44:58s\n",
            "epoch 315| loss: 0.06857 | train_auc: 0.99734 | valid_auc: 0.99685 |  0:45:06s\n",
            "epoch 316| loss: 0.06844 | train_auc: 0.99735 | valid_auc: 0.99687 |  0:45:15s\n",
            "epoch 317| loss: 0.068   | train_auc: 0.99738 | valid_auc: 0.9969  |  0:45:24s\n",
            "epoch 318| loss: 0.06816 | train_auc: 0.99736 | valid_auc: 0.99688 |  0:45:32s\n",
            "epoch 319| loss: 0.06826 | train_auc: 0.99736 | valid_auc: 0.99687 |  0:45:41s\n",
            "epoch 320| loss: 0.06792 | train_auc: 0.99737 | valid_auc: 0.99689 |  0:45:50s\n",
            "epoch 321| loss: 0.06788 | train_auc: 0.99737 | valid_auc: 0.99692 |  0:45:58s\n",
            "epoch 322| loss: 0.06783 | train_auc: 0.99738 | valid_auc: 0.9969  |  0:46:07s\n",
            "epoch 323| loss: 0.06794 | train_auc: 0.99737 | valid_auc: 0.9969  |  0:46:15s\n",
            "epoch 324| loss: 0.06808 | train_auc: 0.99737 | valid_auc: 0.9969  |  0:46:24s\n",
            "epoch 325| loss: 0.06801 | train_auc: 0.99738 | valid_auc: 0.99691 |  0:46:33s\n",
            "epoch 326| loss: 0.06808 | train_auc: 0.99737 | valid_auc: 0.9969  |  0:46:41s\n",
            "epoch 327| loss: 0.06814 | train_auc: 0.99738 | valid_auc: 0.99691 |  0:46:50s\n",
            "epoch 328| loss: 0.06796 | train_auc: 0.99736 | valid_auc: 0.9969  |  0:46:59s\n",
            "epoch 329| loss: 0.06802 | train_auc: 0.99739 | valid_auc: 0.99693 |  0:47:07s\n",
            "epoch 330| loss: 0.06781 | train_auc: 0.99737 | valid_auc: 0.99691 |  0:47:16s\n",
            "epoch 331| loss: 0.06799 | train_auc: 0.99737 | valid_auc: 0.99689 |  0:47:25s\n",
            "epoch 332| loss: 0.06777 | train_auc: 0.99739 | valid_auc: 0.99694 |  0:47:33s\n",
            "epoch 333| loss: 0.0677  | train_auc: 0.99738 | valid_auc: 0.99691 |  0:47:42s\n",
            "epoch 334| loss: 0.0678  | train_auc: 0.99739 | valid_auc: 0.99691 |  0:47:51s\n",
            "epoch 335| loss: 0.06802 | train_auc: 0.99737 | valid_auc: 0.99688 |  0:48:00s\n",
            "epoch 336| loss: 0.068   | train_auc: 0.99742 | valid_auc: 0.99696 |  0:48:09s\n",
            "epoch 337| loss: 0.06751 | train_auc: 0.9974  | valid_auc: 0.99694 |  0:48:18s\n",
            "epoch 338| loss: 0.06752 | train_auc: 0.99741 | valid_auc: 0.99693 |  0:48:26s\n",
            "epoch 339| loss: 0.06785 | train_auc: 0.9974  | valid_auc: 0.99695 |  0:48:35s\n",
            "epoch 340| loss: 0.06807 | train_auc: 0.99735 | valid_auc: 0.99691 |  0:48:43s\n",
            "epoch 341| loss: 0.06779 | train_auc: 0.9974  | valid_auc: 0.99695 |  0:48:52s\n",
            "epoch 342| loss: 0.06751 | train_auc: 0.9974  | valid_auc: 0.99692 |  0:49:01s\n",
            "epoch 343| loss: 0.06783 | train_auc: 0.99737 | valid_auc: 0.99693 |  0:49:10s\n",
            "epoch 344| loss: 0.06798 | train_auc: 0.99742 | valid_auc: 0.99696 |  0:49:18s\n",
            "epoch 345| loss: 0.06794 | train_auc: 0.9974  | valid_auc: 0.99694 |  0:49:27s\n",
            "epoch 346| loss: 0.06746 | train_auc: 0.99741 | valid_auc: 0.99695 |  0:49:36s\n",
            "epoch 347| loss: 0.06755 | train_auc: 0.99742 | valid_auc: 0.99694 |  0:49:45s\n",
            "epoch 348| loss: 0.06724 | train_auc: 0.99742 | valid_auc: 0.99695 |  0:49:53s\n",
            "epoch 349| loss: 0.06735 | train_auc: 0.99739 | valid_auc: 0.99692 |  0:50:02s\n",
            "epoch 350| loss: 0.06744 | train_auc: 0.99741 | valid_auc: 0.99693 |  0:50:11s\n",
            "epoch 351| loss: 0.06746 | train_auc: 0.99743 | valid_auc: 0.99694 |  0:50:19s\n",
            "epoch 352| loss: 0.06746 | train_auc: 0.9974  | valid_auc: 0.99692 |  0:50:28s\n",
            "epoch 353| loss: 0.06747 | train_auc: 0.99743 | valid_auc: 0.99696 |  0:50:37s\n",
            "epoch 354| loss: 0.06739 | train_auc: 0.99742 | valid_auc: 0.99694 |  0:50:46s\n",
            "epoch 355| loss: 0.06752 | train_auc: 0.99741 | valid_auc: 0.99693 |  0:50:54s\n",
            "epoch 356| loss: 0.06753 | train_auc: 0.99741 | valid_auc: 0.99694 |  0:51:03s\n",
            "epoch 357| loss: 0.06734 | train_auc: 0.99743 | valid_auc: 0.99695 |  0:51:11s\n",
            "epoch 358| loss: 0.06725 | train_auc: 0.99742 | valid_auc: 0.99695 |  0:51:20s\n",
            "epoch 359| loss: 0.06729 | train_auc: 0.99743 | valid_auc: 0.99696 |  0:51:29s\n",
            "epoch 360| loss: 0.06729 | train_auc: 0.99741 | valid_auc: 0.99696 |  0:51:37s\n",
            "epoch 361| loss: 0.06739 | train_auc: 0.99741 | valid_auc: 0.99692 |  0:51:46s\n",
            "epoch 362| loss: 0.06728 | train_auc: 0.99743 | valid_auc: 0.99696 |  0:51:55s\n",
            "epoch 363| loss: 0.06746 | train_auc: 0.99743 | valid_auc: 0.99696 |  0:52:04s\n",
            "epoch 364| loss: 0.06736 | train_auc: 0.99744 | valid_auc: 0.99698 |  0:52:13s\n",
            "epoch 365| loss: 0.06737 | train_auc: 0.99742 | valid_auc: 0.99695 |  0:52:21s\n",
            "epoch 366| loss: 0.06748 | train_auc: 0.99744 | valid_auc: 0.99699 |  0:52:30s\n",
            "epoch 367| loss: 0.06721 | train_auc: 0.99744 | valid_auc: 0.99696 |  0:52:39s\n",
            "epoch 368| loss: 0.06744 | train_auc: 0.99742 | valid_auc: 0.99696 |  0:52:47s\n",
            "epoch 369| loss: 0.06731 | train_auc: 0.9974  | valid_auc: 0.99692 |  0:52:56s\n",
            "epoch 370| loss: 0.06763 | train_auc: 0.99741 | valid_auc: 0.99695 |  0:53:04s\n",
            "epoch 371| loss: 0.06731 | train_auc: 0.99743 | valid_auc: 0.99698 |  0:53:13s\n",
            "epoch 372| loss: 0.06731 | train_auc: 0.99744 | valid_auc: 0.99697 |  0:53:22s\n",
            "epoch 373| loss: 0.06759 | train_auc: 0.99742 | valid_auc: 0.99695 |  0:53:31s\n",
            "epoch 374| loss: 0.06714 | train_auc: 0.99743 | valid_auc: 0.99696 |  0:53:40s\n",
            "epoch 375| loss: 0.06744 | train_auc: 0.99744 | valid_auc: 0.99696 |  0:53:48s\n",
            "epoch 376| loss: 0.06711 | train_auc: 0.99744 | valid_auc: 0.99696 |  0:53:57s\n",
            "epoch 377| loss: 0.06751 | train_auc: 0.99742 | valid_auc: 0.99695 |  0:54:05s\n",
            "epoch 378| loss: 0.06725 | train_auc: 0.99744 | valid_auc: 0.99696 |  0:54:14s\n",
            "epoch 379| loss: 0.06724 | train_auc: 0.99744 | valid_auc: 0.99698 |  0:54:23s\n",
            "epoch 380| loss: 0.0669  | train_auc: 0.99745 | valid_auc: 0.99697 |  0:54:31s\n",
            "epoch 381| loss: 0.06691 | train_auc: 0.99745 | valid_auc: 0.99696 |  0:54:40s\n",
            "epoch 382| loss: 0.06714 | train_auc: 0.99744 | valid_auc: 0.99696 |  0:54:48s\n",
            "epoch 383| loss: 0.06714 | train_auc: 0.99746 | valid_auc: 0.99699 |  0:54:57s\n",
            "epoch 384| loss: 0.06703 | train_auc: 0.99745 | valid_auc: 0.99698 |  0:55:06s\n",
            "epoch 385| loss: 0.06718 | train_auc: 0.99745 | valid_auc: 0.99698 |  0:55:15s\n",
            "epoch 386| loss: 0.06689 | train_auc: 0.99745 | valid_auc: 0.99696 |  0:55:24s\n",
            "epoch 387| loss: 0.06693 | train_auc: 0.99746 | valid_auc: 0.99697 |  0:55:33s\n",
            "epoch 388| loss: 0.06716 | train_auc: 0.99746 | valid_auc: 0.99698 |  0:55:41s\n",
            "epoch 389| loss: 0.06692 | train_auc: 0.99745 | valid_auc: 0.99699 |  0:55:51s\n",
            "epoch 390| loss: 0.06709 | train_auc: 0.99743 | valid_auc: 0.99699 |  0:55:59s\n",
            "epoch 391| loss: 0.06699 | train_auc: 0.99746 | valid_auc: 0.99698 |  0:56:08s\n",
            "epoch 392| loss: 0.06694 | train_auc: 0.99746 | valid_auc: 0.997   |  0:56:16s\n",
            "epoch 393| loss: 0.06707 | train_auc: 0.99746 | valid_auc: 0.997   |  0:56:25s\n",
            "epoch 394| loss: 0.06694 | train_auc: 0.99745 | valid_auc: 0.99698 |  0:56:34s\n",
            "epoch 395| loss: 0.06682 | train_auc: 0.99745 | valid_auc: 0.99697 |  0:56:43s\n",
            "epoch 396| loss: 0.06684 | train_auc: 0.99743 | valid_auc: 0.99694 |  0:56:51s\n",
            "epoch 397| loss: 0.06699 | train_auc: 0.99744 | valid_auc: 0.99697 |  0:57:00s\n",
            "epoch 398| loss: 0.067   | train_auc: 0.99745 | valid_auc: 0.99698 |  0:57:09s\n",
            "epoch 399| loss: 0.06691 | train_auc: 0.99747 | valid_auc: 0.99698 |  0:57:18s\n",
            "epoch 400| loss: 0.06694 | train_auc: 0.99747 | valid_auc: 0.99701 |  0:57:27s\n",
            "epoch 401| loss: 0.06681 | train_auc: 0.99747 | valid_auc: 0.997   |  0:57:36s\n",
            "epoch 402| loss: 0.06688 | train_auc: 0.99746 | valid_auc: 0.99699 |  0:57:45s\n",
            "epoch 403| loss: 0.06678 | train_auc: 0.99746 | valid_auc: 0.99697 |  0:57:53s\n",
            "epoch 404| loss: 0.06698 | train_auc: 0.99746 | valid_auc: 0.99697 |  0:58:02s\n",
            "epoch 405| loss: 0.067   | train_auc: 0.99746 | valid_auc: 0.99698 |  0:58:10s\n",
            "epoch 406| loss: 0.06674 | train_auc: 0.99746 | valid_auc: 0.99699 |  0:58:19s\n",
            "epoch 407| loss: 0.0671  | train_auc: 0.99747 | valid_auc: 0.99699 |  0:58:27s\n",
            "epoch 408| loss: 0.06684 | train_auc: 0.99747 | valid_auc: 0.997   |  0:58:36s\n",
            "epoch 409| loss: 0.06686 | train_auc: 0.99747 | valid_auc: 0.99698 |  0:58:45s\n",
            "epoch 410| loss: 0.06681 | train_auc: 0.99747 | valid_auc: 0.99699 |  0:58:53s\n",
            "epoch 411| loss: 0.06674 | train_auc: 0.99748 | valid_auc: 0.99701 |  0:59:02s\n",
            "epoch 412| loss: 0.06682 | train_auc: 0.99746 | valid_auc: 0.99698 |  0:59:11s\n",
            "epoch 413| loss: 0.06703 | train_auc: 0.99745 | valid_auc: 0.997   |  0:59:20s\n",
            "epoch 414| loss: 0.06698 | train_auc: 0.99744 | valid_auc: 0.99695 |  0:59:28s\n",
            "epoch 415| loss: 0.06696 | train_auc: 0.99747 | valid_auc: 0.99699 |  0:59:37s\n",
            "epoch 416| loss: 0.0668  | train_auc: 0.99747 | valid_auc: 0.997   |  0:59:46s\n",
            "epoch 417| loss: 0.06669 | train_auc: 0.99748 | valid_auc: 0.997   |  0:59:55s\n",
            "epoch 418| loss: 0.06648 | train_auc: 0.99748 | valid_auc: 0.997   |  1:00:03s\n",
            "epoch 419| loss: 0.06676 | train_auc: 0.99748 | valid_auc: 0.997   |  1:00:12s\n",
            "epoch 420| loss: 0.06664 | train_auc: 0.99747 | valid_auc: 0.99699 |  1:00:20s\n",
            "\n",
            "Early stopping occurred at epoch 420 with best_epoch = 400 and best_valid_auc = 0.99701\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9970087619693524\n",
            "----- 3 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 17136409.15303| val_0_unsup_loss: 3978.27344|  0:00:05s\n",
            "epoch 1  | loss: 3639.56104| val_0_unsup_loss: 2.85442 |  0:00:11s\n",
            "epoch 2  | loss: 2.68801 | val_0_unsup_loss: 1.89472 |  0:00:16s\n",
            "epoch 3  | loss: 2.6055  | val_0_unsup_loss: 1.5939  |  0:00:22s\n",
            "epoch 4  | loss: 3.6669  | val_0_unsup_loss: 1.48463 |  0:00:27s\n",
            "epoch 5  | loss: 5.94227 | val_0_unsup_loss: 1.42498 |  0:00:33s\n",
            "epoch 6  | loss: 3.19397 | val_0_unsup_loss: 1.3947  |  0:00:39s\n",
            "epoch 7  | loss: 8.44204 | val_0_unsup_loss: 1.3842  |  0:00:44s\n",
            "epoch 8  | loss: 1.99725 | val_0_unsup_loss: 1.37447 |  0:00:50s\n",
            "epoch 9  | loss: 1.40548 | val_0_unsup_loss: 1.36693 |  0:00:55s\n",
            "epoch 10 | loss: 36.26642| val_0_unsup_loss: 1.36236 |  0:01:01s\n",
            "epoch 11 | loss: 1.40636 | val_0_unsup_loss: 1.35951 |  0:01:07s\n",
            "epoch 12 | loss: 1.37278 | val_0_unsup_loss: 1.35764 |  0:01:12s\n",
            "epoch 13 | loss: 1.35792 | val_0_unsup_loss: 1.35664 |  0:01:18s\n",
            "epoch 14 | loss: 1.35603 | val_0_unsup_loss: 1.35554 |  0:01:24s\n",
            "epoch 15 | loss: 1.35485 | val_0_unsup_loss: 1.35487 |  0:01:29s\n",
            "epoch 16 | loss: 1.35461 | val_0_unsup_loss: 1.35452 |  0:01:35s\n",
            "epoch 17 | loss: 1.35436 | val_0_unsup_loss: 1.35436 |  0:01:41s\n",
            "epoch 18 | loss: 1.35418 | val_0_unsup_loss: 1.3543  |  0:01:46s\n",
            "epoch 19 | loss: 1.35483 | val_0_unsup_loss: 1.3543  |  0:01:52s\n",
            "epoch 20 | loss: 1.35415 | val_0_unsup_loss: 1.35442 |  0:01:58s\n",
            "epoch 21 | loss: 1.35412 | val_0_unsup_loss: 1.35435 |  0:02:04s\n",
            "epoch 22 | loss: 1.3542  | val_0_unsup_loss: 1.35428 |  0:02:09s\n",
            "epoch 23 | loss: 1.35755 | val_0_unsup_loss: 1.35432 |  0:02:15s\n",
            "epoch 24 | loss: 1.35413 | val_0_unsup_loss: 1.35432 |  0:02:20s\n",
            "epoch 25 | loss: 1.35441 | val_0_unsup_loss: 1.35429 |  0:02:26s\n",
            "epoch 26 | loss: 1.35418 | val_0_unsup_loss: 1.35427 |  0:02:31s\n",
            "epoch 27 | loss: 1.35429 | val_0_unsup_loss: 1.35425 |  0:02:37s\n",
            "epoch 28 | loss: 1.35443 | val_0_unsup_loss: 1.35425 |  0:02:42s\n",
            "epoch 29 | loss: 1.3542  | val_0_unsup_loss: 1.35425 |  0:02:48s\n",
            "epoch 30 | loss: 1.35391 | val_0_unsup_loss: 1.35424 |  0:02:54s\n",
            "epoch 31 | loss: 1.35399 | val_0_unsup_loss: 1.35424 |  0:02:59s\n",
            "epoch 32 | loss: 1.3541  | val_0_unsup_loss: 1.35424 |  0:03:05s\n",
            "epoch 33 | loss: 1.35519 | val_0_unsup_loss: 1.35429 |  0:03:10s\n",
            "epoch 34 | loss: 1.867   | val_0_unsup_loss: 1.35433 |  0:03:16s\n",
            "epoch 35 | loss: 1.35429 | val_0_unsup_loss: 1.35427 |  0:03:21s\n",
            "epoch 36 | loss: 1.35418 | val_0_unsup_loss: 1.35426 |  0:03:27s\n",
            "epoch 37 | loss: 1.35449 | val_0_unsup_loss: 1.35426 |  0:03:33s\n",
            "epoch 38 | loss: 1.36347 | val_0_unsup_loss: 1.35429 |  0:03:38s\n",
            "epoch 39 | loss: 1.3545  | val_0_unsup_loss: 1.35447 |  0:03:44s\n",
            "epoch 40 | loss: 1.35433 | val_0_unsup_loss: 1.35441 |  0:03:49s\n",
            "epoch 41 | loss: 1.35441 | val_0_unsup_loss: 1.35432 |  0:03:55s\n",
            "epoch 42 | loss: 1.35408 | val_0_unsup_loss: 1.35427 |  0:04:00s\n",
            "epoch 43 | loss: 1.51904 | val_0_unsup_loss: 1.35427 |  0:04:06s\n",
            "epoch 44 | loss: 1.41605 | val_0_unsup_loss: 1.35432 |  0:04:12s\n",
            "epoch 45 | loss: 1.35458 | val_0_unsup_loss: 1.35432 |  0:04:17s\n",
            "epoch 46 | loss: 1.59961 | val_0_unsup_loss: 1.35473 |  0:04:23s\n",
            "epoch 47 | loss: 1.44399 | val_0_unsup_loss: 1.35508 |  0:04:28s\n",
            "epoch 48 | loss: 1.41042 | val_0_unsup_loss: 1.35456 |  0:04:34s\n",
            "epoch 49 | loss: 1.36589 | val_0_unsup_loss: 1.35448 |  0:04:39s\n",
            "epoch 50 | loss: 1.36313 | val_0_unsup_loss: 1.35474 |  0:04:45s\n",
            "epoch 51 | loss: 3.3744  | val_0_unsup_loss: 1.35464 |  0:04:50s\n",
            "\n",
            "Early stopping occurred at epoch 51 with best_epoch = 31 and best_val_0_unsup_loss = 1.35424\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.68701 | train_auc: 0.62191 | valid_auc: 0.62402 |  0:00:08s\n",
            "epoch 1  | loss: 0.49489 | train_auc: 0.80979 | valid_auc: 0.80862 |  0:00:17s\n",
            "epoch 2  | loss: 0.25863 | train_auc: 0.63816 | valid_auc: 0.63717 |  0:00:26s\n",
            "epoch 3  | loss: 0.19037 | train_auc: 0.77872 | valid_auc: 0.78011 |  0:00:34s\n",
            "epoch 4  | loss: 0.16301 | train_auc: 0.90331 | valid_auc: 0.90336 |  0:00:43s\n",
            "epoch 5  | loss: 0.14986 | train_auc: 0.88846 | valid_auc: 0.88838 |  0:00:52s\n",
            "epoch 6  | loss: 0.14478 | train_auc: 0.93756 | valid_auc: 0.9367  |  0:01:01s\n",
            "epoch 7  | loss: 0.13845 | train_auc: 0.95455 | valid_auc: 0.95413 |  0:01:09s\n",
            "epoch 8  | loss: 0.13489 | train_auc: 0.96717 | valid_auc: 0.96608 |  0:01:18s\n",
            "epoch 9  | loss: 0.12985 | train_auc: 0.98157 | valid_auc: 0.98101 |  0:01:27s\n",
            "epoch 10 | loss: 0.12712 | train_auc: 0.9786  | valid_auc: 0.97787 |  0:01:36s\n",
            "epoch 11 | loss: 0.12403 | train_auc: 0.98494 | valid_auc: 0.98434 |  0:01:45s\n",
            "epoch 12 | loss: 0.12176 | train_auc: 0.98606 | valid_auc: 0.98543 |  0:01:54s\n",
            "epoch 13 | loss: 0.1194  | train_auc: 0.98826 | valid_auc: 0.98751 |  0:02:03s\n",
            "epoch 14 | loss: 0.11687 | train_auc: 0.98883 | valid_auc: 0.98793 |  0:02:12s\n",
            "epoch 15 | loss: 0.11328 | train_auc: 0.98896 | valid_auc: 0.98804 |  0:02:21s\n",
            "epoch 16 | loss: 0.11326 | train_auc: 0.98943 | valid_auc: 0.98883 |  0:02:29s\n",
            "epoch 17 | loss: 0.11005 | train_auc: 0.98917 | valid_auc: 0.98837 |  0:02:38s\n",
            "epoch 18 | loss: 0.1079  | train_auc: 0.98772 | valid_auc: 0.98712 |  0:02:46s\n",
            "epoch 19 | loss: 0.10918 | train_auc: 0.99101 | valid_auc: 0.99044 |  0:02:55s\n",
            "epoch 20 | loss: 0.10513 | train_auc: 0.99171 | valid_auc: 0.99112 |  0:03:04s\n",
            "epoch 21 | loss: 0.10304 | train_auc: 0.99164 | valid_auc: 0.99111 |  0:03:13s\n",
            "epoch 22 | loss: 0.1025  | train_auc: 0.99288 | valid_auc: 0.99218 |  0:03:22s\n",
            "epoch 23 | loss: 0.10104 | train_auc: 0.99296 | valid_auc: 0.99238 |  0:03:30s\n",
            "epoch 24 | loss: 0.09868 | train_auc: 0.99353 | valid_auc: 0.99296 |  0:03:39s\n",
            "epoch 25 | loss: 0.09772 | train_auc: 0.99308 | valid_auc: 0.99246 |  0:03:48s\n",
            "epoch 26 | loss: 0.09756 | train_auc: 0.99396 | valid_auc: 0.99337 |  0:03:57s\n",
            "epoch 27 | loss: 0.09564 | train_auc: 0.99367 | valid_auc: 0.99305 |  0:04:06s\n",
            "epoch 28 | loss: 0.09497 | train_auc: 0.99414 | valid_auc: 0.99346 |  0:04:15s\n",
            "epoch 29 | loss: 0.0937  | train_auc: 0.99432 | valid_auc: 0.99383 |  0:04:23s\n",
            "epoch 30 | loss: 0.09366 | train_auc: 0.99434 | valid_auc: 0.99379 |  0:04:32s\n",
            "epoch 31 | loss: 0.0923  | train_auc: 0.99451 | valid_auc: 0.99394 |  0:04:40s\n",
            "epoch 32 | loss: 0.09229 | train_auc: 0.99468 | valid_auc: 0.99421 |  0:04:49s\n",
            "epoch 33 | loss: 0.09258 | train_auc: 0.99501 | valid_auc: 0.99456 |  0:04:58s\n",
            "epoch 34 | loss: 0.08941 | train_auc: 0.99469 | valid_auc: 0.99423 |  0:05:07s\n",
            "epoch 35 | loss: 0.08917 | train_auc: 0.99489 | valid_auc: 0.99445 |  0:05:15s\n",
            "epoch 36 | loss: 0.08876 | train_auc: 0.99377 | valid_auc: 0.99315 |  0:05:24s\n",
            "epoch 37 | loss: 0.08849 | train_auc: 0.99425 | valid_auc: 0.99363 |  0:05:33s\n",
            "epoch 38 | loss: 0.08762 | train_auc: 0.99514 | valid_auc: 0.99463 |  0:05:42s\n",
            "epoch 39 | loss: 0.08735 | train_auc: 0.99485 | valid_auc: 0.9944  |  0:05:51s\n",
            "epoch 40 | loss: 0.08892 | train_auc: 0.99513 | valid_auc: 0.99459 |  0:06:00s\n",
            "epoch 41 | loss: 0.08832 | train_auc: 0.99537 | valid_auc: 0.99492 |  0:06:08s\n",
            "epoch 42 | loss: 0.08645 | train_auc: 0.99537 | valid_auc: 0.99487 |  0:06:17s\n",
            "epoch 43 | loss: 0.08558 | train_auc: 0.99462 | valid_auc: 0.99416 |  0:06:26s\n",
            "epoch 44 | loss: 0.08568 | train_auc: 0.99526 | valid_auc: 0.99487 |  0:06:34s\n",
            "epoch 45 | loss: 0.0849  | train_auc: 0.99541 | valid_auc: 0.99487 |  0:06:43s\n",
            "epoch 46 | loss: 0.08531 | train_auc: 0.99547 | valid_auc: 0.99497 |  0:06:52s\n",
            "epoch 47 | loss: 0.08446 | train_auc: 0.99541 | valid_auc: 0.99483 |  0:07:01s\n",
            "epoch 48 | loss: 0.08389 | train_auc: 0.99571 | valid_auc: 0.99519 |  0:07:09s\n",
            "epoch 49 | loss: 0.08507 | train_auc: 0.99579 | valid_auc: 0.99526 |  0:07:18s\n",
            "epoch 50 | loss: 0.08339 | train_auc: 0.99575 | valid_auc: 0.99524 |  0:07:27s\n",
            "epoch 51 | loss: 0.08474 | train_auc: 0.99575 | valid_auc: 0.99514 |  0:07:36s\n",
            "epoch 52 | loss: 0.08481 | train_auc: 0.99575 | valid_auc: 0.99525 |  0:07:45s\n",
            "epoch 53 | loss: 0.08496 | train_auc: 0.99577 | valid_auc: 0.99519 |  0:07:54s\n",
            "epoch 54 | loss: 0.08384 | train_auc: 0.99581 | valid_auc: 0.99535 |  0:08:03s\n",
            "epoch 55 | loss: 0.08177 | train_auc: 0.996   | valid_auc: 0.99543 |  0:08:12s\n",
            "epoch 56 | loss: 0.0832  | train_auc: 0.99534 | valid_auc: 0.99474 |  0:08:21s\n",
            "epoch 57 | loss: 0.08313 | train_auc: 0.99571 | valid_auc: 0.99512 |  0:08:29s\n",
            "epoch 58 | loss: 0.08154 | train_auc: 0.99617 | valid_auc: 0.99568 |  0:08:38s\n",
            "epoch 59 | loss: 0.08096 | train_auc: 0.9958  | valid_auc: 0.99529 |  0:08:47s\n",
            "epoch 60 | loss: 0.08189 | train_auc: 0.99546 | valid_auc: 0.99498 |  0:08:55s\n",
            "epoch 61 | loss: 0.08018 | train_auc: 0.99601 | valid_auc: 0.99557 |  0:09:04s\n",
            "epoch 62 | loss: 0.07963 | train_auc: 0.99615 | valid_auc: 0.99563 |  0:09:13s\n",
            "epoch 63 | loss: 0.08021 | train_auc: 0.99625 | valid_auc: 0.99577 |  0:09:22s\n",
            "epoch 64 | loss: 0.08042 | train_auc: 0.99613 | valid_auc: 0.99567 |  0:09:31s\n",
            "epoch 65 | loss: 0.08006 | train_auc: 0.99552 | valid_auc: 0.99496 |  0:09:40s\n",
            "epoch 66 | loss: 0.07855 | train_auc: 0.99593 | valid_auc: 0.99541 |  0:09:49s\n",
            "epoch 67 | loss: 0.07939 | train_auc: 0.99632 | valid_auc: 0.99586 |  0:09:58s\n",
            "epoch 68 | loss: 0.07961 | train_auc: 0.99631 | valid_auc: 0.99585 |  0:10:07s\n",
            "epoch 69 | loss: 0.07877 | train_auc: 0.99653 | valid_auc: 0.99604 |  0:10:15s\n",
            "epoch 70 | loss: 0.07763 | train_auc: 0.99644 | valid_auc: 0.99594 |  0:10:24s\n",
            "epoch 71 | loss: 0.07678 | train_auc: 0.9966  | valid_auc: 0.99611 |  0:10:33s\n",
            "epoch 72 | loss: 0.07776 | train_auc: 0.99648 | valid_auc: 0.99594 |  0:10:41s\n",
            "epoch 73 | loss: 0.07863 | train_auc: 0.99644 | valid_auc: 0.99587 |  0:10:50s\n",
            "epoch 74 | loss: 0.07809 | train_auc: 0.99622 | valid_auc: 0.99574 |  0:10:59s\n",
            "epoch 75 | loss: 0.07822 | train_auc: 0.99589 | valid_auc: 0.99535 |  0:11:08s\n",
            "epoch 76 | loss: 0.07762 | train_auc: 0.99673 | valid_auc: 0.9962  |  0:11:16s\n",
            "epoch 77 | loss: 0.07622 | train_auc: 0.99583 | valid_auc: 0.99526 |  0:11:25s\n",
            "epoch 78 | loss: 0.07624 | train_auc: 0.99601 | valid_auc: 0.99539 |  0:11:34s\n",
            "epoch 79 | loss: 0.07653 | train_auc: 0.99673 | valid_auc: 0.99624 |  0:11:43s\n",
            "epoch 80 | loss: 0.07733 | train_auc: 0.99657 | valid_auc: 0.99609 |  0:11:52s\n",
            "epoch 81 | loss: 0.07704 | train_auc: 0.99581 | valid_auc: 0.99515 |  0:12:01s\n",
            "epoch 82 | loss: 0.07607 | train_auc: 0.99404 | valid_auc: 0.99329 |  0:12:10s\n",
            "epoch 83 | loss: 0.07631 | train_auc: 0.99596 | valid_auc: 0.99536 |  0:12:19s\n",
            "epoch 84 | loss: 0.07557 | train_auc: 0.99667 | valid_auc: 0.99616 |  0:12:27s\n",
            "epoch 85 | loss: 0.07489 | train_auc: 0.99673 | valid_auc: 0.99617 |  0:12:36s\n",
            "epoch 86 | loss: 0.07512 | train_auc: 0.99666 | valid_auc: 0.99614 |  0:12:45s\n",
            "epoch 87 | loss: 0.07409 | train_auc: 0.99636 | valid_auc: 0.9958  |  0:12:54s\n",
            "epoch 88 | loss: 0.07335 | train_auc: 0.99674 | valid_auc: 0.99626 |  0:13:03s\n",
            "epoch 89 | loss: 0.07361 | train_auc: 0.99587 | valid_auc: 0.99539 |  0:13:12s\n",
            "epoch 90 | loss: 0.07469 | train_auc: 0.99667 | valid_auc: 0.9962  |  0:13:20s\n",
            "epoch 91 | loss: 0.0738  | train_auc: 0.99682 | valid_auc: 0.99625 |  0:13:29s\n",
            "epoch 92 | loss: 0.07281 | train_auc: 0.99683 | valid_auc: 0.99629 |  0:13:38s\n",
            "epoch 93 | loss: 0.07239 | train_auc: 0.99691 | valid_auc: 0.99642 |  0:13:47s\n",
            "epoch 94 | loss: 0.07257 | train_auc: 0.99666 | valid_auc: 0.99614 |  0:13:56s\n",
            "epoch 95 | loss: 0.0727  | train_auc: 0.99646 | valid_auc: 0.99591 |  0:14:05s\n",
            "epoch 96 | loss: 0.07218 | train_auc: 0.99655 | valid_auc: 0.99604 |  0:14:14s\n",
            "epoch 97 | loss: 0.07319 | train_auc: 0.99644 | valid_auc: 0.99598 |  0:14:24s\n",
            "epoch 98 | loss: 0.07323 | train_auc: 0.99558 | valid_auc: 0.99501 |  0:14:32s\n",
            "epoch 99 | loss: 0.07177 | train_auc: 0.99646 | valid_auc: 0.99591 |  0:14:41s\n",
            "epoch 100| loss: 0.07243 | train_auc: 0.99635 | valid_auc: 0.99582 |  0:14:50s\n",
            "epoch 101| loss: 0.0714  | train_auc: 0.99681 | valid_auc: 0.99626 |  0:14:59s\n",
            "epoch 102| loss: 0.07256 | train_auc: 0.9966  | valid_auc: 0.99608 |  0:15:08s\n",
            "epoch 103| loss: 0.07215 | train_auc: 0.9967  | valid_auc: 0.99617 |  0:15:17s\n",
            "epoch 104| loss: 0.07196 | train_auc: 0.99687 | valid_auc: 0.99631 |  0:15:26s\n",
            "epoch 105| loss: 0.07188 | train_auc: 0.99702 | valid_auc: 0.99647 |  0:15:35s\n",
            "epoch 106| loss: 0.07135 | train_auc: 0.9972  | valid_auc: 0.99666 |  0:15:44s\n",
            "epoch 107| loss: 0.07082 | train_auc: 0.99697 | valid_auc: 0.99646 |  0:15:53s\n",
            "epoch 108| loss: 0.07097 | train_auc: 0.99692 | valid_auc: 0.99643 |  0:16:02s\n",
            "epoch 109| loss: 0.07181 | train_auc: 0.99691 | valid_auc: 0.99634 |  0:16:11s\n",
            "epoch 110| loss: 0.07109 | train_auc: 0.99708 | valid_auc: 0.9966  |  0:16:20s\n",
            "epoch 111| loss: 0.07092 | train_auc: 0.99708 | valid_auc: 0.99654 |  0:16:29s\n",
            "epoch 112| loss: 0.0707  | train_auc: 0.99689 | valid_auc: 0.99635 |  0:16:37s\n",
            "epoch 113| loss: 0.0704  | train_auc: 0.99714 | valid_auc: 0.99661 |  0:16:46s\n",
            "epoch 114| loss: 0.0719  | train_auc: 0.99683 | valid_auc: 0.99621 |  0:16:55s\n",
            "epoch 115| loss: 0.07207 | train_auc: 0.99713 | valid_auc: 0.99657 |  0:17:04s\n",
            "epoch 116| loss: 0.07059 | train_auc: 0.99714 | valid_auc: 0.99657 |  0:17:12s\n",
            "epoch 117| loss: 0.06989 | train_auc: 0.99706 | valid_auc: 0.99647 |  0:17:21s\n",
            "epoch 118| loss: 0.06987 | train_auc: 0.99713 | valid_auc: 0.99663 |  0:17:30s\n",
            "epoch 119| loss: 0.07005 | train_auc: 0.99709 | valid_auc: 0.99657 |  0:17:39s\n",
            "epoch 120| loss: 0.07041 | train_auc: 0.99725 | valid_auc: 0.9967  |  0:17:48s\n",
            "epoch 121| loss: 0.06955 | train_auc: 0.99722 | valid_auc: 0.99667 |  0:17:57s\n",
            "epoch 122| loss: 0.07014 | train_auc: 0.99717 | valid_auc: 0.99664 |  0:18:06s\n",
            "epoch 123| loss: 0.06936 | train_auc: 0.99701 | valid_auc: 0.99646 |  0:18:15s\n",
            "epoch 124| loss: 0.06953 | train_auc: 0.99719 | valid_auc: 0.99661 |  0:18:24s\n",
            "epoch 125| loss: 0.06933 | train_auc: 0.99722 | valid_auc: 0.99665 |  0:18:33s\n",
            "epoch 126| loss: 0.06937 | train_auc: 0.99728 | valid_auc: 0.99669 |  0:18:42s\n",
            "epoch 127| loss: 0.06966 | train_auc: 0.99723 | valid_auc: 0.99668 |  0:18:51s\n",
            "epoch 128| loss: 0.07016 | train_auc: 0.99696 | valid_auc: 0.99639 |  0:18:59s\n",
            "epoch 129| loss: 0.06953 | train_auc: 0.99721 | valid_auc: 0.99667 |  0:19:08s\n",
            "epoch 130| loss: 0.06936 | train_auc: 0.99734 | valid_auc: 0.9968  |  0:19:17s\n",
            "epoch 131| loss: 0.06872 | train_auc: 0.9973  | valid_auc: 0.99671 |  0:19:26s\n",
            "epoch 132| loss: 0.06904 | train_auc: 0.99724 | valid_auc: 0.99671 |  0:19:35s\n",
            "epoch 133| loss: 0.06894 | train_auc: 0.99722 | valid_auc: 0.99668 |  0:19:43s\n",
            "epoch 134| loss: 0.06879 | train_auc: 0.99731 | valid_auc: 0.99672 |  0:19:52s\n",
            "epoch 135| loss: 0.06908 | train_auc: 0.99729 | valid_auc: 0.99676 |  0:20:02s\n",
            "epoch 136| loss: 0.06912 | train_auc: 0.99722 | valid_auc: 0.99668 |  0:20:11s\n",
            "epoch 137| loss: 0.07038 | train_auc: 0.99726 | valid_auc: 0.99673 |  0:20:19s\n",
            "epoch 138| loss: 0.06872 | train_auc: 0.99726 | valid_auc: 0.99668 |  0:20:28s\n",
            "epoch 139| loss: 0.06955 | train_auc: 0.99736 | valid_auc: 0.99681 |  0:20:37s\n",
            "epoch 140| loss: 0.06903 | train_auc: 0.99728 | valid_auc: 0.99671 |  0:20:46s\n",
            "epoch 141| loss: 0.06868 | train_auc: 0.99726 | valid_auc: 0.99665 |  0:20:55s\n",
            "epoch 142| loss: 0.06826 | train_auc: 0.99734 | valid_auc: 0.99676 |  0:21:04s\n",
            "epoch 143| loss: 0.06845 | train_auc: 0.99732 | valid_auc: 0.99673 |  0:21:13s\n",
            "epoch 144| loss: 0.06796 | train_auc: 0.99734 | valid_auc: 0.99672 |  0:21:22s\n",
            "epoch 145| loss: 0.06824 | train_auc: 0.99724 | valid_auc: 0.99668 |  0:21:30s\n",
            "epoch 146| loss: 0.0682  | train_auc: 0.99734 | valid_auc: 0.99676 |  0:21:39s\n",
            "epoch 147| loss: 0.06867 | train_auc: 0.99738 | valid_auc: 0.99684 |  0:21:48s\n",
            "epoch 148| loss: 0.06881 | train_auc: 0.99741 | valid_auc: 0.99682 |  0:21:57s\n",
            "epoch 149| loss: 0.06799 | train_auc: 0.99736 | valid_auc: 0.99676 |  0:22:06s\n",
            "epoch 150| loss: 0.0674  | train_auc: 0.99738 | valid_auc: 0.99677 |  0:22:15s\n",
            "epoch 151| loss: 0.06715 | train_auc: 0.99727 | valid_auc: 0.9967  |  0:22:23s\n",
            "epoch 152| loss: 0.06806 | train_auc: 0.99736 | valid_auc: 0.9968  |  0:22:33s\n",
            "epoch 153| loss: 0.06704 | train_auc: 0.99742 | valid_auc: 0.99686 |  0:22:41s\n",
            "epoch 154| loss: 0.06713 | train_auc: 0.99732 | valid_auc: 0.99681 |  0:22:51s\n",
            "epoch 155| loss: 0.06734 | train_auc: 0.99743 | valid_auc: 0.99685 |  0:23:00s\n",
            "epoch 156| loss: 0.06716 | train_auc: 0.99734 | valid_auc: 0.99675 |  0:23:09s\n",
            "epoch 157| loss: 0.06727 | train_auc: 0.99735 | valid_auc: 0.99675 |  0:23:17s\n",
            "epoch 158| loss: 0.06743 | train_auc: 0.99744 | valid_auc: 0.99684 |  0:23:26s\n",
            "epoch 159| loss: 0.0683  | train_auc: 0.99732 | valid_auc: 0.99673 |  0:23:35s\n",
            "epoch 160| loss: 0.0677  | train_auc: 0.99736 | valid_auc: 0.99675 |  0:23:43s\n",
            "epoch 161| loss: 0.06771 | train_auc: 0.99746 | valid_auc: 0.9969  |  0:23:52s\n",
            "epoch 162| loss: 0.06726 | train_auc: 0.99736 | valid_auc: 0.99678 |  0:24:01s\n",
            "epoch 163| loss: 0.06709 | train_auc: 0.99749 | valid_auc: 0.99689 |  0:24:10s\n",
            "epoch 164| loss: 0.06691 | train_auc: 0.99733 | valid_auc: 0.99675 |  0:24:19s\n",
            "epoch 165| loss: 0.06676 | train_auc: 0.99736 | valid_auc: 0.99678 |  0:24:28s\n",
            "epoch 166| loss: 0.06661 | train_auc: 0.99749 | valid_auc: 0.9969  |  0:24:37s\n",
            "epoch 167| loss: 0.06713 | train_auc: 0.99744 | valid_auc: 0.99686 |  0:24:46s\n",
            "epoch 168| loss: 0.06684 | train_auc: 0.99744 | valid_auc: 0.99692 |  0:24:55s\n",
            "epoch 169| loss: 0.06728 | train_auc: 0.99738 | valid_auc: 0.99684 |  0:25:04s\n",
            "epoch 170| loss: 0.06688 | train_auc: 0.99748 | valid_auc: 0.99688 |  0:25:13s\n",
            "epoch 171| loss: 0.06699 | train_auc: 0.9975  | valid_auc: 0.99688 |  0:25:22s\n",
            "epoch 172| loss: 0.06692 | train_auc: 0.99742 | valid_auc: 0.99678 |  0:25:30s\n",
            "epoch 173| loss: 0.06679 | train_auc: 0.99755 | valid_auc: 0.99694 |  0:25:39s\n",
            "epoch 174| loss: 0.06659 | train_auc: 0.99755 | valid_auc: 0.99699 |  0:25:48s\n",
            "epoch 175| loss: 0.06685 | train_auc: 0.99742 | valid_auc: 0.99681 |  0:25:57s\n",
            "epoch 176| loss: 0.06663 | train_auc: 0.9975  | valid_auc: 0.99692 |  0:26:06s\n",
            "epoch 177| loss: 0.06624 | train_auc: 0.99749 | valid_auc: 0.99689 |  0:26:14s\n",
            "epoch 178| loss: 0.06605 | train_auc: 0.99752 | valid_auc: 0.99693 |  0:26:23s\n",
            "epoch 179| loss: 0.06642 | train_auc: 0.99743 | valid_auc: 0.9968  |  0:26:32s\n",
            "epoch 180| loss: 0.06705 | train_auc: 0.99744 | valid_auc: 0.99685 |  0:26:41s\n",
            "epoch 181| loss: 0.0667  | train_auc: 0.99743 | valid_auc: 0.9968  |  0:26:50s\n",
            "epoch 182| loss: 0.06713 | train_auc: 0.99752 | valid_auc: 0.99698 |  0:26:59s\n",
            "epoch 183| loss: 0.06657 | train_auc: 0.99754 | valid_auc: 0.99694 |  0:27:08s\n",
            "epoch 184| loss: 0.06589 | train_auc: 0.99755 | valid_auc: 0.99694 |  0:27:17s\n",
            "epoch 185| loss: 0.06596 | train_auc: 0.99751 | valid_auc: 0.9969  |  0:27:26s\n",
            "epoch 186| loss: 0.06587 | train_auc: 0.99751 | valid_auc: 0.99695 |  0:27:35s\n",
            "epoch 187| loss: 0.06631 | train_auc: 0.9976  | valid_auc: 0.99699 |  0:27:44s\n",
            "epoch 188| loss: 0.06612 | train_auc: 0.99751 | valid_auc: 0.99695 |  0:27:53s\n",
            "epoch 189| loss: 0.06604 | train_auc: 0.99752 | valid_auc: 0.99691 |  0:28:02s\n",
            "epoch 190| loss: 0.06614 | train_auc: 0.99752 | valid_auc: 0.99694 |  0:28:11s\n",
            "epoch 191| loss: 0.06565 | train_auc: 0.99754 | valid_auc: 0.99692 |  0:28:19s\n",
            "epoch 192| loss: 0.06622 | train_auc: 0.99742 | valid_auc: 0.99684 |  0:28:28s\n",
            "epoch 193| loss: 0.06603 | train_auc: 0.99755 | valid_auc: 0.99693 |  0:28:37s\n",
            "epoch 194| loss: 0.06544 | train_auc: 0.99752 | valid_auc: 0.99688 |  0:28:46s\n",
            "\n",
            "Early stopping occurred at epoch 194 with best_epoch = 174 and best_valid_auc = 0.99699\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9969903220847864\n",
            "----- 4 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 17390770.11746| val_0_unsup_loss: 9383.84473|  0:00:05s\n",
            "epoch 1  | loss: 4636.31789| val_0_unsup_loss: 2.6186  |  0:00:11s\n",
            "epoch 2  | loss: 16.85675| val_0_unsup_loss: 2.04328 |  0:00:16s\n",
            "epoch 3  | loss: 2.1357  | val_0_unsup_loss: 1.6959  |  0:00:22s\n",
            "epoch 4  | loss: 1.91087 | val_0_unsup_loss: 1.58098 |  0:00:28s\n",
            "epoch 5  | loss: 1.56718 | val_0_unsup_loss: 1.51103 |  0:00:34s\n",
            "epoch 6  | loss: 60.64986| val_0_unsup_loss: 1.46403 |  0:00:39s\n",
            "epoch 7  | loss: 2.35763 | val_0_unsup_loss: 1.42507 |  0:00:45s\n",
            "epoch 8  | loss: 1.70632 | val_0_unsup_loss: 1.3916  |  0:00:50s\n",
            "epoch 9  | loss: 1.39639 | val_0_unsup_loss: 1.37156 |  0:00:56s\n",
            "epoch 10 | loss: 1.42697 | val_0_unsup_loss: 1.35954 |  0:01:01s\n",
            "epoch 11 | loss: 1.38895 | val_0_unsup_loss: 1.35767 |  0:01:07s\n",
            "epoch 12 | loss: 2.36347 | val_0_unsup_loss: 1.35653 |  0:01:12s\n",
            "epoch 13 | loss: 1.35524 | val_0_unsup_loss: 1.35591 |  0:01:18s\n",
            "epoch 14 | loss: 1.35433 | val_0_unsup_loss: 1.35561 |  0:01:24s\n",
            "epoch 15 | loss: 1.35433 | val_0_unsup_loss: 1.35548 |  0:01:29s\n",
            "epoch 16 | loss: 1.35422 | val_0_unsup_loss: 1.35542 |  0:01:35s\n",
            "epoch 17 | loss: 1.35389 | val_0_unsup_loss: 1.35538 |  0:01:40s\n",
            "epoch 18 | loss: 1.36586 | val_0_unsup_loss: 1.35552 |  0:01:46s\n",
            "epoch 19 | loss: 1.35599 | val_0_unsup_loss: 1.3555  |  0:01:51s\n",
            "epoch 20 | loss: 1.35413 | val_0_unsup_loss: 1.3554  |  0:01:57s\n",
            "epoch 21 | loss: 1.41492 | val_0_unsup_loss: 1.35537 |  0:02:03s\n",
            "epoch 22 | loss: 4.00539 | val_0_unsup_loss: 1.3554  |  0:02:08s\n",
            "epoch 23 | loss: 1.43611 | val_0_unsup_loss: 1.35535 |  0:02:14s\n",
            "epoch 24 | loss: 1.3538  | val_0_unsup_loss: 1.35534 |  0:02:19s\n",
            "epoch 25 | loss: 1.35413 | val_0_unsup_loss: 1.35534 |  0:02:25s\n",
            "epoch 26 | loss: 1.35393 | val_0_unsup_loss: 1.35533 |  0:02:30s\n",
            "epoch 27 | loss: 1.35397 | val_0_unsup_loss: 1.35532 |  0:02:36s\n",
            "epoch 28 | loss: 1.35428 | val_0_unsup_loss: 1.35532 |  0:02:42s\n",
            "epoch 29 | loss: 1.35424 | val_0_unsup_loss: 1.35531 |  0:02:47s\n",
            "epoch 30 | loss: 1.3539  | val_0_unsup_loss: 1.35531 |  0:02:53s\n",
            "epoch 31 | loss: 1.35397 | val_0_unsup_loss: 1.35531 |  0:02:58s\n",
            "epoch 32 | loss: 1.35396 | val_0_unsup_loss: 1.35531 |  0:03:04s\n",
            "epoch 33 | loss: 1.35411 | val_0_unsup_loss: 1.35531 |  0:03:10s\n",
            "epoch 34 | loss: 1.35427 | val_0_unsup_loss: 1.35531 |  0:03:15s\n",
            "epoch 35 | loss: 1.35405 | val_0_unsup_loss: 1.3553  |  0:03:21s\n",
            "epoch 36 | loss: 1.35411 | val_0_unsup_loss: 1.3553  |  0:03:27s\n",
            "epoch 37 | loss: 1.35442 | val_0_unsup_loss: 1.3553  |  0:03:32s\n",
            "epoch 38 | loss: 1.35412 | val_0_unsup_loss: 1.3553  |  0:03:38s\n",
            "epoch 39 | loss: 1.3541  | val_0_unsup_loss: 1.3553  |  0:03:44s\n",
            "epoch 40 | loss: 1.3538  | val_0_unsup_loss: 1.3553  |  0:03:49s\n",
            "epoch 41 | loss: 1.35376 | val_0_unsup_loss: 1.3553  |  0:03:55s\n",
            "epoch 42 | loss: 1.35409 | val_0_unsup_loss: 1.3553  |  0:04:00s\n",
            "epoch 43 | loss: 1.35377 | val_0_unsup_loss: 1.3553  |  0:04:06s\n",
            "epoch 44 | loss: 1.35414 | val_0_unsup_loss: 1.3553  |  0:04:11s\n",
            "epoch 45 | loss: 1.35404 | val_0_unsup_loss: 1.3553  |  0:04:17s\n",
            "epoch 46 | loss: 1.35404 | val_0_unsup_loss: 1.3553  |  0:04:22s\n",
            "epoch 47 | loss: 1.3541  | val_0_unsup_loss: 1.3553  |  0:04:28s\n",
            "epoch 48 | loss: 1.3541  | val_0_unsup_loss: 1.3553  |  0:04:34s\n",
            "epoch 49 | loss: 1.35409 | val_0_unsup_loss: 1.3553  |  0:04:39s\n",
            "epoch 50 | loss: 1.35403 | val_0_unsup_loss: 1.3553  |  0:04:45s\n",
            "epoch 51 | loss: 1.4588  | val_0_unsup_loss: 1.35595 |  0:04:50s\n",
            "epoch 52 | loss: 1.3546  | val_0_unsup_loss: 1.35553 |  0:04:56s\n",
            "epoch 53 | loss: 1.35397 | val_0_unsup_loss: 1.35534 |  0:05:02s\n",
            "epoch 54 | loss: 1.35409 | val_0_unsup_loss: 1.35531 |  0:05:07s\n",
            "epoch 55 | loss: 1.43004 | val_0_unsup_loss: 1.35542 |  0:05:13s\n",
            "epoch 56 | loss: 1.36093 | val_0_unsup_loss: 1.35585 |  0:05:18s\n",
            "epoch 57 | loss: 1.66602 | val_0_unsup_loss: 1.35578 |  0:05:24s\n",
            "epoch 58 | loss: 1.39196 | val_0_unsup_loss: 1.35565 |  0:05:29s\n",
            "epoch 59 | loss: 1.35927 | val_0_unsup_loss: 1.35567 |  0:05:35s\n",
            "epoch 60 | loss: 1.44592 | val_0_unsup_loss: 1.35548 |  0:05:40s\n",
            "epoch 61 | loss: 1.35396 | val_0_unsup_loss: 1.35535 |  0:05:46s\n",
            "epoch 62 | loss: 1.35421 | val_0_unsup_loss: 1.35532 |  0:05:52s\n",
            "epoch 63 | loss: 1.37116 | val_0_unsup_loss: 1.35549 |  0:05:57s\n",
            "epoch 64 | loss: 1.35435 | val_0_unsup_loss: 1.35557 |  0:06:03s\n",
            "epoch 65 | loss: 1.35657 | val_0_unsup_loss: 1.35574 |  0:06:08s\n",
            "\n",
            "Early stopping occurred at epoch 65 with best_epoch = 45 and best_val_0_unsup_loss = 1.3553\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.69205 | train_auc: 0.5568  | valid_auc: 0.5544  |  0:00:08s\n",
            "epoch 1  | loss: 0.57558 | train_auc: 0.70809 | valid_auc: 0.70636 |  0:00:17s\n",
            "epoch 2  | loss: 0.33058 | train_auc: 0.91199 | valid_auc: 0.91266 |  0:00:26s\n",
            "epoch 3  | loss: 0.23251 | train_auc: 0.91384 | valid_auc: 0.91419 |  0:00:34s\n",
            "epoch 4  | loss: 0.20594 | train_auc: 0.95028 | valid_auc: 0.94999 |  0:00:43s\n",
            "epoch 5  | loss: 0.1894  | train_auc: 0.95484 | valid_auc: 0.95507 |  0:00:53s\n",
            "epoch 6  | loss: 0.18332 | train_auc: 0.95804 | valid_auc: 0.95829 |  0:01:02s\n",
            "epoch 7  | loss: 0.17973 | train_auc: 0.96905 | valid_auc: 0.9684  |  0:01:11s\n",
            "epoch 8  | loss: 0.17706 | train_auc: 0.9725  | valid_auc: 0.97179 |  0:01:20s\n",
            "epoch 9  | loss: 0.17484 | train_auc: 0.97229 | valid_auc: 0.97162 |  0:01:29s\n",
            "epoch 10 | loss: 0.17249 | train_auc: 0.97344 | valid_auc: 0.97282 |  0:01:38s\n",
            "epoch 11 | loss: 0.16944 | train_auc: 0.97739 | valid_auc: 0.97731 |  0:01:48s\n",
            "epoch 12 | loss: 0.16562 | train_auc: 0.96847 | valid_auc: 0.96785 |  0:01:57s\n",
            "epoch 13 | loss: 0.16228 | train_auc: 0.97839 | valid_auc: 0.97852 |  0:02:06s\n",
            "epoch 14 | loss: 0.15928 | train_auc: 0.97848 | valid_auc: 0.97858 |  0:02:15s\n",
            "epoch 15 | loss: 0.15438 | train_auc: 0.98017 | valid_auc: 0.97969 |  0:02:24s\n",
            "epoch 16 | loss: 0.14865 | train_auc: 0.98196 | valid_auc: 0.98164 |  0:02:33s\n",
            "epoch 17 | loss: 0.14518 | train_auc: 0.98528 | valid_auc: 0.98483 |  0:02:42s\n",
            "epoch 18 | loss: 0.14266 | train_auc: 0.98341 | valid_auc: 0.98295 |  0:02:51s\n",
            "epoch 19 | loss: 0.1389  | train_auc: 0.98669 | valid_auc: 0.98623 |  0:03:00s\n",
            "epoch 20 | loss: 0.1355  | train_auc: 0.98621 | valid_auc: 0.98571 |  0:03:08s\n",
            "epoch 21 | loss: 0.13289 | train_auc: 0.98678 | valid_auc: 0.98616 |  0:03:17s\n",
            "epoch 22 | loss: 0.12982 | train_auc: 0.98771 | valid_auc: 0.98707 |  0:03:26s\n",
            "epoch 23 | loss: 0.1268  | train_auc: 0.98862 | valid_auc: 0.98805 |  0:03:35s\n",
            "epoch 24 | loss: 0.12672 | train_auc: 0.98677 | valid_auc: 0.98625 |  0:03:44s\n",
            "epoch 25 | loss: 0.12435 | train_auc: 0.98928 | valid_auc: 0.9886  |  0:03:54s\n",
            "epoch 26 | loss: 0.12409 | train_auc: 0.98849 | valid_auc: 0.98806 |  0:04:02s\n",
            "epoch 27 | loss: 0.1236  | train_auc: 0.98837 | valid_auc: 0.98788 |  0:04:11s\n",
            "epoch 28 | loss: 0.12155 | train_auc: 0.9893  | valid_auc: 0.98883 |  0:04:21s\n",
            "epoch 29 | loss: 0.12051 | train_auc: 0.99004 | valid_auc: 0.98948 |  0:04:30s\n",
            "epoch 30 | loss: 0.1195  | train_auc: 0.99054 | valid_auc: 0.99013 |  0:04:38s\n",
            "epoch 31 | loss: 0.11574 | train_auc: 0.99026 | valid_auc: 0.9898  |  0:04:47s\n",
            "epoch 32 | loss: 0.11366 | train_auc: 0.98949 | valid_auc: 0.98898 |  0:04:56s\n",
            "epoch 33 | loss: 0.10918 | train_auc: 0.98907 | valid_auc: 0.9887  |  0:05:05s\n",
            "epoch 34 | loss: 0.10734 | train_auc: 0.9921  | valid_auc: 0.99167 |  0:05:14s\n",
            "epoch 35 | loss: 0.10515 | train_auc: 0.9931  | valid_auc: 0.99249 |  0:05:23s\n",
            "epoch 36 | loss: 0.10399 | train_auc: 0.99227 | valid_auc: 0.99173 |  0:05:32s\n",
            "epoch 37 | loss: 0.10301 | train_auc: 0.99254 | valid_auc: 0.99187 |  0:05:41s\n",
            "epoch 38 | loss: 0.10232 | train_auc: 0.99315 | valid_auc: 0.99247 |  0:05:50s\n",
            "epoch 39 | loss: 0.10171 | train_auc: 0.99358 | valid_auc: 0.99293 |  0:05:59s\n",
            "epoch 40 | loss: 0.10203 | train_auc: 0.99336 | valid_auc: 0.99277 |  0:06:08s\n",
            "epoch 41 | loss: 0.1015  | train_auc: 0.9935  | valid_auc: 0.99279 |  0:06:17s\n",
            "epoch 42 | loss: 0.1003  | train_auc: 0.99415 | valid_auc: 0.9935  |  0:06:26s\n",
            "epoch 43 | loss: 0.1015  | train_auc: 0.99374 | valid_auc: 0.99311 |  0:06:35s\n",
            "epoch 44 | loss: 0.09921 | train_auc: 0.9934  | valid_auc: 0.99276 |  0:06:44s\n",
            "epoch 45 | loss: 0.09793 | train_auc: 0.99393 | valid_auc: 0.99332 |  0:06:53s\n",
            "epoch 46 | loss: 0.09749 | train_auc: 0.99412 | valid_auc: 0.9934  |  0:07:02s\n",
            "epoch 47 | loss: 0.09762 | train_auc: 0.99432 | valid_auc: 0.9937  |  0:07:10s\n",
            "epoch 48 | loss: 0.09916 | train_auc: 0.99383 | valid_auc: 0.99307 |  0:07:19s\n",
            "epoch 49 | loss: 0.10162 | train_auc: 0.99391 | valid_auc: 0.99323 |  0:07:28s\n",
            "epoch 50 | loss: 0.09747 | train_auc: 0.99417 | valid_auc: 0.99347 |  0:07:37s\n",
            "epoch 51 | loss: 0.0973  | train_auc: 0.99432 | valid_auc: 0.99365 |  0:07:46s\n",
            "epoch 52 | loss: 0.09786 | train_auc: 0.99406 | valid_auc: 0.99339 |  0:07:55s\n",
            "epoch 53 | loss: 0.09651 | train_auc: 0.99414 | valid_auc: 0.99335 |  0:08:04s\n",
            "epoch 54 | loss: 0.09584 | train_auc: 0.99436 | valid_auc: 0.99361 |  0:08:13s\n",
            "epoch 55 | loss: 0.09975 | train_auc: 0.99399 | valid_auc: 0.99331 |  0:08:22s\n",
            "epoch 56 | loss: 0.09681 | train_auc: 0.99295 | valid_auc: 0.99225 |  0:08:31s\n",
            "epoch 57 | loss: 0.09554 | train_auc: 0.99231 | valid_auc: 0.99162 |  0:08:40s\n",
            "epoch 58 | loss: 0.09405 | train_auc: 0.99375 | valid_auc: 0.99314 |  0:08:49s\n",
            "epoch 59 | loss: 0.09403 | train_auc: 0.99448 | valid_auc: 0.99372 |  0:08:58s\n",
            "epoch 60 | loss: 0.09325 | train_auc: 0.99467 | valid_auc: 0.9939  |  0:09:07s\n",
            "epoch 61 | loss: 0.09332 | train_auc: 0.99487 | valid_auc: 0.99418 |  0:09:15s\n",
            "epoch 62 | loss: 0.09251 | train_auc: 0.99465 | valid_auc: 0.99394 |  0:09:24s\n",
            "epoch 63 | loss: 0.0924  | train_auc: 0.99429 | valid_auc: 0.99353 |  0:09:33s\n",
            "epoch 64 | loss: 0.09206 | train_auc: 0.99491 | valid_auc: 0.9942  |  0:09:42s\n",
            "epoch 65 | loss: 0.09209 | train_auc: 0.99484 | valid_auc: 0.99411 |  0:09:51s\n",
            "epoch 66 | loss: 0.09145 | train_auc: 0.99477 | valid_auc: 0.99403 |  0:10:00s\n",
            "epoch 67 | loss: 0.09095 | train_auc: 0.99502 | valid_auc: 0.99427 |  0:10:09s\n",
            "epoch 68 | loss: 0.09144 | train_auc: 0.99487 | valid_auc: 0.99424 |  0:10:18s\n",
            "epoch 69 | loss: 0.09207 | train_auc: 0.99426 | valid_auc: 0.99355 |  0:10:28s\n",
            "epoch 70 | loss: 0.09115 | train_auc: 0.99518 | valid_auc: 0.99453 |  0:10:37s\n",
            "epoch 71 | loss: 0.09088 | train_auc: 0.99507 | valid_auc: 0.9944  |  0:10:45s\n",
            "epoch 72 | loss: 0.09037 | train_auc: 0.9953  | valid_auc: 0.99457 |  0:10:54s\n",
            "epoch 73 | loss: 0.09134 | train_auc: 0.9952  | valid_auc: 0.99447 |  0:11:03s\n",
            "epoch 74 | loss: 0.09024 | train_auc: 0.99527 | valid_auc: 0.99459 |  0:11:12s\n",
            "epoch 75 | loss: 0.09026 | train_auc: 0.99533 | valid_auc: 0.99465 |  0:11:21s\n",
            "epoch 76 | loss: 0.08993 | train_auc: 0.99526 | valid_auc: 0.99456 |  0:11:30s\n",
            "epoch 77 | loss: 0.09041 | train_auc: 0.99522 | valid_auc: 0.99448 |  0:11:39s\n",
            "epoch 78 | loss: 0.09034 | train_auc: 0.99512 | valid_auc: 0.99446 |  0:11:48s\n",
            "epoch 79 | loss: 0.08956 | train_auc: 0.99487 | valid_auc: 0.99422 |  0:11:57s\n",
            "epoch 80 | loss: 0.08852 | train_auc: 0.9954  | valid_auc: 0.9947  |  0:12:06s\n",
            "epoch 81 | loss: 0.08561 | train_auc: 0.99571 | valid_auc: 0.99502 |  0:12:15s\n",
            "epoch 82 | loss: 0.0849  | train_auc: 0.99572 | valid_auc: 0.99514 |  0:12:24s\n",
            "epoch 83 | loss: 0.08468 | train_auc: 0.9957  | valid_auc: 0.99514 |  0:12:33s\n",
            "epoch 84 | loss: 0.08429 | train_auc: 0.99554 | valid_auc: 0.99493 |  0:12:42s\n",
            "epoch 85 | loss: 0.08278 | train_auc: 0.99583 | valid_auc: 0.99521 |  0:12:52s\n",
            "epoch 86 | loss: 0.08289 | train_auc: 0.9957  | valid_auc: 0.9951  |  0:13:00s\n",
            "epoch 87 | loss: 0.08367 | train_auc: 0.996   | valid_auc: 0.99534 |  0:13:09s\n",
            "epoch 88 | loss: 0.08272 | train_auc: 0.99582 | valid_auc: 0.99514 |  0:13:18s\n",
            "epoch 89 | loss: 0.08317 | train_auc: 0.99589 | valid_auc: 0.99526 |  0:13:27s\n",
            "epoch 90 | loss: 0.08258 | train_auc: 0.99586 | valid_auc: 0.99525 |  0:13:36s\n",
            "epoch 91 | loss: 0.08182 | train_auc: 0.99573 | valid_auc: 0.99515 |  0:13:45s\n",
            "epoch 92 | loss: 0.08113 | train_auc: 0.99588 | valid_auc: 0.99527 |  0:13:54s\n",
            "epoch 93 | loss: 0.0816  | train_auc: 0.99576 | valid_auc: 0.99514 |  0:14:03s\n",
            "epoch 94 | loss: 0.08049 | train_auc: 0.99601 | valid_auc: 0.99536 |  0:14:12s\n",
            "epoch 95 | loss: 0.08065 | train_auc: 0.99612 | valid_auc: 0.99548 |  0:14:21s\n",
            "epoch 96 | loss: 0.08039 | train_auc: 0.99586 | valid_auc: 0.99523 |  0:14:30s\n",
            "epoch 97 | loss: 0.08057 | train_auc: 0.99609 | valid_auc: 0.99543 |  0:14:39s\n",
            "epoch 98 | loss: 0.08025 | train_auc: 0.99592 | valid_auc: 0.99521 |  0:14:48s\n",
            "epoch 99 | loss: 0.08115 | train_auc: 0.99635 | valid_auc: 0.99569 |  0:14:57s\n",
            "epoch 100| loss: 0.08072 | train_auc: 0.99604 | valid_auc: 0.99542 |  0:15:06s\n",
            "epoch 101| loss: 0.0801  | train_auc: 0.99553 | valid_auc: 0.99483 |  0:15:15s\n",
            "epoch 102| loss: 0.07998 | train_auc: 0.99597 | valid_auc: 0.99535 |  0:15:24s\n",
            "epoch 103| loss: 0.0831  | train_auc: 0.99479 | valid_auc: 0.99416 |  0:15:33s\n",
            "epoch 104| loss: 0.08262 | train_auc: 0.99524 | valid_auc: 0.99466 |  0:15:42s\n",
            "epoch 105| loss: 0.08087 | train_auc: 0.99582 | valid_auc: 0.99522 |  0:15:51s\n",
            "epoch 106| loss: 0.08005 | train_auc: 0.9962  | valid_auc: 0.99559 |  0:16:00s\n",
            "epoch 107| loss: 0.0798  | train_auc: 0.99603 | valid_auc: 0.99542 |  0:16:09s\n",
            "epoch 108| loss: 0.08008 | train_auc: 0.99625 | valid_auc: 0.99559 |  0:16:18s\n",
            "epoch 109| loss: 0.08014 | train_auc: 0.9962  | valid_auc: 0.99553 |  0:16:27s\n",
            "epoch 110| loss: 0.07911 | train_auc: 0.99634 | valid_auc: 0.99571 |  0:16:36s\n",
            "epoch 111| loss: 0.07874 | train_auc: 0.9966  | valid_auc: 0.99598 |  0:16:45s\n",
            "epoch 112| loss: 0.0782  | train_auc: 0.99651 | valid_auc: 0.9959  |  0:16:54s\n",
            "epoch 113| loss: 0.0781  | train_auc: 0.9962  | valid_auc: 0.99556 |  0:17:03s\n",
            "epoch 114| loss: 0.07901 | train_auc: 0.9963  | valid_auc: 0.99562 |  0:17:12s\n",
            "epoch 115| loss: 0.07794 | train_auc: 0.9965  | valid_auc: 0.99583 |  0:17:21s\n",
            "epoch 116| loss: 0.0783  | train_auc: 0.9962  | valid_auc: 0.99558 |  0:17:30s\n",
            "epoch 117| loss: 0.07861 | train_auc: 0.99654 | valid_auc: 0.99587 |  0:17:39s\n",
            "epoch 118| loss: 0.0773  | train_auc: 0.99637 | valid_auc: 0.99576 |  0:17:48s\n",
            "epoch 119| loss: 0.07755 | train_auc: 0.99635 | valid_auc: 0.99566 |  0:17:57s\n",
            "epoch 120| loss: 0.07776 | train_auc: 0.99636 | valid_auc: 0.99569 |  0:18:06s\n",
            "epoch 121| loss: 0.07743 | train_auc: 0.99662 | valid_auc: 0.99597 |  0:18:15s\n",
            "epoch 122| loss: 0.07774 | train_auc: 0.99649 | valid_auc: 0.99578 |  0:18:24s\n",
            "epoch 123| loss: 0.07701 | train_auc: 0.99657 | valid_auc: 0.99591 |  0:18:33s\n",
            "epoch 124| loss: 0.07702 | train_auc: 0.99672 | valid_auc: 0.99607 |  0:18:42s\n",
            "epoch 125| loss: 0.0772  | train_auc: 0.99653 | valid_auc: 0.99579 |  0:18:51s\n",
            "epoch 126| loss: 0.07632 | train_auc: 0.99664 | valid_auc: 0.99594 |  0:19:00s\n",
            "epoch 127| loss: 0.07617 | train_auc: 0.99674 | valid_auc: 0.99608 |  0:19:09s\n",
            "epoch 128| loss: 0.07598 | train_auc: 0.99674 | valid_auc: 0.99609 |  0:19:18s\n",
            "epoch 129| loss: 0.07559 | train_auc: 0.99667 | valid_auc: 0.99605 |  0:19:27s\n",
            "epoch 130| loss: 0.07613 | train_auc: 0.98066 | valid_auc: 0.97964 |  0:19:36s\n",
            "epoch 131| loss: 0.07564 | train_auc: 0.98767 | valid_auc: 0.98663 |  0:19:45s\n",
            "epoch 132| loss: 0.07612 | train_auc: 0.99568 | valid_auc: 0.99493 |  0:19:55s\n",
            "epoch 133| loss: 0.07556 | train_auc: 0.99663 | valid_auc: 0.99596 |  0:20:04s\n",
            "epoch 134| loss: 0.0765  | train_auc: 0.99665 | valid_auc: 0.99597 |  0:20:12s\n",
            "epoch 135| loss: 0.07615 | train_auc: 0.99634 | valid_auc: 0.99569 |  0:20:21s\n",
            "epoch 136| loss: 0.07581 | train_auc: 0.99658 | valid_auc: 0.99594 |  0:20:30s\n",
            "epoch 137| loss: 0.0753  | train_auc: 0.99657 | valid_auc: 0.99589 |  0:20:38s\n",
            "epoch 138| loss: 0.07491 | train_auc: 0.99662 | valid_auc: 0.99592 |  0:20:47s\n",
            "epoch 139| loss: 0.07501 | train_auc: 0.99671 | valid_auc: 0.99605 |  0:20:56s\n",
            "epoch 140| loss: 0.07451 | train_auc: 0.99674 | valid_auc: 0.99605 |  0:21:05s\n",
            "epoch 141| loss: 0.07469 | train_auc: 0.99675 | valid_auc: 0.99607 |  0:21:14s\n",
            "epoch 142| loss: 0.07452 | train_auc: 0.9966  | valid_auc: 0.99591 |  0:21:23s\n",
            "epoch 143| loss: 0.0748  | train_auc: 0.99673 | valid_auc: 0.99607 |  0:21:32s\n",
            "epoch 144| loss: 0.07439 | train_auc: 0.99661 | valid_auc: 0.99591 |  0:21:41s\n",
            "epoch 145| loss: 0.07479 | train_auc: 0.99672 | valid_auc: 0.996   |  0:21:51s\n",
            "epoch 146| loss: 0.07479 | train_auc: 0.99668 | valid_auc: 0.99601 |  0:22:00s\n",
            "epoch 147| loss: 0.0752  | train_auc: 0.99629 | valid_auc: 0.99553 |  0:22:09s\n",
            "epoch 148| loss: 0.07433 | train_auc: 0.99651 | valid_auc: 0.99576 |  0:22:18s\n",
            "\n",
            "Early stopping occurred at epoch 148 with best_epoch = 128 and best_valid_auc = 0.99609\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9960882836627988\n",
            "----- 5 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 17520348.68368| val_0_unsup_loss: 2778.15479|  0:00:05s\n",
            "epoch 1  | loss: 3203.97276| val_0_unsup_loss: 2.50209 |  0:00:11s\n",
            "epoch 2  | loss: 23.59687| val_0_unsup_loss: 2.07648 |  0:00:17s\n",
            "epoch 3  | loss: 3.72933 | val_0_unsup_loss: 1.68344 |  0:00:22s\n",
            "epoch 4  | loss: 1.63535 | val_0_unsup_loss: 1.4903  |  0:00:28s\n",
            "epoch 5  | loss: 1.79011 | val_0_unsup_loss: 1.41296 |  0:00:34s\n",
            "epoch 6  | loss: 2.17013 | val_0_unsup_loss: 1.38152 |  0:00:39s\n",
            "epoch 7  | loss: 1.40978 | val_0_unsup_loss: 1.36874 |  0:00:45s\n",
            "epoch 8  | loss: 1.37251 | val_0_unsup_loss: 1.36359 |  0:00:51s\n",
            "epoch 9  | loss: 1.36281 | val_0_unsup_loss: 1.36049 |  0:00:56s\n",
            "epoch 10 | loss: 1.36028 | val_0_unsup_loss: 1.35856 |  0:01:02s\n",
            "epoch 11 | loss: 1.45555 | val_0_unsup_loss: 1.35742 |  0:01:08s\n",
            "epoch 12 | loss: 10.24053| val_0_unsup_loss: 1.35655 |  0:01:13s\n",
            "epoch 13 | loss: 1.35655 | val_0_unsup_loss: 1.35576 |  0:01:19s\n",
            "epoch 14 | loss: 1.35625 | val_0_unsup_loss: 1.35517 |  0:01:25s\n",
            "epoch 15 | loss: 1.35566 | val_0_unsup_loss: 1.35475 |  0:01:31s\n",
            "epoch 16 | loss: 1.35493 | val_0_unsup_loss: 1.35442 |  0:01:36s\n",
            "epoch 17 | loss: 1.35461 | val_0_unsup_loss: 1.35427 |  0:01:42s\n",
            "epoch 18 | loss: 1.35489 | val_0_unsup_loss: 1.35441 |  0:01:48s\n",
            "epoch 19 | loss: 1.35493 | val_0_unsup_loss: 1.35429 |  0:01:54s\n",
            "epoch 20 | loss: 1.35454 | val_0_unsup_loss: 1.35413 |  0:01:59s\n",
            "epoch 21 | loss: 1.35479 | val_0_unsup_loss: 1.354   |  0:02:05s\n",
            "epoch 22 | loss: 1.35423 | val_0_unsup_loss: 1.35391 |  0:02:11s\n",
            "epoch 23 | loss: 1.35469 | val_0_unsup_loss: 1.35385 |  0:02:17s\n",
            "epoch 24 | loss: 1.35432 | val_0_unsup_loss: 1.35381 |  0:02:22s\n",
            "epoch 25 | loss: 1.35434 | val_0_unsup_loss: 1.35378 |  0:02:28s\n",
            "epoch 26 | loss: 1.35409 | val_0_unsup_loss: 1.35377 |  0:02:34s\n",
            "epoch 27 | loss: 1.3543  | val_0_unsup_loss: 1.35376 |  0:02:40s\n",
            "epoch 28 | loss: 1.35437 | val_0_unsup_loss: 1.35376 |  0:02:46s\n",
            "epoch 29 | loss: 1.51876 | val_0_unsup_loss: 1.3541  |  0:02:51s\n",
            "epoch 30 | loss: 1.35423 | val_0_unsup_loss: 1.35387 |  0:02:57s\n",
            "epoch 31 | loss: 1.35444 | val_0_unsup_loss: 1.35379 |  0:03:03s\n",
            "epoch 32 | loss: 1.35423 | val_0_unsup_loss: 1.35377 |  0:03:09s\n",
            "epoch 33 | loss: 1.35407 | val_0_unsup_loss: 1.35376 |  0:03:14s\n",
            "epoch 34 | loss: 1.35486 | val_0_unsup_loss: 1.35376 |  0:03:19s\n",
            "epoch 35 | loss: 1.39947 | val_0_unsup_loss: 1.35383 |  0:03:25s\n",
            "epoch 36 | loss: 1.35524 | val_0_unsup_loss: 1.35407 |  0:03:31s\n",
            "epoch 37 | loss: 2.73298 | val_0_unsup_loss: 1.35499 |  0:03:37s\n",
            "epoch 38 | loss: 1.43594 | val_0_unsup_loss: 1.35502 |  0:03:42s\n",
            "epoch 39 | loss: 2.07697 | val_0_unsup_loss: 1.3542  |  0:03:48s\n",
            "epoch 40 | loss: 1.35769 | val_0_unsup_loss: 1.35402 |  0:03:54s\n",
            "epoch 41 | loss: 1.46204 | val_0_unsup_loss: 1.3544  |  0:03:59s\n",
            "epoch 42 | loss: 1.35476 | val_0_unsup_loss: 1.35394 |  0:04:05s\n",
            "epoch 43 | loss: 1.35406 | val_0_unsup_loss: 1.35381 |  0:04:11s\n",
            "epoch 44 | loss: 1.35427 | val_0_unsup_loss: 1.35378 |  0:04:16s\n",
            "epoch 45 | loss: 1.35415 | val_0_unsup_loss: 1.35377 |  0:04:22s\n",
            "epoch 46 | loss: 1.354   | val_0_unsup_loss: 1.35376 |  0:04:28s\n",
            "epoch 47 | loss: 1.35406 | val_0_unsup_loss: 1.35376 |  0:04:34s\n",
            "epoch 48 | loss: 1.35422 | val_0_unsup_loss: 1.35376 |  0:04:39s\n",
            "epoch 49 | loss: 1.35411 | val_0_unsup_loss: 1.35376 |  0:04:45s\n",
            "epoch 50 | loss: 1.35416 | val_0_unsup_loss: 1.35376 |  0:04:51s\n",
            "epoch 51 | loss: 1.35437 | val_0_unsup_loss: 1.35376 |  0:04:57s\n",
            "epoch 52 | loss: 1.35439 | val_0_unsup_loss: 1.35376 |  0:05:02s\n",
            "epoch 53 | loss: 1.35396 | val_0_unsup_loss: 1.35376 |  0:05:08s\n",
            "epoch 54 | loss: 1.35386 | val_0_unsup_loss: 1.35376 |  0:05:14s\n",
            "epoch 55 | loss: 1.35414 | val_0_unsup_loss: 1.35376 |  0:05:19s\n",
            "epoch 56 | loss: 1.35406 | val_0_unsup_loss: 1.35376 |  0:05:25s\n",
            "epoch 57 | loss: 1.35419 | val_0_unsup_loss: 1.35376 |  0:05:31s\n",
            "epoch 58 | loss: 1.35378 | val_0_unsup_loss: 1.35376 |  0:05:37s\n",
            "epoch 59 | loss: 1.35406 | val_0_unsup_loss: 1.35376 |  0:05:42s\n",
            "epoch 60 | loss: 1.35442 | val_0_unsup_loss: 1.35376 |  0:05:48s\n",
            "epoch 61 | loss: 1.35389 | val_0_unsup_loss: 1.35376 |  0:05:54s\n",
            "epoch 62 | loss: 1.35436 | val_0_unsup_loss: 1.35376 |  0:05:59s\n",
            "epoch 63 | loss: 1.35405 | val_0_unsup_loss: 1.35376 |  0:06:05s\n",
            "epoch 64 | loss: 1.35405 | val_0_unsup_loss: 1.35376 |  0:06:11s\n",
            "epoch 65 | loss: 1.35408 | val_0_unsup_loss: 1.35376 |  0:06:16s\n",
            "epoch 66 | loss: 1.35399 | val_0_unsup_loss: 1.35376 |  0:06:22s\n",
            "epoch 67 | loss: 1.35401 | val_0_unsup_loss: 1.35376 |  0:06:28s\n",
            "epoch 68 | loss: 1.35408 | val_0_unsup_loss: 1.35376 |  0:06:33s\n",
            "epoch 69 | loss: 1.35435 | val_0_unsup_loss: 1.35376 |  0:06:39s\n",
            "epoch 70 | loss: 1.35366 | val_0_unsup_loss: 1.35376 |  0:06:45s\n",
            "epoch 71 | loss: 1.35407 | val_0_unsup_loss: 1.35376 |  0:06:51s\n",
            "epoch 72 | loss: 1.35416 | val_0_unsup_loss: 1.35376 |  0:06:56s\n",
            "epoch 73 | loss: 1.3541  | val_0_unsup_loss: 1.35376 |  0:07:02s\n",
            "epoch 74 | loss: 1.35407 | val_0_unsup_loss: 1.35376 |  0:07:07s\n",
            "\n",
            "Early stopping occurred at epoch 74 with best_epoch = 54 and best_val_0_unsup_loss = 1.35376\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.68873 | train_auc: 0.57592 | valid_auc: 0.57685 |  0:00:08s\n",
            "epoch 1  | loss: 0.63418 | train_auc: 0.67129 | valid_auc: 0.67153 |  0:00:17s\n",
            "epoch 2  | loss: 0.44816 | train_auc: 0.85252 | valid_auc: 0.85417 |  0:00:27s\n",
            "epoch 3  | loss: 0.29022 | train_auc: 0.89685 | valid_auc: 0.89592 |  0:00:36s\n",
            "epoch 4  | loss: 0.22069 | train_auc: 0.93789 | valid_auc: 0.93729 |  0:00:45s\n",
            "epoch 5  | loss: 0.18146 | train_auc: 0.94971 | valid_auc: 0.94887 |  0:00:54s\n",
            "epoch 6  | loss: 0.16436 | train_auc: 0.96326 | valid_auc: 0.96281 |  0:01:03s\n",
            "epoch 7  | loss: 0.15595 | train_auc: 0.96349 | valid_auc: 0.96264 |  0:01:12s\n",
            "epoch 8  | loss: 0.15128 | train_auc: 0.97327 | valid_auc: 0.9726  |  0:01:22s\n",
            "epoch 9  | loss: 0.14701 | train_auc: 0.96801 | valid_auc: 0.96739 |  0:01:31s\n",
            "epoch 10 | loss: 0.14437 | train_auc: 0.97577 | valid_auc: 0.97511 |  0:01:40s\n",
            "epoch 11 | loss: 0.1417  | train_auc: 0.97981 | valid_auc: 0.97941 |  0:01:49s\n",
            "epoch 12 | loss: 0.13816 | train_auc: 0.98001 | valid_auc: 0.97938 |  0:01:58s\n",
            "epoch 13 | loss: 0.135   | train_auc: 0.98268 | valid_auc: 0.98198 |  0:02:07s\n",
            "epoch 14 | loss: 0.13355 | train_auc: 0.98519 | valid_auc: 0.98452 |  0:02:17s\n",
            "epoch 15 | loss: 0.1298  | train_auc: 0.98055 | valid_auc: 0.98015 |  0:02:25s\n",
            "epoch 16 | loss: 0.12997 | train_auc: 0.98258 | valid_auc: 0.98196 |  0:02:34s\n",
            "epoch 17 | loss: 0.12561 | train_auc: 0.98711 | valid_auc: 0.98673 |  0:02:43s\n",
            "epoch 18 | loss: 0.12466 | train_auc: 0.9737  | valid_auc: 0.97299 |  0:02:52s\n",
            "epoch 19 | loss: 0.12124 | train_auc: 0.96486 | valid_auc: 0.96424 |  0:03:01s\n",
            "epoch 20 | loss: 0.1195  | train_auc: 0.96384 | valid_auc: 0.96293 |  0:03:11s\n",
            "epoch 21 | loss: 0.1175  | train_auc: 0.97886 | valid_auc: 0.97834 |  0:03:20s\n",
            "epoch 22 | loss: 0.11328 | train_auc: 0.98297 | valid_auc: 0.98251 |  0:03:29s\n",
            "epoch 23 | loss: 0.1103  | train_auc: 0.98713 | valid_auc: 0.98673 |  0:03:38s\n",
            "epoch 24 | loss: 0.10814 | train_auc: 0.98763 | valid_auc: 0.98703 |  0:03:48s\n",
            "epoch 25 | loss: 0.10924 | train_auc: 0.98792 | valid_auc: 0.98764 |  0:03:57s\n",
            "epoch 26 | loss: 0.10979 | train_auc: 0.98953 | valid_auc: 0.98914 |  0:04:06s\n",
            "epoch 27 | loss: 0.10833 | train_auc: 0.99082 | valid_auc: 0.99063 |  0:04:15s\n",
            "epoch 28 | loss: 0.10661 | train_auc: 0.9925  | valid_auc: 0.9922  |  0:04:24s\n",
            "epoch 29 | loss: 0.10639 | train_auc: 0.99171 | valid_auc: 0.99147 |  0:04:34s\n",
            "epoch 30 | loss: 0.10743 | train_auc: 0.98967 | valid_auc: 0.98944 |  0:04:43s\n",
            "epoch 31 | loss: 0.10921 | train_auc: 0.98853 | valid_auc: 0.98831 |  0:04:52s\n",
            "epoch 32 | loss: 0.11144 | train_auc: 0.98965 | valid_auc: 0.98951 |  0:05:01s\n",
            "epoch 33 | loss: 0.10595 | train_auc: 0.99217 | valid_auc: 0.99199 |  0:05:10s\n",
            "epoch 34 | loss: 0.10211 | train_auc: 0.99323 | valid_auc: 0.99289 |  0:05:19s\n",
            "epoch 35 | loss: 0.10073 | train_auc: 0.99356 | valid_auc: 0.99329 |  0:05:28s\n",
            "epoch 36 | loss: 0.09804 | train_auc: 0.99389 | valid_auc: 0.99366 |  0:05:37s\n",
            "epoch 37 | loss: 0.09783 | train_auc: 0.99345 | valid_auc: 0.99318 |  0:05:46s\n",
            "epoch 38 | loss: 0.09667 | train_auc: 0.99362 | valid_auc: 0.99334 |  0:05:55s\n",
            "epoch 39 | loss: 0.096   | train_auc: 0.99382 | valid_auc: 0.99359 |  0:06:04s\n",
            "epoch 40 | loss: 0.09507 | train_auc: 0.9944  | valid_auc: 0.99413 |  0:06:13s\n",
            "epoch 41 | loss: 0.09466 | train_auc: 0.9944  | valid_auc: 0.99421 |  0:06:23s\n",
            "epoch 42 | loss: 0.09443 | train_auc: 0.99476 | valid_auc: 0.99442 |  0:06:32s\n",
            "epoch 43 | loss: 0.09301 | train_auc: 0.99463 | valid_auc: 0.99434 |  0:06:41s\n",
            "epoch 44 | loss: 0.09216 | train_auc: 0.9948  | valid_auc: 0.99446 |  0:06:51s\n",
            "epoch 45 | loss: 0.09233 | train_auc: 0.99497 | valid_auc: 0.99469 |  0:07:00s\n",
            "epoch 46 | loss: 0.09142 | train_auc: 0.99507 | valid_auc: 0.99472 |  0:07:09s\n",
            "epoch 47 | loss: 0.0907  | train_auc: 0.99505 | valid_auc: 0.99469 |  0:07:18s\n",
            "epoch 48 | loss: 0.09064 | train_auc: 0.99521 | valid_auc: 0.99495 |  0:07:28s\n",
            "epoch 49 | loss: 0.09058 | train_auc: 0.99527 | valid_auc: 0.99488 |  0:07:37s\n",
            "epoch 50 | loss: 0.09039 | train_auc: 0.99532 | valid_auc: 0.99493 |  0:07:46s\n",
            "epoch 51 | loss: 0.08908 | train_auc: 0.99546 | valid_auc: 0.9951  |  0:07:55s\n",
            "epoch 52 | loss: 0.08845 | train_auc: 0.99536 | valid_auc: 0.99497 |  0:08:04s\n",
            "epoch 53 | loss: 0.08849 | train_auc: 0.99547 | valid_auc: 0.99507 |  0:08:13s\n",
            "epoch 54 | loss: 0.08796 | train_auc: 0.99452 | valid_auc: 0.9942  |  0:08:23s\n",
            "epoch 55 | loss: 0.08742 | train_auc: 0.99521 | valid_auc: 0.99482 |  0:08:32s\n",
            "epoch 56 | loss: 0.08739 | train_auc: 0.99559 | valid_auc: 0.99525 |  0:08:41s\n",
            "epoch 57 | loss: 0.08692 | train_auc: 0.99566 | valid_auc: 0.99527 |  0:08:50s\n",
            "epoch 58 | loss: 0.08709 | train_auc: 0.99563 | valid_auc: 0.99523 |  0:09:00s\n",
            "epoch 59 | loss: 0.08631 | train_auc: 0.99557 | valid_auc: 0.99513 |  0:09:09s\n",
            "epoch 60 | loss: 0.08573 | train_auc: 0.99576 | valid_auc: 0.99537 |  0:09:18s\n",
            "epoch 61 | loss: 0.08558 | train_auc: 0.99559 | valid_auc: 0.99517 |  0:09:28s\n",
            "epoch 62 | loss: 0.08588 | train_auc: 0.9957  | valid_auc: 0.99537 |  0:09:37s\n",
            "epoch 63 | loss: 0.08644 | train_auc: 0.99547 | valid_auc: 0.99505 |  0:09:46s\n",
            "epoch 64 | loss: 0.08575 | train_auc: 0.99584 | valid_auc: 0.99549 |  0:09:55s\n",
            "epoch 65 | loss: 0.08451 | train_auc: 0.99578 | valid_auc: 0.99539 |  0:10:04s\n",
            "epoch 66 | loss: 0.08563 | train_auc: 0.99583 | valid_auc: 0.99543 |  0:10:13s\n",
            "epoch 67 | loss: 0.08502 | train_auc: 0.99573 | valid_auc: 0.99536 |  0:10:22s\n",
            "epoch 68 | loss: 0.08517 | train_auc: 0.99537 | valid_auc: 0.99499 |  0:10:31s\n",
            "epoch 69 | loss: 0.0843  | train_auc: 0.9957  | valid_auc: 0.99523 |  0:10:40s\n",
            "epoch 70 | loss: 0.08416 | train_auc: 0.99552 | valid_auc: 0.99511 |  0:10:49s\n",
            "epoch 71 | loss: 0.08398 | train_auc: 0.99545 | valid_auc: 0.99504 |  0:10:58s\n",
            "epoch 72 | loss: 0.0846  | train_auc: 0.99555 | valid_auc: 0.99515 |  0:11:08s\n",
            "epoch 73 | loss: 0.08377 | train_auc: 0.99549 | valid_auc: 0.99509 |  0:11:17s\n",
            "epoch 74 | loss: 0.08405 | train_auc: 0.99545 | valid_auc: 0.99506 |  0:11:26s\n",
            "epoch 75 | loss: 0.08357 | train_auc: 0.99547 | valid_auc: 0.99512 |  0:11:35s\n",
            "epoch 76 | loss: 0.08208 | train_auc: 0.99562 | valid_auc: 0.99525 |  0:11:44s\n",
            "epoch 77 | loss: 0.08161 | train_auc: 0.99582 | valid_auc: 0.99538 |  0:11:54s\n",
            "epoch 78 | loss: 0.08242 | train_auc: 0.99557 | valid_auc: 0.99518 |  0:12:03s\n",
            "epoch 79 | loss: 0.08227 | train_auc: 0.99553 | valid_auc: 0.99511 |  0:12:12s\n",
            "epoch 80 | loss: 0.08258 | train_auc: 0.99568 | valid_auc: 0.99528 |  0:12:21s\n",
            "epoch 81 | loss: 0.08229 | train_auc: 0.99577 | valid_auc: 0.99542 |  0:12:30s\n",
            "epoch 82 | loss: 0.08153 | train_auc: 0.99599 | valid_auc: 0.9956  |  0:12:39s\n",
            "epoch 83 | loss: 0.0818  | train_auc: 0.99589 | valid_auc: 0.99549 |  0:12:48s\n",
            "epoch 84 | loss: 0.08258 | train_auc: 0.99577 | valid_auc: 0.99537 |  0:12:57s\n",
            "epoch 85 | loss: 0.08098 | train_auc: 0.99623 | valid_auc: 0.9958  |  0:13:06s\n",
            "epoch 86 | loss: 0.08054 | train_auc: 0.99616 | valid_auc: 0.99575 |  0:13:15s\n",
            "epoch 87 | loss: 0.08102 | train_auc: 0.99626 | valid_auc: 0.99581 |  0:13:24s\n",
            "epoch 88 | loss: 0.08097 | train_auc: 0.99612 | valid_auc: 0.99568 |  0:13:33s\n",
            "epoch 89 | loss: 0.08054 | train_auc: 0.99624 | valid_auc: 0.99582 |  0:13:43s\n",
            "epoch 90 | loss: 0.08028 | train_auc: 0.99632 | valid_auc: 0.9959  |  0:13:52s\n",
            "epoch 91 | loss: 0.08086 | train_auc: 0.99628 | valid_auc: 0.99584 |  0:14:01s\n",
            "epoch 92 | loss: 0.07996 | train_auc: 0.99635 | valid_auc: 0.99591 |  0:14:11s\n",
            "epoch 93 | loss: 0.07971 | train_auc: 0.99622 | valid_auc: 0.99581 |  0:14:20s\n",
            "epoch 94 | loss: 0.0797  | train_auc: 0.9962  | valid_auc: 0.99575 |  0:14:29s\n",
            "epoch 95 | loss: 0.08016 | train_auc: 0.99586 | valid_auc: 0.99544 |  0:14:38s\n",
            "epoch 96 | loss: 0.07992 | train_auc: 0.99632 | valid_auc: 0.99589 |  0:14:47s\n",
            "epoch 97 | loss: 0.07976 | train_auc: 0.99626 | valid_auc: 0.99579 |  0:14:56s\n",
            "epoch 98 | loss: 0.0794  | train_auc: 0.99623 | valid_auc: 0.99581 |  0:15:05s\n",
            "epoch 99 | loss: 0.07906 | train_auc: 0.99634 | valid_auc: 0.99591 |  0:15:15s\n",
            "epoch 100| loss: 0.07872 | train_auc: 0.99644 | valid_auc: 0.99599 |  0:15:24s\n",
            "epoch 101| loss: 0.07935 | train_auc: 0.99648 | valid_auc: 0.99605 |  0:15:33s\n",
            "epoch 102| loss: 0.07855 | train_auc: 0.99644 | valid_auc: 0.99596 |  0:15:42s\n",
            "epoch 103| loss: 0.07909 | train_auc: 0.99636 | valid_auc: 0.9959  |  0:15:51s\n",
            "epoch 104| loss: 0.0788  | train_auc: 0.99618 | valid_auc: 0.99574 |  0:16:00s\n",
            "epoch 105| loss: 0.08069 | train_auc: 0.99642 | valid_auc: 0.99603 |  0:16:09s\n",
            "epoch 106| loss: 0.07989 | train_auc: 0.99623 | valid_auc: 0.99575 |  0:16:19s\n",
            "epoch 107| loss: 0.08026 | train_auc: 0.99622 | valid_auc: 0.99576 |  0:16:28s\n",
            "epoch 108| loss: 0.07977 | train_auc: 0.99633 | valid_auc: 0.99586 |  0:16:37s\n",
            "epoch 109| loss: 0.07907 | train_auc: 0.99636 | valid_auc: 0.99588 |  0:16:46s\n",
            "epoch 110| loss: 0.07833 | train_auc: 0.9964  | valid_auc: 0.99591 |  0:16:55s\n",
            "epoch 111| loss: 0.07839 | train_auc: 0.99638 | valid_auc: 0.99587 |  0:17:05s\n",
            "epoch 112| loss: 0.07831 | train_auc: 0.99644 | valid_auc: 0.996   |  0:17:14s\n",
            "epoch 113| loss: 0.07793 | train_auc: 0.99645 | valid_auc: 0.99596 |  0:17:23s\n",
            "epoch 114| loss: 0.07762 | train_auc: 0.99661 | valid_auc: 0.99615 |  0:17:32s\n",
            "epoch 115| loss: 0.07713 | train_auc: 0.99658 | valid_auc: 0.99609 |  0:17:41s\n",
            "epoch 116| loss: 0.07707 | train_auc: 0.99658 | valid_auc: 0.99608 |  0:17:50s\n",
            "epoch 117| loss: 0.07662 | train_auc: 0.99667 | valid_auc: 0.9962  |  0:17:59s\n",
            "epoch 118| loss: 0.07703 | train_auc: 0.99666 | valid_auc: 0.99618 |  0:18:09s\n",
            "epoch 119| loss: 0.07715 | train_auc: 0.99663 | valid_auc: 0.99615 |  0:18:18s\n",
            "epoch 120| loss: 0.07713 | train_auc: 0.99659 | valid_auc: 0.99613 |  0:18:27s\n",
            "epoch 121| loss: 0.07698 | train_auc: 0.99635 | valid_auc: 0.99582 |  0:18:36s\n",
            "epoch 122| loss: 0.07706 | train_auc: 0.99667 | valid_auc: 0.99623 |  0:18:45s\n",
            "epoch 123| loss: 0.07631 | train_auc: 0.99643 | valid_auc: 0.99596 |  0:18:54s\n",
            "epoch 124| loss: 0.07624 | train_auc: 0.99652 | valid_auc: 0.99602 |  0:19:04s\n",
            "epoch 125| loss: 0.0766  | train_auc: 0.99669 | valid_auc: 0.99622 |  0:19:13s\n",
            "epoch 126| loss: 0.07549 | train_auc: 0.99667 | valid_auc: 0.99621 |  0:19:22s\n",
            "epoch 127| loss: 0.07566 | train_auc: 0.99672 | valid_auc: 0.99623 |  0:19:31s\n",
            "epoch 128| loss: 0.07567 | train_auc: 0.99644 | valid_auc: 0.99596 |  0:19:41s\n",
            "epoch 129| loss: 0.07661 | train_auc: 0.99656 | valid_auc: 0.99612 |  0:19:50s\n",
            "epoch 130| loss: 0.07642 | train_auc: 0.99671 | valid_auc: 0.99626 |  0:19:59s\n",
            "epoch 131| loss: 0.07555 | train_auc: 0.99669 | valid_auc: 0.9962  |  0:20:08s\n",
            "epoch 132| loss: 0.07514 | train_auc: 0.99682 | valid_auc: 0.99635 |  0:20:17s\n",
            "epoch 133| loss: 0.07469 | train_auc: 0.99674 | valid_auc: 0.99625 |  0:20:27s\n",
            "epoch 134| loss: 0.07516 | train_auc: 0.99672 | valid_auc: 0.99622 |  0:20:36s\n",
            "epoch 135| loss: 0.07583 | train_auc: 0.99671 | valid_auc: 0.99625 |  0:20:45s\n",
            "epoch 136| loss: 0.07579 | train_auc: 0.99675 | valid_auc: 0.99624 |  0:20:54s\n",
            "epoch 137| loss: 0.07501 | train_auc: 0.99683 | valid_auc: 0.99637 |  0:21:03s\n",
            "epoch 138| loss: 0.07572 | train_auc: 0.99672 | valid_auc: 0.99623 |  0:21:12s\n",
            "epoch 139| loss: 0.07625 | train_auc: 0.99669 | valid_auc: 0.99614 |  0:21:21s\n",
            "epoch 140| loss: 0.07608 | train_auc: 0.99561 | valid_auc: 0.99512 |  0:21:30s\n",
            "epoch 141| loss: 0.07555 | train_auc: 0.99634 | valid_auc: 0.99583 |  0:21:40s\n",
            "epoch 142| loss: 0.07448 | train_auc: 0.99659 | valid_auc: 0.9961  |  0:21:49s\n",
            "epoch 143| loss: 0.07529 | train_auc: 0.99684 | valid_auc: 0.9963  |  0:21:58s\n",
            "epoch 144| loss: 0.07495 | train_auc: 0.99671 | valid_auc: 0.99615 |  0:22:07s\n",
            "epoch 145| loss: 0.07477 | train_auc: 0.99681 | valid_auc: 0.99623 |  0:22:17s\n",
            "epoch 146| loss: 0.07523 | train_auc: 0.99664 | valid_auc: 0.99609 |  0:22:26s\n",
            "epoch 147| loss: 0.07502 | train_auc: 0.99672 | valid_auc: 0.99626 |  0:22:35s\n",
            "epoch 148| loss: 0.07542 | train_auc: 0.9966  | valid_auc: 0.99607 |  0:22:44s\n",
            "epoch 149| loss: 0.07527 | train_auc: 0.99677 | valid_auc: 0.99627 |  0:22:53s\n",
            "epoch 150| loss: 0.07434 | train_auc: 0.99681 | valid_auc: 0.99632 |  0:23:02s\n",
            "epoch 151| loss: 0.07386 | train_auc: 0.9968  | valid_auc: 0.99628 |  0:23:11s\n",
            "epoch 152| loss: 0.07374 | train_auc: 0.99687 | valid_auc: 0.99631 |  0:23:20s\n",
            "epoch 153| loss: 0.07346 | train_auc: 0.99686 | valid_auc: 0.99632 |  0:23:29s\n",
            "epoch 154| loss: 0.07364 | train_auc: 0.99689 | valid_auc: 0.9964  |  0:23:38s\n",
            "epoch 155| loss: 0.07408 | train_auc: 0.99668 | valid_auc: 0.99618 |  0:23:48s\n",
            "epoch 156| loss: 0.07442 | train_auc: 0.99677 | valid_auc: 0.99627 |  0:23:57s\n",
            "epoch 157| loss: 0.07413 | train_auc: 0.9969  | valid_auc: 0.99636 |  0:24:06s\n",
            "epoch 158| loss: 0.07411 | train_auc: 0.99686 | valid_auc: 0.99634 |  0:24:15s\n",
            "epoch 159| loss: 0.07387 | train_auc: 0.99694 | valid_auc: 0.99638 |  0:24:25s\n",
            "epoch 160| loss: 0.07366 | train_auc: 0.99692 | valid_auc: 0.9964  |  0:24:34s\n",
            "epoch 161| loss: 0.07305 | train_auc: 0.99686 | valid_auc: 0.99637 |  0:24:43s\n",
            "epoch 162| loss: 0.07349 | train_auc: 0.99693 | valid_auc: 0.99639 |  0:24:53s\n",
            "epoch 163| loss: 0.07336 | train_auc: 0.99694 | valid_auc: 0.99643 |  0:25:02s\n",
            "epoch 164| loss: 0.07349 | train_auc: 0.99694 | valid_auc: 0.99645 |  0:25:11s\n",
            "epoch 165| loss: 0.07366 | train_auc: 0.99693 | valid_auc: 0.99644 |  0:25:21s\n",
            "epoch 166| loss: 0.07342 | train_auc: 0.99696 | valid_auc: 0.99641 |  0:25:30s\n",
            "epoch 167| loss: 0.07269 | train_auc: 0.99698 | valid_auc: 0.99647 |  0:25:39s\n",
            "epoch 168| loss: 0.07258 | train_auc: 0.99704 | valid_auc: 0.99657 |  0:25:48s\n",
            "epoch 169| loss: 0.07362 | train_auc: 0.99696 | valid_auc: 0.99645 |  0:25:57s\n",
            "epoch 170| loss: 0.07342 | train_auc: 0.99699 | valid_auc: 0.99648 |  0:26:06s\n",
            "epoch 171| loss: 0.07327 | train_auc: 0.99702 | valid_auc: 0.99651 |  0:26:15s\n",
            "epoch 172| loss: 0.07274 | train_auc: 0.99702 | valid_auc: 0.9965  |  0:26:24s\n",
            "epoch 173| loss: 0.07302 | train_auc: 0.99694 | valid_auc: 0.9964  |  0:26:33s\n",
            "epoch 174| loss: 0.07293 | train_auc: 0.99702 | valid_auc: 0.9965  |  0:26:43s\n",
            "epoch 175| loss: 0.07244 | train_auc: 0.997   | valid_auc: 0.99647 |  0:26:52s\n",
            "epoch 176| loss: 0.07297 | train_auc: 0.99697 | valid_auc: 0.99647 |  0:27:02s\n",
            "epoch 177| loss: 0.07289 | train_auc: 0.99705 | valid_auc: 0.99652 |  0:27:11s\n",
            "epoch 178| loss: 0.07287 | train_auc: 0.99706 | valid_auc: 0.99653 |  0:27:20s\n",
            "epoch 179| loss: 0.07268 | train_auc: 0.99697 | valid_auc: 0.99646 |  0:27:30s\n",
            "epoch 180| loss: 0.07211 | train_auc: 0.99705 | valid_auc: 0.99658 |  0:27:39s\n",
            "epoch 181| loss: 0.07199 | train_auc: 0.99707 | valid_auc: 0.99655 |  0:27:48s\n",
            "epoch 182| loss: 0.07191 | train_auc: 0.99705 | valid_auc: 0.99652 |  0:27:57s\n",
            "epoch 183| loss: 0.07124 | train_auc: 0.99709 | valid_auc: 0.99657 |  0:28:06s\n",
            "epoch 184| loss: 0.07219 | train_auc: 0.99707 | valid_auc: 0.99658 |  0:28:15s\n",
            "epoch 185| loss: 0.07218 | train_auc: 0.99697 | valid_auc: 0.99648 |  0:28:24s\n",
            "epoch 186| loss: 0.07241 | train_auc: 0.99696 | valid_auc: 0.99646 |  0:28:33s\n",
            "epoch 187| loss: 0.07285 | train_auc: 0.99697 | valid_auc: 0.99644 |  0:28:43s\n",
            "epoch 188| loss: 0.07202 | train_auc: 0.99698 | valid_auc: 0.99643 |  0:28:52s\n",
            "epoch 189| loss: 0.07237 | train_auc: 0.99714 | valid_auc: 0.99662 |  0:29:01s\n",
            "epoch 190| loss: 0.07186 | train_auc: 0.99706 | valid_auc: 0.99653 |  0:29:10s\n",
            "epoch 191| loss: 0.07206 | train_auc: 0.99706 | valid_auc: 0.99656 |  0:29:20s\n",
            "epoch 192| loss: 0.07176 | train_auc: 0.99706 | valid_auc: 0.99651 |  0:29:29s\n",
            "epoch 193| loss: 0.07169 | train_auc: 0.99709 | valid_auc: 0.99653 |  0:29:39s\n",
            "epoch 194| loss: 0.07168 | train_auc: 0.99714 | valid_auc: 0.99661 |  0:29:48s\n",
            "epoch 195| loss: 0.07152 | train_auc: 0.99713 | valid_auc: 0.99659 |  0:29:57s\n",
            "epoch 196| loss: 0.07115 | train_auc: 0.99713 | valid_auc: 0.99661 |  0:30:07s\n",
            "epoch 197| loss: 0.07105 | train_auc: 0.99711 | valid_auc: 0.99659 |  0:30:16s\n",
            "epoch 198| loss: 0.07107 | train_auc: 0.99705 | valid_auc: 0.9965  |  0:30:25s\n",
            "epoch 199| loss: 0.07071 | train_auc: 0.99716 | valid_auc: 0.99665 |  0:30:34s\n",
            "epoch 200| loss: 0.07099 | train_auc: 0.99715 | valid_auc: 0.99662 |  0:30:43s\n",
            "epoch 201| loss: 0.0716  | train_auc: 0.99708 | valid_auc: 0.99652 |  0:30:52s\n",
            "epoch 202| loss: 0.07115 | train_auc: 0.99715 | valid_auc: 0.99663 |  0:31:01s\n",
            "epoch 203| loss: 0.07075 | train_auc: 0.99718 | valid_auc: 0.99665 |  0:31:10s\n",
            "epoch 204| loss: 0.07095 | train_auc: 0.99712 | valid_auc: 0.99657 |  0:31:19s\n",
            "epoch 205| loss: 0.07118 | train_auc: 0.99707 | valid_auc: 0.99655 |  0:31:29s\n",
            "epoch 206| loss: 0.0706  | train_auc: 0.99715 | valid_auc: 0.99667 |  0:31:38s\n",
            "epoch 207| loss: 0.07093 | train_auc: 0.99715 | valid_auc: 0.99662 |  0:31:48s\n",
            "epoch 208| loss: 0.07056 | train_auc: 0.99715 | valid_auc: 0.99661 |  0:31:57s\n",
            "epoch 209| loss: 0.07087 | train_auc: 0.9972  | valid_auc: 0.99668 |  0:32:07s\n",
            "epoch 210| loss: 0.07118 | train_auc: 0.99714 | valid_auc: 0.99661 |  0:32:16s\n",
            "epoch 211| loss: 0.07113 | train_auc: 0.99708 | valid_auc: 0.99654 |  0:32:26s\n",
            "epoch 212| loss: 0.07101 | train_auc: 0.99707 | valid_auc: 0.99656 |  0:32:35s\n",
            "epoch 213| loss: 0.07065 | train_auc: 0.99717 | valid_auc: 0.99665 |  0:32:44s\n",
            "epoch 214| loss: 0.07099 | train_auc: 0.99714 | valid_auc: 0.99664 |  0:32:54s\n",
            "epoch 215| loss: 0.07074 | train_auc: 0.99719 | valid_auc: 0.9967  |  0:33:03s\n",
            "epoch 216| loss: 0.07077 | train_auc: 0.99718 | valid_auc: 0.99666 |  0:33:12s\n",
            "epoch 217| loss: 0.07061 | train_auc: 0.99719 | valid_auc: 0.99667 |  0:33:21s\n",
            "epoch 218| loss: 0.07038 | train_auc: 0.99717 | valid_auc: 0.99665 |  0:33:30s\n",
            "epoch 219| loss: 0.07022 | train_auc: 0.99721 | valid_auc: 0.99667 |  0:33:39s\n",
            "epoch 220| loss: 0.07013 | train_auc: 0.99726 | valid_auc: 0.99674 |  0:33:48s\n",
            "epoch 221| loss: 0.07028 | train_auc: 0.99717 | valid_auc: 0.99662 |  0:33:57s\n",
            "epoch 222| loss: 0.07064 | train_auc: 0.99718 | valid_auc: 0.99665 |  0:34:07s\n",
            "epoch 223| loss: 0.07    | train_auc: 0.99721 | valid_auc: 0.99671 |  0:34:16s\n",
            "epoch 224| loss: 0.0702  | train_auc: 0.99724 | valid_auc: 0.9967  |  0:34:25s\n",
            "epoch 225| loss: 0.07041 | train_auc: 0.99725 | valid_auc: 0.99672 |  0:34:34s\n",
            "epoch 226| loss: 0.07022 | train_auc: 0.99724 | valid_auc: 0.99671 |  0:34:43s\n",
            "epoch 227| loss: 0.06971 | train_auc: 0.99722 | valid_auc: 0.99669 |  0:34:52s\n",
            "epoch 228| loss: 0.07005 | train_auc: 0.99726 | valid_auc: 0.99675 |  0:35:02s\n",
            "epoch 229| loss: 0.06983 | train_auc: 0.99722 | valid_auc: 0.99671 |  0:35:11s\n",
            "epoch 230| loss: 0.07016 | train_auc: 0.99723 | valid_auc: 0.99667 |  0:35:20s\n",
            "epoch 231| loss: 0.0697  | train_auc: 0.99721 | valid_auc: 0.99667 |  0:35:29s\n",
            "epoch 232| loss: 0.07006 | train_auc: 0.99717 | valid_auc: 0.9966  |  0:35:38s\n",
            "epoch 233| loss: 0.06976 | train_auc: 0.99728 | valid_auc: 0.99677 |  0:35:47s\n",
            "epoch 234| loss: 0.06944 | train_auc: 0.99727 | valid_auc: 0.99672 |  0:35:57s\n",
            "epoch 235| loss: 0.06978 | train_auc: 0.99722 | valid_auc: 0.99666 |  0:36:06s\n",
            "epoch 236| loss: 0.07054 | train_auc: 0.99715 | valid_auc: 0.99661 |  0:36:15s\n",
            "epoch 237| loss: 0.07008 | train_auc: 0.99724 | valid_auc: 0.99666 |  0:36:24s\n",
            "epoch 238| loss: 0.06964 | train_auc: 0.99722 | valid_auc: 0.9967  |  0:36:34s\n",
            "epoch 239| loss: 0.0695  | train_auc: 0.99724 | valid_auc: 0.99671 |  0:36:43s\n",
            "epoch 240| loss: 0.06956 | train_auc: 0.99729 | valid_auc: 0.99675 |  0:36:53s\n",
            "epoch 241| loss: 0.06928 | train_auc: 0.99732 | valid_auc: 0.99679 |  0:37:02s\n",
            "epoch 242| loss: 0.06918 | train_auc: 0.99728 | valid_auc: 0.99674 |  0:37:11s\n",
            "epoch 243| loss: 0.0697  | train_auc: 0.99727 | valid_auc: 0.99674 |  0:37:21s\n",
            "epoch 244| loss: 0.06949 | train_auc: 0.99726 | valid_auc: 0.9967  |  0:37:30s\n",
            "epoch 245| loss: 0.06977 | train_auc: 0.99729 | valid_auc: 0.99674 |  0:37:39s\n",
            "epoch 246| loss: 0.06924 | train_auc: 0.99729 | valid_auc: 0.99675 |  0:37:48s\n",
            "epoch 247| loss: 0.06975 | train_auc: 0.99726 | valid_auc: 0.99674 |  0:37:57s\n",
            "epoch 248| loss: 0.06929 | train_auc: 0.99728 | valid_auc: 0.99675 |  0:38:06s\n",
            "epoch 249| loss: 0.06899 | train_auc: 0.99732 | valid_auc: 0.99679 |  0:38:15s\n",
            "epoch 250| loss: 0.06903 | train_auc: 0.9973  | valid_auc: 0.99673 |  0:38:24s\n",
            "epoch 251| loss: 0.06926 | train_auc: 0.99728 | valid_auc: 0.99674 |  0:38:33s\n",
            "epoch 252| loss: 0.06965 | train_auc: 0.99722 | valid_auc: 0.99669 |  0:38:42s\n",
            "epoch 253| loss: 0.06965 | train_auc: 0.99724 | valid_auc: 0.99668 |  0:38:52s\n",
            "epoch 254| loss: 0.06921 | train_auc: 0.99731 | valid_auc: 0.99676 |  0:39:01s\n",
            "epoch 255| loss: 0.06956 | train_auc: 0.99726 | valid_auc: 0.9967  |  0:39:10s\n",
            "epoch 256| loss: 0.06979 | train_auc: 0.99716 | valid_auc: 0.99664 |  0:39:19s\n",
            "epoch 257| loss: 0.06977 | train_auc: 0.99726 | valid_auc: 0.99676 |  0:39:28s\n",
            "epoch 258| loss: 0.06956 | train_auc: 0.9973  | valid_auc: 0.99676 |  0:39:38s\n",
            "epoch 259| loss: 0.06888 | train_auc: 0.99725 | valid_auc: 0.99669 |  0:39:47s\n",
            "epoch 260| loss: 0.06887 | train_auc: 0.99735 | valid_auc: 0.99683 |  0:39:56s\n",
            "epoch 261| loss: 0.06904 | train_auc: 0.99733 | valid_auc: 0.9968  |  0:40:05s\n",
            "epoch 262| loss: 0.06896 | train_auc: 0.99731 | valid_auc: 0.99674 |  0:40:15s\n",
            "epoch 263| loss: 0.06895 | train_auc: 0.99733 | valid_auc: 0.99679 |  0:40:24s\n",
            "epoch 264| loss: 0.06827 | train_auc: 0.99735 | valid_auc: 0.99678 |  0:40:33s\n",
            "epoch 265| loss: 0.0685  | train_auc: 0.99732 | valid_auc: 0.99677 |  0:40:43s\n",
            "epoch 266| loss: 0.06877 | train_auc: 0.99734 | valid_auc: 0.99678 |  0:40:52s\n",
            "epoch 267| loss: 0.06875 | train_auc: 0.99733 | valid_auc: 0.99679 |  0:41:01s\n",
            "epoch 268| loss: 0.06857 | train_auc: 0.99728 | valid_auc: 0.99674 |  0:41:10s\n",
            "epoch 269| loss: 0.06833 | train_auc: 0.99733 | valid_auc: 0.99678 |  0:41:19s\n",
            "epoch 270| loss: 0.06839 | train_auc: 0.99734 | valid_auc: 0.99679 |  0:41:28s\n",
            "epoch 271| loss: 0.06853 | train_auc: 0.99736 | valid_auc: 0.99679 |  0:41:37s\n",
            "epoch 272| loss: 0.06819 | train_auc: 0.99736 | valid_auc: 0.99682 |  0:41:46s\n",
            "epoch 273| loss: 0.06852 | train_auc: 0.99737 | valid_auc: 0.9968  |  0:41:55s\n",
            "epoch 274| loss: 0.06863 | train_auc: 0.99736 | valid_auc: 0.99681 |  0:42:04s\n",
            "epoch 275| loss: 0.06843 | train_auc: 0.99738 | valid_auc: 0.99679 |  0:42:13s\n",
            "epoch 276| loss: 0.06818 | train_auc: 0.99737 | valid_auc: 0.99679 |  0:42:23s\n",
            "epoch 277| loss: 0.06891 | train_auc: 0.99725 | valid_auc: 0.99669 |  0:42:32s\n",
            "epoch 278| loss: 0.06926 | train_auc: 0.99733 | valid_auc: 0.99678 |  0:42:42s\n",
            "epoch 279| loss: 0.06872 | train_auc: 0.9974  | valid_auc: 0.99683 |  0:42:51s\n",
            "epoch 280| loss: 0.06883 | train_auc: 0.99737 | valid_auc: 0.99681 |  0:43:00s\n",
            "epoch 281| loss: 0.06844 | train_auc: 0.99736 | valid_auc: 0.99681 |  0:43:09s\n",
            "epoch 282| loss: 0.06828 | train_auc: 0.99735 | valid_auc: 0.99675 |  0:43:19s\n",
            "epoch 283| loss: 0.06835 | train_auc: 0.99734 | valid_auc: 0.99678 |  0:43:28s\n",
            "epoch 284| loss: 0.06839 | train_auc: 0.99739 | valid_auc: 0.99681 |  0:43:37s\n",
            "epoch 285| loss: 0.06839 | train_auc: 0.99739 | valid_auc: 0.99683 |  0:43:47s\n",
            "epoch 286| loss: 0.06824 | train_auc: 0.99738 | valid_auc: 0.99681 |  0:43:56s\n",
            "epoch 287| loss: 0.06822 | train_auc: 0.99737 | valid_auc: 0.99683 |  0:44:06s\n",
            "epoch 288| loss: 0.06819 | train_auc: 0.99741 | valid_auc: 0.99684 |  0:44:15s\n",
            "epoch 289| loss: 0.06791 | train_auc: 0.99741 | valid_auc: 0.99685 |  0:44:24s\n",
            "epoch 290| loss: 0.06803 | train_auc: 0.99741 | valid_auc: 0.99686 |  0:44:33s\n",
            "epoch 291| loss: 0.06836 | train_auc: 0.99739 | valid_auc: 0.99682 |  0:44:42s\n",
            "epoch 292| loss: 0.06814 | train_auc: 0.99737 | valid_auc: 0.9968  |  0:44:51s\n",
            "epoch 293| loss: 0.06809 | train_auc: 0.99736 | valid_auc: 0.99678 |  0:45:00s\n",
            "epoch 294| loss: 0.06813 | train_auc: 0.99742 | valid_auc: 0.99687 |  0:45:09s\n",
            "epoch 295| loss: 0.06786 | train_auc: 0.99739 | valid_auc: 0.99683 |  0:45:19s\n",
            "epoch 296| loss: 0.06806 | train_auc: 0.99738 | valid_auc: 0.99682 |  0:45:28s\n",
            "epoch 297| loss: 0.06782 | train_auc: 0.99735 | valid_auc: 0.9968  |  0:45:37s\n",
            "epoch 298| loss: 0.06777 | train_auc: 0.99736 | valid_auc: 0.99676 |  0:45:46s\n",
            "epoch 299| loss: 0.0681  | train_auc: 0.99743 | valid_auc: 0.99687 |  0:45:55s\n",
            "epoch 300| loss: 0.06793 | train_auc: 0.9974  | valid_auc: 0.99683 |  0:46:04s\n",
            "epoch 301| loss: 0.06825 | train_auc: 0.9974  | valid_auc: 0.99683 |  0:46:14s\n",
            "epoch 302| loss: 0.06794 | train_auc: 0.99742 | valid_auc: 0.99686 |  0:46:23s\n",
            "epoch 303| loss: 0.0679  | train_auc: 0.99742 | valid_auc: 0.99684 |  0:46:33s\n",
            "epoch 304| loss: 0.06796 | train_auc: 0.99742 | valid_auc: 0.99682 |  0:46:42s\n",
            "epoch 305| loss: 0.06771 | train_auc: 0.99738 | valid_auc: 0.99683 |  0:46:51s\n",
            "epoch 306| loss: 0.06781 | train_auc: 0.99738 | valid_auc: 0.9968  |  0:47:00s\n",
            "epoch 307| loss: 0.06778 | train_auc: 0.99741 | valid_auc: 0.9968  |  0:47:10s\n",
            "epoch 308| loss: 0.06774 | train_auc: 0.99739 | valid_auc: 0.99682 |  0:47:19s\n",
            "epoch 309| loss: 0.0676  | train_auc: 0.99738 | valid_auc: 0.99681 |  0:47:28s\n",
            "epoch 310| loss: 0.0678  | train_auc: 0.99737 | valid_auc: 0.99678 |  0:47:37s\n",
            "epoch 311| loss: 0.06785 | train_auc: 0.9974  | valid_auc: 0.9968  |  0:47:46s\n",
            "epoch 312| loss: 0.06774 | train_auc: 0.99742 | valid_auc: 0.99683 |  0:47:56s\n",
            "epoch 313| loss: 0.06762 | train_auc: 0.99744 | valid_auc: 0.99688 |  0:48:05s\n",
            "epoch 314| loss: 0.06731 | train_auc: 0.99744 | valid_auc: 0.99687 |  0:48:15s\n",
            "epoch 315| loss: 0.06752 | train_auc: 0.99746 | valid_auc: 0.99687 |  0:48:24s\n",
            "epoch 316| loss: 0.0672  | train_auc: 0.99747 | valid_auc: 0.99688 |  0:48:34s\n",
            "epoch 317| loss: 0.06725 | train_auc: 0.99745 | valid_auc: 0.99688 |  0:48:43s\n",
            "epoch 318| loss: 0.06738 | train_auc: 0.99744 | valid_auc: 0.99687 |  0:48:52s\n",
            "epoch 319| loss: 0.06741 | train_auc: 0.99745 | valid_auc: 0.99685 |  0:49:01s\n",
            "epoch 320| loss: 0.06734 | train_auc: 0.99745 | valid_auc: 0.99686 |  0:49:11s\n",
            "epoch 321| loss: 0.06752 | train_auc: 0.99743 | valid_auc: 0.99687 |  0:49:20s\n",
            "epoch 322| loss: 0.06737 | train_auc: 0.99745 | valid_auc: 0.99686 |  0:49:29s\n",
            "epoch 323| loss: 0.06726 | train_auc: 0.99746 | valid_auc: 0.99687 |  0:49:38s\n",
            "epoch 324| loss: 0.06696 | train_auc: 0.99743 | valid_auc: 0.99681 |  0:49:47s\n",
            "epoch 325| loss: 0.06714 | train_auc: 0.99739 | valid_auc: 0.99675 |  0:49:56s\n",
            "epoch 326| loss: 0.06743 | train_auc: 0.99743 | valid_auc: 0.99683 |  0:50:05s\n",
            "epoch 327| loss: 0.06721 | train_auc: 0.99742 | valid_auc: 0.99682 |  0:50:14s\n",
            "epoch 328| loss: 0.06732 | train_auc: 0.99745 | valid_auc: 0.99686 |  0:50:23s\n",
            "epoch 329| loss: 0.06737 | train_auc: 0.99745 | valid_auc: 0.99686 |  0:50:33s\n",
            "epoch 330| loss: 0.06742 | train_auc: 0.99745 | valid_auc: 0.99684 |  0:50:42s\n",
            "epoch 331| loss: 0.06714 | train_auc: 0.99747 | valid_auc: 0.99689 |  0:50:51s\n",
            "epoch 332| loss: 0.0674  | train_auc: 0.99742 | valid_auc: 0.99682 |  0:51:00s\n",
            "epoch 333| loss: 0.06743 | train_auc: 0.99745 | valid_auc: 0.99682 |  0:51:10s\n",
            "epoch 334| loss: 0.06736 | train_auc: 0.99744 | valid_auc: 0.99682 |  0:51:19s\n",
            "epoch 335| loss: 0.0674  | train_auc: 0.99745 | valid_auc: 0.99683 |  0:51:28s\n",
            "epoch 336| loss: 0.06714 | train_auc: 0.99745 | valid_auc: 0.99686 |  0:51:38s\n",
            "epoch 337| loss: 0.0672  | train_auc: 0.99748 | valid_auc: 0.99686 |  0:51:47s\n",
            "epoch 338| loss: 0.06699 | train_auc: 0.99745 | valid_auc: 0.99684 |  0:51:56s\n",
            "epoch 339| loss: 0.06719 | train_auc: 0.9974  | valid_auc: 0.99682 |  0:52:06s\n",
            "epoch 340| loss: 0.06706 | train_auc: 0.99747 | valid_auc: 0.9969  |  0:52:15s\n",
            "epoch 341| loss: 0.06691 | train_auc: 0.99748 | valid_auc: 0.9969  |  0:52:24s\n",
            "epoch 342| loss: 0.06712 | train_auc: 0.99748 | valid_auc: 0.9969  |  0:52:33s\n",
            "epoch 343| loss: 0.0668  | train_auc: 0.99748 | valid_auc: 0.99686 |  0:52:42s\n",
            "epoch 344| loss: 0.06712 | train_auc: 0.99749 | valid_auc: 0.9969  |  0:52:51s\n",
            "epoch 345| loss: 0.0669  | train_auc: 0.99748 | valid_auc: 0.99689 |  0:53:00s\n",
            "epoch 346| loss: 0.06691 | train_auc: 0.99748 | valid_auc: 0.99686 |  0:53:09s\n",
            "epoch 347| loss: 0.06679 | train_auc: 0.99747 | valid_auc: 0.99686 |  0:53:18s\n",
            "epoch 348| loss: 0.06715 | train_auc: 0.99745 | valid_auc: 0.99686 |  0:53:27s\n",
            "epoch 349| loss: 0.06704 | train_auc: 0.99747 | valid_auc: 0.99686 |  0:53:37s\n",
            "epoch 350| loss: 0.06713 | train_auc: 0.99747 | valid_auc: 0.99686 |  0:53:46s\n",
            "epoch 351| loss: 0.06667 | train_auc: 0.9975  | valid_auc: 0.99688 |  0:53:55s\n",
            "epoch 352| loss: 0.0669  | train_auc: 0.99748 | valid_auc: 0.99686 |  0:54:05s\n",
            "epoch 353| loss: 0.06682 | train_auc: 0.9975  | valid_auc: 0.99691 |  0:54:14s\n",
            "epoch 354| loss: 0.06682 | train_auc: 0.9975  | valid_auc: 0.99689 |  0:54:23s\n",
            "epoch 355| loss: 0.06702 | train_auc: 0.9975  | valid_auc: 0.99689 |  0:54:33s\n",
            "epoch 356| loss: 0.06713 | train_auc: 0.99746 | valid_auc: 0.99687 |  0:54:42s\n",
            "epoch 357| loss: 0.06686 | train_auc: 0.9975  | valid_auc: 0.9969  |  0:54:51s\n",
            "epoch 358| loss: 0.06708 | train_auc: 0.99748 | valid_auc: 0.99689 |  0:55:01s\n",
            "epoch 359| loss: 0.067   | train_auc: 0.99749 | valid_auc: 0.99689 |  0:55:10s\n",
            "epoch 360| loss: 0.06699 | train_auc: 0.9975  | valid_auc: 0.99691 |  0:55:19s\n",
            "epoch 361| loss: 0.06678 | train_auc: 0.99749 | valid_auc: 0.99688 |  0:55:28s\n",
            "epoch 362| loss: 0.06669 | train_auc: 0.9975  | valid_auc: 0.9969  |  0:55:37s\n",
            "epoch 363| loss: 0.06703 | train_auc: 0.9975  | valid_auc: 0.99687 |  0:55:46s\n",
            "epoch 364| loss: 0.06686 | train_auc: 0.9975  | valid_auc: 0.99691 |  0:55:55s\n",
            "epoch 365| loss: 0.06705 | train_auc: 0.99748 | valid_auc: 0.99687 |  0:56:04s\n",
            "epoch 366| loss: 0.06681 | train_auc: 0.99751 | valid_auc: 0.99691 |  0:56:14s\n",
            "epoch 367| loss: 0.0669  | train_auc: 0.99751 | valid_auc: 0.9969  |  0:56:23s\n",
            "epoch 368| loss: 0.06675 | train_auc: 0.99748 | valid_auc: 0.99688 |  0:56:32s\n",
            "epoch 369| loss: 0.06652 | train_auc: 0.99751 | valid_auc: 0.9969  |  0:56:42s\n",
            "epoch 370| loss: 0.06679 | train_auc: 0.99749 | valid_auc: 0.99685 |  0:56:51s\n",
            "epoch 371| loss: 0.0668  | train_auc: 0.99751 | valid_auc: 0.99688 |  0:57:01s\n",
            "epoch 372| loss: 0.06678 | train_auc: 0.9975  | valid_auc: 0.9969  |  0:57:10s\n",
            "epoch 373| loss: 0.06654 | train_auc: 0.99752 | valid_auc: 0.9969  |  0:57:19s\n",
            "epoch 374| loss: 0.06651 | train_auc: 0.9975  | valid_auc: 0.99687 |  0:57:29s\n",
            "epoch 375| loss: 0.06667 | train_auc: 0.99749 | valid_auc: 0.99686 |  0:57:38s\n",
            "epoch 376| loss: 0.06663 | train_auc: 0.99751 | valid_auc: 0.99688 |  0:57:48s\n",
            "epoch 377| loss: 0.06652 | train_auc: 0.99752 | valid_auc: 0.99691 |  0:57:57s\n",
            "epoch 378| loss: 0.0664  | train_auc: 0.99751 | valid_auc: 0.9969  |  0:58:06s\n",
            "epoch 379| loss: 0.06648 | train_auc: 0.99752 | valid_auc: 0.9969  |  0:58:15s\n",
            "epoch 380| loss: 0.06674 | train_auc: 0.99752 | valid_auc: 0.99691 |  0:58:24s\n",
            "epoch 381| loss: 0.06673 | train_auc: 0.99752 | valid_auc: 0.99691 |  0:58:33s\n",
            "epoch 382| loss: 0.06669 | train_auc: 0.99751 | valid_auc: 0.9969  |  0:58:42s\n",
            "epoch 383| loss: 0.06679 | train_auc: 0.99752 | valid_auc: 0.9969  |  0:58:51s\n",
            "epoch 384| loss: 0.06685 | train_auc: 0.99751 | valid_auc: 0.99689 |  0:59:01s\n",
            "\n",
            "Early stopping occurred at epoch 384 with best_epoch = 364 and best_valid_auc = 0.99691\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.996913748392861\n",
            "----- 6 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 17235100.79282| val_0_unsup_loss: 1200.16724|  0:00:05s\n",
            "epoch 1  | loss: 4418.66355| val_0_unsup_loss: 2.66219 |  0:00:11s\n",
            "epoch 2  | loss: 6.2903  | val_0_unsup_loss: 2.06382 |  0:00:17s\n",
            "epoch 3  | loss: 2.05601 | val_0_unsup_loss: 1.71426 |  0:00:23s\n",
            "epoch 4  | loss: 26.61078| val_0_unsup_loss: 1.5547  |  0:00:29s\n",
            "epoch 5  | loss: 2.9932  | val_0_unsup_loss: 1.466   |  0:00:35s\n",
            "epoch 6  | loss: 11.22377| val_0_unsup_loss: 1.41185 |  0:00:40s\n",
            "epoch 7  | loss: 1.5908  | val_0_unsup_loss: 1.37909 |  0:00:46s\n",
            "epoch 8  | loss: 2.50921 | val_0_unsup_loss: 1.36721 |  0:00:52s\n",
            "epoch 9  | loss: 1.36813 | val_0_unsup_loss: 1.36231 |  0:00:58s\n",
            "epoch 10 | loss: 1.55712 | val_0_unsup_loss: 1.35981 |  0:01:04s\n",
            "epoch 11 | loss: 1.36072 | val_0_unsup_loss: 1.35775 |  0:01:10s\n",
            "epoch 12 | loss: 1.35774 | val_0_unsup_loss: 1.35617 |  0:01:16s\n",
            "epoch 13 | loss: 1.3585  | val_0_unsup_loss: 1.35517 |  0:01:21s\n",
            "epoch 14 | loss: 1.36823 | val_0_unsup_loss: 1.3548  |  0:01:27s\n",
            "epoch 15 | loss: 1.57708 | val_0_unsup_loss: 1.35569 |  0:01:33s\n",
            "epoch 16 | loss: 1.35556 | val_0_unsup_loss: 1.3543  |  0:01:39s\n",
            "epoch 17 | loss: 3.29858 | val_0_unsup_loss: 1.35417 |  0:01:45s\n",
            "epoch 18 | loss: 1.35439 | val_0_unsup_loss: 1.35385 |  0:01:51s\n",
            "epoch 19 | loss: 1.35444 | val_0_unsup_loss: 1.35376 |  0:01:57s\n",
            "epoch 20 | loss: 1.63512 | val_0_unsup_loss: 1.35379 |  0:02:03s\n",
            "epoch 21 | loss: 1.35542 | val_0_unsup_loss: 1.35374 |  0:02:08s\n",
            "epoch 22 | loss: 1.59348 | val_0_unsup_loss: 1.35411 |  0:02:14s\n",
            "epoch 23 | loss: 1.736   | val_0_unsup_loss: 1.35396 |  0:02:20s\n",
            "epoch 24 | loss: 2.18052 | val_0_unsup_loss: 1.3541  |  0:02:26s\n",
            "epoch 25 | loss: 1.35455 | val_0_unsup_loss: 1.35374 |  0:02:32s\n",
            "epoch 26 | loss: 1.35441 | val_0_unsup_loss: 1.35366 |  0:02:38s\n",
            "epoch 27 | loss: 1.35451 | val_0_unsup_loss: 1.35363 |  0:02:44s\n",
            "epoch 28 | loss: 1.35439 | val_0_unsup_loss: 1.35362 |  0:02:50s\n",
            "epoch 29 | loss: 1.35411 | val_0_unsup_loss: 1.35362 |  0:02:55s\n",
            "epoch 30 | loss: 1.35407 | val_0_unsup_loss: 1.35361 |  0:03:01s\n",
            "epoch 31 | loss: 1.35441 | val_0_unsup_loss: 1.35364 |  0:03:07s\n",
            "epoch 32 | loss: 2.16616 | val_0_unsup_loss: 1.35462 |  0:03:13s\n",
            "epoch 33 | loss: 2.12655 | val_0_unsup_loss: 1.35402 |  0:03:18s\n",
            "epoch 34 | loss: 1.36142 | val_0_unsup_loss: 1.35387 |  0:03:24s\n",
            "epoch 35 | loss: 1.59803 | val_0_unsup_loss: 1.35393 |  0:03:30s\n",
            "epoch 36 | loss: 1.36807 | val_0_unsup_loss: 1.35465 |  0:03:36s\n",
            "epoch 37 | loss: 1.35741 | val_0_unsup_loss: 1.35411 |  0:03:42s\n",
            "epoch 38 | loss: 1.35473 | val_0_unsup_loss: 1.35386 |  0:03:47s\n",
            "epoch 39 | loss: 1.35432 | val_0_unsup_loss: 1.35369 |  0:03:53s\n",
            "epoch 40 | loss: 1.35437 | val_0_unsup_loss: 1.35363 |  0:03:59s\n",
            "epoch 41 | loss: 1.35407 | val_0_unsup_loss: 1.35362 |  0:04:05s\n",
            "epoch 42 | loss: 1.35431 | val_0_unsup_loss: 1.35362 |  0:04:11s\n",
            "epoch 43 | loss: 1.35408 | val_0_unsup_loss: 1.35361 |  0:04:17s\n",
            "epoch 44 | loss: 1.35423 | val_0_unsup_loss: 1.35361 |  0:04:23s\n",
            "epoch 45 | loss: 1.35429 | val_0_unsup_loss: 1.35361 |  0:04:29s\n",
            "epoch 46 | loss: 1.3539  | val_0_unsup_loss: 1.35361 |  0:04:34s\n",
            "epoch 47 | loss: 1.35434 | val_0_unsup_loss: 1.35361 |  0:04:40s\n",
            "epoch 48 | loss: 1.35418 | val_0_unsup_loss: 1.35361 |  0:04:46s\n",
            "epoch 49 | loss: 1.35441 | val_0_unsup_loss: 1.35361 |  0:04:52s\n",
            "epoch 50 | loss: 1.35476 | val_0_unsup_loss: 1.35361 |  0:04:58s\n",
            "epoch 51 | loss: 1.35426 | val_0_unsup_loss: 1.35361 |  0:05:04s\n",
            "epoch 52 | loss: 1.35461 | val_0_unsup_loss: 1.35361 |  0:05:10s\n",
            "epoch 53 | loss: 1.35402 | val_0_unsup_loss: 1.35361 |  0:05:15s\n",
            "epoch 54 | loss: 1.35436 | val_0_unsup_loss: 1.35361 |  0:05:21s\n",
            "epoch 55 | loss: 1.35413 | val_0_unsup_loss: 1.35361 |  0:05:27s\n",
            "epoch 56 | loss: 1.35432 | val_0_unsup_loss: 1.35361 |  0:05:33s\n",
            "epoch 57 | loss: 1.35441 | val_0_unsup_loss: 1.35361 |  0:05:38s\n",
            "epoch 58 | loss: 1.35403 | val_0_unsup_loss: 1.35361 |  0:05:44s\n",
            "epoch 59 | loss: 1.35405 | val_0_unsup_loss: 1.35361 |  0:05:50s\n",
            "epoch 60 | loss: 1.35419 | val_0_unsup_loss: 1.35361 |  0:05:56s\n",
            "epoch 61 | loss: 1.35421 | val_0_unsup_loss: 1.35361 |  0:06:01s\n",
            "epoch 62 | loss: 1.35429 | val_0_unsup_loss: 1.35361 |  0:06:07s\n",
            "epoch 63 | loss: 1.35448 | val_0_unsup_loss: 1.35361 |  0:06:13s\n",
            "epoch 64 | loss: 1.35425 | val_0_unsup_loss: 1.35361 |  0:06:19s\n",
            "epoch 65 | loss: 1.35417 | val_0_unsup_loss: 1.35361 |  0:06:25s\n",
            "epoch 66 | loss: 1.35401 | val_0_unsup_loss: 1.35361 |  0:06:31s\n",
            "epoch 67 | loss: 1.35442 | val_0_unsup_loss: 1.35361 |  0:06:36s\n",
            "epoch 68 | loss: 1.35405 | val_0_unsup_loss: 1.35361 |  0:06:42s\n",
            "epoch 69 | loss: 1.35438 | val_0_unsup_loss: 1.35361 |  0:06:48s\n",
            "epoch 70 | loss: 1.35395 | val_0_unsup_loss: 1.35361 |  0:06:54s\n",
            "epoch 71 | loss: 1.35388 | val_0_unsup_loss: 1.35361 |  0:07:00s\n",
            "epoch 72 | loss: 1.35445 | val_0_unsup_loss: 1.35361 |  0:07:06s\n",
            "epoch 73 | loss: 1.35409 | val_0_unsup_loss: 1.35361 |  0:07:11s\n",
            "epoch 74 | loss: 1.35406 | val_0_unsup_loss: 1.35361 |  0:07:17s\n",
            "epoch 75 | loss: 1.35424 | val_0_unsup_loss: 1.35361 |  0:07:23s\n",
            "epoch 76 | loss: 1.35427 | val_0_unsup_loss: 1.35361 |  0:07:29s\n",
            "epoch 77 | loss: 1.35419 | val_0_unsup_loss: 1.35361 |  0:07:35s\n",
            "epoch 78 | loss: 1.354   | val_0_unsup_loss: 1.35361 |  0:07:41s\n",
            "epoch 79 | loss: 1.35431 | val_0_unsup_loss: 1.35361 |  0:07:47s\n",
            "epoch 80 | loss: 1.35417 | val_0_unsup_loss: 1.35361 |  0:07:53s\n",
            "epoch 81 | loss: 1.35408 | val_0_unsup_loss: 1.35361 |  0:07:58s\n",
            "\n",
            "Early stopping occurred at epoch 81 with best_epoch = 61 and best_val_0_unsup_loss = 1.35361\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.69318 | train_auc: 0.52209 | valid_auc: 0.51945 |  0:00:09s\n",
            "epoch 1  | loss: 0.66438 | train_auc: 0.72833 | valid_auc: 0.73129 |  0:00:18s\n",
            "epoch 2  | loss: 0.46101 | train_auc: 0.82515 | valid_auc: 0.82436 |  0:00:28s\n",
            "epoch 3  | loss: 0.27928 | train_auc: 0.7108  | valid_auc: 0.71057 |  0:00:37s\n",
            "epoch 4  | loss: 0.21651 | train_auc: 0.71608 | valid_auc: 0.71453 |  0:00:46s\n",
            "epoch 5  | loss: 0.19281 | train_auc: 0.81711 | valid_auc: 0.81486 |  0:00:56s\n",
            "epoch 6  | loss: 0.18216 | train_auc: 0.80532 | valid_auc: 0.80244 |  0:01:05s\n",
            "epoch 7  | loss: 0.17624 | train_auc: 0.86736 | valid_auc: 0.86497 |  0:01:14s\n",
            "epoch 8  | loss: 0.17204 | train_auc: 0.94296 | valid_auc: 0.94094 |  0:01:24s\n",
            "epoch 9  | loss: 0.16959 | train_auc: 0.9549  | valid_auc: 0.953   |  0:01:33s\n",
            "epoch 10 | loss: 0.16541 | train_auc: 0.95249 | valid_auc: 0.95069 |  0:01:42s\n",
            "epoch 11 | loss: 0.15769 | train_auc: 0.96884 | valid_auc: 0.96761 |  0:01:51s\n",
            "epoch 12 | loss: 0.1516  | train_auc: 0.97805 | valid_auc: 0.97713 |  0:02:00s\n",
            "epoch 13 | loss: 0.14628 | train_auc: 0.97928 | valid_auc: 0.97844 |  0:02:10s\n",
            "epoch 14 | loss: 0.14291 | train_auc: 0.98134 | valid_auc: 0.98059 |  0:02:19s\n",
            "epoch 15 | loss: 0.13861 | train_auc: 0.97982 | valid_auc: 0.97912 |  0:02:28s\n",
            "epoch 16 | loss: 0.13193 | train_auc: 0.98642 | valid_auc: 0.98575 |  0:02:38s\n",
            "epoch 17 | loss: 0.12677 | train_auc: 0.98685 | valid_auc: 0.9861  |  0:02:47s\n",
            "epoch 18 | loss: 0.12354 | train_auc: 0.98612 | valid_auc: 0.98531 |  0:02:56s\n",
            "epoch 19 | loss: 0.12084 | train_auc: 0.98849 | valid_auc: 0.98757 |  0:03:06s\n",
            "epoch 20 | loss: 0.11871 | train_auc: 0.9899  | valid_auc: 0.98908 |  0:03:15s\n",
            "epoch 21 | loss: 0.11609 | train_auc: 0.98943 | valid_auc: 0.9886  |  0:03:25s\n",
            "epoch 22 | loss: 0.1159  | train_auc: 0.99038 | valid_auc: 0.98955 |  0:03:34s\n",
            "epoch 23 | loss: 0.11401 | train_auc: 0.99062 | valid_auc: 0.98974 |  0:03:44s\n",
            "epoch 24 | loss: 0.11212 | train_auc: 0.99024 | valid_auc: 0.98937 |  0:03:53s\n",
            "epoch 25 | loss: 0.11062 | train_auc: 0.99095 | valid_auc: 0.99    |  0:04:02s\n",
            "epoch 26 | loss: 0.10943 | train_auc: 0.9861  | valid_auc: 0.98496 |  0:04:11s\n",
            "epoch 27 | loss: 0.10769 | train_auc: 0.99063 | valid_auc: 0.98975 |  0:04:20s\n",
            "epoch 28 | loss: 0.10875 | train_auc: 0.99157 | valid_auc: 0.9908  |  0:04:30s\n",
            "epoch 29 | loss: 0.10764 | train_auc: 0.99154 | valid_auc: 0.99074 |  0:04:39s\n",
            "epoch 30 | loss: 0.10718 | train_auc: 0.99093 | valid_auc: 0.99004 |  0:04:48s\n",
            "epoch 31 | loss: 0.10661 | train_auc: 0.99118 | valid_auc: 0.99025 |  0:04:58s\n",
            "epoch 32 | loss: 0.10607 | train_auc: 0.99153 | valid_auc: 0.9907  |  0:05:07s\n",
            "epoch 33 | loss: 0.10555 | train_auc: 0.99132 | valid_auc: 0.99049 |  0:05:17s\n",
            "epoch 34 | loss: 0.10457 | train_auc: 0.99154 | valid_auc: 0.99065 |  0:05:26s\n",
            "epoch 35 | loss: 0.10419 | train_auc: 0.99178 | valid_auc: 0.99104 |  0:05:35s\n",
            "epoch 36 | loss: 0.10458 | train_auc: 0.99146 | valid_auc: 0.9906  |  0:05:45s\n",
            "epoch 37 | loss: 0.1037  | train_auc: 0.99168 | valid_auc: 0.99087 |  0:05:54s\n",
            "epoch 38 | loss: 0.10274 | train_auc: 0.99182 | valid_auc: 0.99102 |  0:06:04s\n",
            "epoch 39 | loss: 0.10251 | train_auc: 0.992   | valid_auc: 0.99125 |  0:06:13s\n",
            "epoch 40 | loss: 0.10244 | train_auc: 0.99173 | valid_auc: 0.9909  |  0:06:23s\n",
            "epoch 41 | loss: 0.10339 | train_auc: 0.99196 | valid_auc: 0.99109 |  0:06:32s\n",
            "epoch 42 | loss: 0.1031  | train_auc: 0.99165 | valid_auc: 0.99074 |  0:06:41s\n",
            "epoch 43 | loss: 0.10195 | train_auc: 0.99219 | valid_auc: 0.9913  |  0:06:51s\n",
            "epoch 44 | loss: 0.10182 | train_auc: 0.99174 | valid_auc: 0.99083 |  0:07:00s\n",
            "epoch 45 | loss: 0.10056 | train_auc: 0.99214 | valid_auc: 0.99133 |  0:07:10s\n",
            "epoch 46 | loss: 0.10025 | train_auc: 0.99057 | valid_auc: 0.98963 |  0:07:19s\n",
            "epoch 47 | loss: 0.1004  | train_auc: 0.99118 | valid_auc: 0.99016 |  0:07:28s\n",
            "epoch 48 | loss: 0.09998 | train_auc: 0.99253 | valid_auc: 0.99177 |  0:07:37s\n",
            "epoch 49 | loss: 0.09957 | train_auc: 0.99228 | valid_auc: 0.99147 |  0:07:47s\n",
            "epoch 50 | loss: 0.0988  | train_auc: 0.99133 | valid_auc: 0.99068 |  0:07:56s\n",
            "epoch 51 | loss: 0.09892 | train_auc: 0.9919  | valid_auc: 0.99089 |  0:08:05s\n",
            "epoch 52 | loss: 0.09788 | train_auc: 0.99257 | valid_auc: 0.99172 |  0:08:15s\n",
            "epoch 53 | loss: 0.09792 | train_auc: 0.99232 | valid_auc: 0.99152 |  0:08:24s\n",
            "epoch 54 | loss: 0.09854 | train_auc: 0.99292 | valid_auc: 0.99209 |  0:08:34s\n",
            "epoch 55 | loss: 0.09866 | train_auc: 0.99255 | valid_auc: 0.99174 |  0:08:43s\n",
            "epoch 56 | loss: 0.09796 | train_auc: 0.99227 | valid_auc: 0.99158 |  0:08:53s\n",
            "epoch 57 | loss: 0.09869 | train_auc: 0.98833 | valid_auc: 0.98716 |  0:09:02s\n",
            "epoch 58 | loss: 0.09809 | train_auc: 0.98603 | valid_auc: 0.98467 |  0:09:12s\n",
            "epoch 59 | loss: 0.09863 | train_auc: 0.9921  | valid_auc: 0.99099 |  0:09:21s\n",
            "epoch 60 | loss: 0.09874 | train_auc: 0.99271 | valid_auc: 0.99188 |  0:09:30s\n",
            "epoch 61 | loss: 0.09862 | train_auc: 0.99272 | valid_auc: 0.99192 |  0:09:40s\n",
            "epoch 62 | loss: 0.09782 | train_auc: 0.99251 | valid_auc: 0.99168 |  0:09:49s\n",
            "epoch 63 | loss: 0.09727 | train_auc: 0.99288 | valid_auc: 0.9919  |  0:09:58s\n",
            "epoch 64 | loss: 0.09717 | train_auc: 0.99183 | valid_auc: 0.99075 |  0:10:08s\n",
            "epoch 65 | loss: 0.09671 | train_auc: 0.99277 | valid_auc: 0.99184 |  0:10:17s\n",
            "epoch 66 | loss: 0.09545 | train_auc: 0.9929  | valid_auc: 0.99214 |  0:10:26s\n",
            "epoch 67 | loss: 0.09517 | train_auc: 0.99276 | valid_auc: 0.99217 |  0:10:35s\n",
            "epoch 68 | loss: 0.09555 | train_auc: 0.99286 | valid_auc: 0.99214 |  0:10:45s\n",
            "epoch 69 | loss: 0.09866 | train_auc: 0.99296 | valid_auc: 0.99231 |  0:10:54s\n",
            "epoch 70 | loss: 0.09918 | train_auc: 0.99335 | valid_auc: 0.99251 |  0:11:03s\n",
            "epoch 71 | loss: 0.09621 | train_auc: 0.99383 | valid_auc: 0.99313 |  0:11:12s\n",
            "epoch 72 | loss: 0.09448 | train_auc: 0.9941  | valid_auc: 0.99345 |  0:11:22s\n",
            "epoch 73 | loss: 0.09433 | train_auc: 0.99357 | valid_auc: 0.99284 |  0:11:31s\n",
            "epoch 74 | loss: 0.09407 | train_auc: 0.99333 | valid_auc: 0.99267 |  0:11:40s\n",
            "epoch 75 | loss: 0.09244 | train_auc: 0.99292 | valid_auc: 0.99225 |  0:11:50s\n",
            "epoch 76 | loss: 0.08958 | train_auc: 0.99473 | valid_auc: 0.99407 |  0:11:59s\n",
            "epoch 77 | loss: 0.08841 | train_auc: 0.99493 | valid_auc: 0.99421 |  0:12:09s\n",
            "epoch 78 | loss: 0.08581 | train_auc: 0.99531 | valid_auc: 0.99462 |  0:12:18s\n",
            "epoch 79 | loss: 0.08573 | train_auc: 0.99544 | valid_auc: 0.99482 |  0:12:28s\n",
            "epoch 80 | loss: 0.08435 | train_auc: 0.99527 | valid_auc: 0.99468 |  0:12:37s\n",
            "epoch 81 | loss: 0.08446 | train_auc: 0.99577 | valid_auc: 0.99516 |  0:12:47s\n",
            "epoch 82 | loss: 0.08475 | train_auc: 0.99575 | valid_auc: 0.99514 |  0:12:56s\n",
            "epoch 83 | loss: 0.08416 | train_auc: 0.995   | valid_auc: 0.9943  |  0:13:05s\n",
            "epoch 84 | loss: 0.08414 | train_auc: 0.99586 | valid_auc: 0.99525 |  0:13:15s\n",
            "epoch 85 | loss: 0.08267 | train_auc: 0.99564 | valid_auc: 0.99496 |  0:13:24s\n",
            "epoch 86 | loss: 0.08255 | train_auc: 0.99564 | valid_auc: 0.99499 |  0:13:33s\n",
            "epoch 87 | loss: 0.08242 | train_auc: 0.99603 | valid_auc: 0.99539 |  0:13:42s\n",
            "epoch 88 | loss: 0.08261 | train_auc: 0.99603 | valid_auc: 0.99549 |  0:13:51s\n",
            "epoch 89 | loss: 0.0824  | train_auc: 0.99614 | valid_auc: 0.99551 |  0:14:01s\n",
            "epoch 90 | loss: 0.08196 | train_auc: 0.99567 | valid_auc: 0.99499 |  0:14:10s\n",
            "epoch 91 | loss: 0.08133 | train_auc: 0.99599 | valid_auc: 0.99528 |  0:14:19s\n",
            "epoch 92 | loss: 0.0812  | train_auc: 0.99613 | valid_auc: 0.9955  |  0:14:28s\n",
            "epoch 93 | loss: 0.08113 | train_auc: 0.9962  | valid_auc: 0.99557 |  0:14:38s\n",
            "epoch 94 | loss: 0.08135 | train_auc: 0.99611 | valid_auc: 0.99549 |  0:14:47s\n",
            "epoch 95 | loss: 0.0813  | train_auc: 0.99349 | valid_auc: 0.99277 |  0:14:57s\n",
            "epoch 96 | loss: 0.08145 | train_auc: 0.98829 | valid_auc: 0.98751 |  0:15:06s\n",
            "epoch 97 | loss: 0.08017 | train_auc: 0.99489 | valid_auc: 0.99426 |  0:15:16s\n",
            "epoch 98 | loss: 0.07995 | train_auc: 0.99623 | valid_auc: 0.99558 |  0:15:25s\n",
            "epoch 99 | loss: 0.08009 | train_auc: 0.99624 | valid_auc: 0.99561 |  0:15:34s\n",
            "epoch 100| loss: 0.08128 | train_auc: 0.99593 | valid_auc: 0.99528 |  0:15:44s\n",
            "epoch 101| loss: 0.07972 | train_auc: 0.99624 | valid_auc: 0.9956  |  0:15:53s\n",
            "epoch 102| loss: 0.07974 | train_auc: 0.99622 | valid_auc: 0.99563 |  0:16:02s\n",
            "epoch 103| loss: 0.0791  | train_auc: 0.99617 | valid_auc: 0.99557 |  0:16:11s\n",
            "epoch 104| loss: 0.0792  | train_auc: 0.99623 | valid_auc: 0.99562 |  0:16:21s\n",
            "epoch 105| loss: 0.07788 | train_auc: 0.99643 | valid_auc: 0.99575 |  0:16:30s\n",
            "epoch 106| loss: 0.07761 | train_auc: 0.99444 | valid_auc: 0.99378 |  0:16:39s\n",
            "epoch 107| loss: 0.07779 | train_auc: 0.99628 | valid_auc: 0.99562 |  0:16:48s\n",
            "epoch 108| loss: 0.07852 | train_auc: 0.99628 | valid_auc: 0.99562 |  0:16:58s\n",
            "epoch 109| loss: 0.08043 | train_auc: 0.99657 | valid_auc: 0.99584 |  0:17:07s\n",
            "epoch 110| loss: 0.07758 | train_auc: 0.99646 | valid_auc: 0.99574 |  0:17:17s\n",
            "epoch 111| loss: 0.07798 | train_auc: 0.99621 | valid_auc: 0.99558 |  0:17:26s\n",
            "epoch 112| loss: 0.07751 | train_auc: 0.99625 | valid_auc: 0.99556 |  0:17:35s\n",
            "epoch 113| loss: 0.07705 | train_auc: 0.99675 | valid_auc: 0.99611 |  0:17:45s\n",
            "epoch 114| loss: 0.0758  | train_auc: 0.9964  | valid_auc: 0.99575 |  0:17:54s\n",
            "epoch 115| loss: 0.07598 | train_auc: 0.99628 | valid_auc: 0.99557 |  0:18:04s\n",
            "epoch 116| loss: 0.07742 | train_auc: 0.99648 | valid_auc: 0.99581 |  0:18:13s\n",
            "epoch 117| loss: 0.07684 | train_auc: 0.99658 | valid_auc: 0.9959  |  0:18:23s\n",
            "epoch 118| loss: 0.07578 | train_auc: 0.99671 | valid_auc: 0.99606 |  0:18:32s\n",
            "epoch 119| loss: 0.07502 | train_auc: 0.99653 | valid_auc: 0.99588 |  0:18:41s\n",
            "epoch 120| loss: 0.07553 | train_auc: 0.99656 | valid_auc: 0.99596 |  0:18:50s\n",
            "epoch 121| loss: 0.07547 | train_auc: 0.9968  | valid_auc: 0.99615 |  0:19:00s\n",
            "epoch 122| loss: 0.0749  | train_auc: 0.99679 | valid_auc: 0.99611 |  0:19:09s\n",
            "epoch 123| loss: 0.07518 | train_auc: 0.99679 | valid_auc: 0.99613 |  0:19:18s\n",
            "epoch 124| loss: 0.07442 | train_auc: 0.99688 | valid_auc: 0.9962  |  0:19:27s\n",
            "epoch 125| loss: 0.07363 | train_auc: 0.99695 | valid_auc: 0.99629 |  0:19:37s\n",
            "epoch 126| loss: 0.07389 | train_auc: 0.99672 | valid_auc: 0.9961  |  0:19:46s\n",
            "epoch 127| loss: 0.07381 | train_auc: 0.99678 | valid_auc: 0.99616 |  0:19:56s\n",
            "epoch 128| loss: 0.07463 | train_auc: 0.99693 | valid_auc: 0.99626 |  0:20:05s\n",
            "epoch 129| loss: 0.07402 | train_auc: 0.99687 | valid_auc: 0.99621 |  0:20:15s\n",
            "epoch 130| loss: 0.07347 | train_auc: 0.99691 | valid_auc: 0.99624 |  0:20:24s\n",
            "epoch 131| loss: 0.07335 | train_auc: 0.99693 | valid_auc: 0.99633 |  0:20:34s\n",
            "epoch 132| loss: 0.07319 | train_auc: 0.99694 | valid_auc: 0.99635 |  0:20:43s\n",
            "epoch 133| loss: 0.07343 | train_auc: 0.99693 | valid_auc: 0.99632 |  0:20:52s\n",
            "epoch 134| loss: 0.07367 | train_auc: 0.99696 | valid_auc: 0.99633 |  0:21:02s\n",
            "epoch 135| loss: 0.07246 | train_auc: 0.99687 | valid_auc: 0.99618 |  0:21:11s\n",
            "epoch 136| loss: 0.07278 | train_auc: 0.99696 | valid_auc: 0.99634 |  0:21:20s\n",
            "epoch 137| loss: 0.07234 | train_auc: 0.99707 | valid_auc: 0.99643 |  0:21:30s\n",
            "epoch 138| loss: 0.07292 | train_auc: 0.99687 | valid_auc: 0.99627 |  0:21:39s\n",
            "epoch 139| loss: 0.07293 | train_auc: 0.99699 | valid_auc: 0.99635 |  0:21:48s\n",
            "epoch 140| loss: 0.07177 | train_auc: 0.99701 | valid_auc: 0.99634 |  0:21:57s\n",
            "epoch 141| loss: 0.07247 | train_auc: 0.99698 | valid_auc: 0.99634 |  0:22:06s\n",
            "epoch 142| loss: 0.0723  | train_auc: 0.99707 | valid_auc: 0.99645 |  0:22:15s\n",
            "epoch 143| loss: 0.07184 | train_auc: 0.99694 | valid_auc: 0.99633 |  0:22:25s\n",
            "epoch 144| loss: 0.07179 | train_auc: 0.99682 | valid_auc: 0.99619 |  0:22:34s\n",
            "epoch 145| loss: 0.07195 | train_auc: 0.99673 | valid_auc: 0.99605 |  0:22:44s\n",
            "epoch 146| loss: 0.07196 | train_auc: 0.99699 | valid_auc: 0.99627 |  0:22:53s\n",
            "epoch 147| loss: 0.0718  | train_auc: 0.99714 | valid_auc: 0.9965  |  0:23:02s\n",
            "epoch 148| loss: 0.07142 | train_auc: 0.99712 | valid_auc: 0.9965  |  0:23:12s\n",
            "epoch 149| loss: 0.07114 | train_auc: 0.99704 | valid_auc: 0.99637 |  0:23:21s\n",
            "epoch 150| loss: 0.07085 | train_auc: 0.99706 | valid_auc: 0.9964  |  0:23:31s\n",
            "epoch 151| loss: 0.07141 | train_auc: 0.99709 | valid_auc: 0.99646 |  0:23:40s\n",
            "epoch 152| loss: 0.07178 | train_auc: 0.99683 | valid_auc: 0.99619 |  0:23:49s\n",
            "epoch 153| loss: 0.07175 | train_auc: 0.99605 | valid_auc: 0.99539 |  0:23:58s\n",
            "epoch 154| loss: 0.07113 | train_auc: 0.99634 | valid_auc: 0.99568 |  0:24:08s\n",
            "epoch 155| loss: 0.07154 | train_auc: 0.99687 | valid_auc: 0.99623 |  0:24:17s\n",
            "epoch 156| loss: 0.07113 | train_auc: 0.99719 | valid_auc: 0.99658 |  0:24:26s\n",
            "epoch 157| loss: 0.07074 | train_auc: 0.99702 | valid_auc: 0.99637 |  0:24:35s\n",
            "epoch 158| loss: 0.07053 | train_auc: 0.99689 | valid_auc: 0.99623 |  0:24:44s\n",
            "epoch 159| loss: 0.07101 | train_auc: 0.99704 | valid_auc: 0.99643 |  0:24:54s\n",
            "epoch 160| loss: 0.07022 | train_auc: 0.9972  | valid_auc: 0.99659 |  0:25:03s\n",
            "epoch 161| loss: 0.07083 | train_auc: 0.99702 | valid_auc: 0.99643 |  0:25:12s\n",
            "epoch 162| loss: 0.07151 | train_auc: 0.99709 | valid_auc: 0.99644 |  0:25:22s\n",
            "epoch 163| loss: 0.07163 | train_auc: 0.9969  | valid_auc: 0.99626 |  0:25:31s\n",
            "epoch 164| loss: 0.07091 | train_auc: 0.99695 | valid_auc: 0.99636 |  0:25:41s\n",
            "epoch 165| loss: 0.07034 | train_auc: 0.99725 | valid_auc: 0.99663 |  0:25:50s\n",
            "epoch 166| loss: 0.07058 | train_auc: 0.99716 | valid_auc: 0.99655 |  0:26:00s\n",
            "epoch 167| loss: 0.06987 | train_auc: 0.99723 | valid_auc: 0.99658 |  0:26:09s\n",
            "epoch 168| loss: 0.06967 | train_auc: 0.99713 | valid_auc: 0.99645 |  0:26:18s\n",
            "epoch 169| loss: 0.06967 | train_auc: 0.99712 | valid_auc: 0.99648 |  0:26:28s\n",
            "epoch 170| loss: 0.06989 | train_auc: 0.99715 | valid_auc: 0.99654 |  0:26:37s\n",
            "epoch 171| loss: 0.06983 | train_auc: 0.99729 | valid_auc: 0.99667 |  0:26:47s\n",
            "epoch 172| loss: 0.06907 | train_auc: 0.9973  | valid_auc: 0.99665 |  0:26:56s\n",
            "epoch 173| loss: 0.07007 | train_auc: 0.99338 | valid_auc: 0.99205 |  0:27:06s\n",
            "epoch 174| loss: 0.06949 | train_auc: 0.99728 | valid_auc: 0.99665 |  0:27:15s\n",
            "epoch 175| loss: 0.06927 | train_auc: 0.99724 | valid_auc: 0.99659 |  0:27:24s\n",
            "epoch 176| loss: 0.06983 | train_auc: 0.99726 | valid_auc: 0.99662 |  0:27:33s\n",
            "epoch 177| loss: 0.0698  | train_auc: 0.99517 | valid_auc: 0.99431 |  0:27:42s\n",
            "epoch 178| loss: 0.07011 | train_auc: 0.99408 | valid_auc: 0.9932  |  0:27:51s\n",
            "epoch 179| loss: 0.07004 | train_auc: 0.99581 | valid_auc: 0.99504 |  0:28:01s\n",
            "epoch 180| loss: 0.06998 | train_auc: 0.99681 | valid_auc: 0.99616 |  0:28:10s\n",
            "epoch 181| loss: 0.06964 | train_auc: 0.99708 | valid_auc: 0.99634 |  0:28:20s\n",
            "epoch 182| loss: 0.06926 | train_auc: 0.99722 | valid_auc: 0.99652 |  0:28:29s\n",
            "epoch 183| loss: 0.06887 | train_auc: 0.99709 | valid_auc: 0.99644 |  0:28:38s\n",
            "epoch 184| loss: 0.06891 | train_auc: 0.99609 | valid_auc: 0.99538 |  0:28:48s\n",
            "epoch 185| loss: 0.06901 | train_auc: 0.99569 | valid_auc: 0.99492 |  0:28:57s\n",
            "epoch 186| loss: 0.06859 | train_auc: 0.99643 | valid_auc: 0.99565 |  0:29:07s\n",
            "epoch 187| loss: 0.06873 | train_auc: 0.99691 | valid_auc: 0.99615 |  0:29:16s\n",
            "epoch 188| loss: 0.06863 | train_auc: 0.99694 | valid_auc: 0.9963  |  0:29:26s\n",
            "epoch 189| loss: 0.06848 | train_auc: 0.99688 | valid_auc: 0.99618 |  0:29:35s\n",
            "epoch 190| loss: 0.06902 | train_auc: 0.99711 | valid_auc: 0.99639 |  0:29:45s\n",
            "epoch 191| loss: 0.06865 | train_auc: 0.99726 | valid_auc: 0.99655 |  0:29:54s\n",
            "\n",
            "Early stopping occurred at epoch 191 with best_epoch = 171 and best_valid_auc = 0.99667\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9966728433480759\n",
            "----- 7 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 18698903.69738| val_0_unsup_loss: 2533.82983|  0:00:05s\n",
            "epoch 1  | loss: 6627.23001| val_0_unsup_loss: 2.17241 |  0:00:11s\n",
            "epoch 2  | loss: 6.50356 | val_0_unsup_loss: 1.95639 |  0:00:17s\n",
            "epoch 3  | loss: 3.58734 | val_0_unsup_loss: 1.71873 |  0:00:23s\n",
            "epoch 4  | loss: 1.82357 | val_0_unsup_loss: 1.57029 |  0:00:29s\n",
            "epoch 5  | loss: 11.38484| val_0_unsup_loss: 1.47734 |  0:00:35s\n",
            "epoch 6  | loss: 1.99661 | val_0_unsup_loss: 1.41058 |  0:00:41s\n",
            "epoch 7  | loss: 1.41507 | val_0_unsup_loss: 1.3758  |  0:00:47s\n",
            "epoch 8  | loss: 1.51658 | val_0_unsup_loss: 1.36672 |  0:00:53s\n",
            "epoch 9  | loss: 2.11845 | val_0_unsup_loss: 1.36042 |  0:00:59s\n",
            "epoch 10 | loss: 1.7734  | val_0_unsup_loss: 1.35889 |  0:01:05s\n",
            "epoch 11 | loss: 2.66147 | val_0_unsup_loss: 1.35723 |  0:01:11s\n",
            "epoch 12 | loss: 1.37143 | val_0_unsup_loss: 1.35549 |  0:01:17s\n",
            "epoch 13 | loss: 1.396   | val_0_unsup_loss: 1.35418 |  0:01:23s\n",
            "epoch 14 | loss: 1.4085  | val_0_unsup_loss: 1.3537  |  0:01:29s\n",
            "epoch 15 | loss: 1.35511 | val_0_unsup_loss: 1.35336 |  0:01:35s\n",
            "epoch 16 | loss: 1.9194  | val_0_unsup_loss: 1.35323 |  0:01:41s\n",
            "epoch 17 | loss: 1.35436 | val_0_unsup_loss: 1.35314 |  0:01:47s\n",
            "epoch 18 | loss: 1.58009 | val_0_unsup_loss: 1.35351 |  0:01:53s\n",
            "epoch 19 | loss: 1.35474 | val_0_unsup_loss: 1.35344 |  0:01:59s\n",
            "epoch 20 | loss: 2.80339 | val_0_unsup_loss: 1.35312 |  0:02:05s\n",
            "epoch 21 | loss: 1.38892 | val_0_unsup_loss: 1.35302 |  0:02:11s\n",
            "epoch 22 | loss: 1.35404 | val_0_unsup_loss: 1.35297 |  0:02:17s\n",
            "epoch 23 | loss: 1.35413 | val_0_unsup_loss: 1.35296 |  0:02:23s\n",
            "epoch 24 | loss: 1.35414 | val_0_unsup_loss: 1.35295 |  0:02:29s\n",
            "epoch 25 | loss: 1.3544  | val_0_unsup_loss: 1.35295 |  0:02:35s\n",
            "epoch 26 | loss: 1.35396 | val_0_unsup_loss: 1.35294 |  0:02:41s\n",
            "epoch 27 | loss: 1.3543  | val_0_unsup_loss: 1.35294 |  0:02:46s\n",
            "epoch 28 | loss: 1.35451 | val_0_unsup_loss: 1.35294 |  0:02:52s\n",
            "epoch 29 | loss: 1.35436 | val_0_unsup_loss: 1.35294 |  0:02:58s\n",
            "epoch 30 | loss: 1.35403 | val_0_unsup_loss: 1.35294 |  0:03:04s\n",
            "epoch 31 | loss: 1.35425 | val_0_unsup_loss: 1.35294 |  0:03:10s\n",
            "epoch 32 | loss: 1.35421 | val_0_unsup_loss: 1.35294 |  0:03:16s\n",
            "epoch 33 | loss: 1.3544  | val_0_unsup_loss: 1.35294 |  0:03:22s\n",
            "epoch 34 | loss: 1.35429 | val_0_unsup_loss: 1.35294 |  0:03:28s\n",
            "epoch 35 | loss: 1.35424 | val_0_unsup_loss: 1.35294 |  0:03:33s\n",
            "epoch 36 | loss: 1.35435 | val_0_unsup_loss: 1.35294 |  0:03:39s\n",
            "epoch 37 | loss: 1.35435 | val_0_unsup_loss: 1.35294 |  0:03:45s\n",
            "epoch 38 | loss: 1.3543  | val_0_unsup_loss: 1.35294 |  0:03:51s\n",
            "epoch 39 | loss: 1.35451 | val_0_unsup_loss: 1.35294 |  0:03:57s\n",
            "epoch 40 | loss: 1.3542  | val_0_unsup_loss: 1.35294 |  0:04:03s\n",
            "epoch 41 | loss: 1.35417 | val_0_unsup_loss: 1.35294 |  0:04:09s\n",
            "epoch 42 | loss: 1.35422 | val_0_unsup_loss: 1.35294 |  0:04:15s\n",
            "epoch 43 | loss: 1.35426 | val_0_unsup_loss: 1.35294 |  0:04:21s\n",
            "epoch 44 | loss: 1.35412 | val_0_unsup_loss: 1.35294 |  0:04:27s\n",
            "epoch 45 | loss: 1.35423 | val_0_unsup_loss: 1.35294 |  0:04:33s\n",
            "epoch 46 | loss: 1.35414 | val_0_unsup_loss: 1.35294 |  0:04:39s\n",
            "epoch 47 | loss: 1.35386 | val_0_unsup_loss: 1.35294 |  0:04:45s\n",
            "epoch 48 | loss: 1.35423 | val_0_unsup_loss: 1.35294 |  0:04:50s\n",
            "epoch 49 | loss: 1.35437 | val_0_unsup_loss: 1.35294 |  0:04:56s\n",
            "epoch 50 | loss: 1.35458 | val_0_unsup_loss: 1.35294 |  0:05:02s\n",
            "epoch 51 | loss: 1.3543  | val_0_unsup_loss: 1.35294 |  0:05:08s\n",
            "epoch 52 | loss: 1.35442 | val_0_unsup_loss: 1.35294 |  0:05:14s\n",
            "epoch 53 | loss: 1.3541  | val_0_unsup_loss: 1.35294 |  0:05:20s\n",
            "epoch 54 | loss: 1.35444 | val_0_unsup_loss: 1.35294 |  0:05:26s\n",
            "epoch 55 | loss: 1.35443 | val_0_unsup_loss: 1.35294 |  0:05:32s\n",
            "epoch 56 | loss: 1.35429 | val_0_unsup_loss: 1.35294 |  0:05:37s\n",
            "epoch 57 | loss: 1.35438 | val_0_unsup_loss: 1.35294 |  0:05:43s\n",
            "\n",
            "Early stopping occurred at epoch 57 with best_epoch = 37 and best_val_0_unsup_loss = 1.35294\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.69313 | train_auc: 0.50307 | valid_auc: 0.50413 |  0:00:09s\n",
            "epoch 1  | loss: 0.64419 | train_auc: 0.72464 | valid_auc: 0.72329 |  0:00:18s\n",
            "epoch 2  | loss: 0.39021 | train_auc: 0.6624  | valid_auc: 0.66231 |  0:00:28s\n",
            "epoch 3  | loss: 0.25282 | train_auc: 0.46968 | valid_auc: 0.46953 |  0:00:37s\n",
            "epoch 4  | loss: 0.20197 | train_auc: 0.59742 | valid_auc: 0.59744 |  0:00:46s\n",
            "epoch 5  | loss: 0.18226 | train_auc: 0.66024 | valid_auc: 0.65977 |  0:00:55s\n",
            "epoch 6  | loss: 0.16861 | train_auc: 0.81882 | valid_auc: 0.8193  |  0:01:05s\n",
            "epoch 7  | loss: 0.15804 | train_auc: 0.84979 | valid_auc: 0.85011 |  0:01:15s\n",
            "epoch 8  | loss: 0.15055 | train_auc: 0.89117 | valid_auc: 0.89264 |  0:01:24s\n",
            "epoch 9  | loss: 0.14531 | train_auc: 0.93058 | valid_auc: 0.93135 |  0:01:34s\n",
            "epoch 10 | loss: 0.14182 | train_auc: 0.95313 | valid_auc: 0.95357 |  0:01:43s\n",
            "epoch 11 | loss: 0.13855 | train_auc: 0.96925 | valid_auc: 0.96929 |  0:01:53s\n",
            "epoch 12 | loss: 0.1347  | train_auc: 0.98112 | valid_auc: 0.98085 |  0:02:02s\n",
            "epoch 13 | loss: 0.13223 | train_auc: 0.97825 | valid_auc: 0.97797 |  0:02:11s\n",
            "epoch 14 | loss: 0.1306  | train_auc: 0.98385 | valid_auc: 0.9836  |  0:02:21s\n",
            "epoch 15 | loss: 0.12822 | train_auc: 0.98303 | valid_auc: 0.98268 |  0:02:30s\n",
            "epoch 16 | loss: 0.12506 | train_auc: 0.986   | valid_auc: 0.98561 |  0:02:40s\n",
            "epoch 17 | loss: 0.12294 | train_auc: 0.98665 | valid_auc: 0.98621 |  0:02:50s\n",
            "epoch 18 | loss: 0.12144 | train_auc: 0.98846 | valid_auc: 0.98788 |  0:02:59s\n",
            "epoch 19 | loss: 0.11833 | train_auc: 0.98889 | valid_auc: 0.98835 |  0:03:08s\n",
            "epoch 20 | loss: 0.11779 | train_auc: 0.98812 | valid_auc: 0.98748 |  0:03:17s\n",
            "epoch 21 | loss: 0.1165  | train_auc: 0.98825 | valid_auc: 0.98775 |  0:03:26s\n",
            "epoch 22 | loss: 0.11556 | train_auc: 0.98949 | valid_auc: 0.98887 |  0:03:36s\n",
            "epoch 23 | loss: 0.11461 | train_auc: 0.99053 | valid_auc: 0.99003 |  0:03:45s\n",
            "epoch 24 | loss: 0.11409 | train_auc: 0.98988 | valid_auc: 0.9893  |  0:03:54s\n",
            "epoch 25 | loss: 0.11301 | train_auc: 0.99048 | valid_auc: 0.98999 |  0:04:04s\n",
            "epoch 26 | loss: 0.11166 | train_auc: 0.99032 | valid_auc: 0.9897  |  0:04:13s\n",
            "epoch 27 | loss: 0.11022 | train_auc: 0.99126 | valid_auc: 0.99066 |  0:04:23s\n",
            "epoch 28 | loss: 0.11011 | train_auc: 0.9913  | valid_auc: 0.99079 |  0:04:33s\n",
            "epoch 29 | loss: 0.11105 | train_auc: 0.99102 | valid_auc: 0.99041 |  0:04:42s\n",
            "epoch 30 | loss: 0.11074 | train_auc: 0.99182 | valid_auc: 0.99127 |  0:04:52s\n",
            "epoch 31 | loss: 0.10906 | train_auc: 0.99177 | valid_auc: 0.99119 |  0:05:01s\n",
            "epoch 32 | loss: 0.10814 | train_auc: 0.99187 | valid_auc: 0.99132 |  0:05:11s\n",
            "epoch 33 | loss: 0.10671 | train_auc: 0.99156 | valid_auc: 0.991   |  0:05:21s\n",
            "epoch 34 | loss: 0.10652 | train_auc: 0.99153 | valid_auc: 0.99089 |  0:05:30s\n",
            "epoch 35 | loss: 0.10615 | train_auc: 0.99208 | valid_auc: 0.99154 |  0:05:40s\n",
            "epoch 36 | loss: 0.1057  | train_auc: 0.99244 | valid_auc: 0.99187 |  0:05:49s\n",
            "epoch 37 | loss: 0.10521 | train_auc: 0.99221 | valid_auc: 0.99165 |  0:05:59s\n",
            "epoch 38 | loss: 0.10464 | train_auc: 0.99259 | valid_auc: 0.99211 |  0:06:08s\n",
            "epoch 39 | loss: 0.10377 | train_auc: 0.99167 | valid_auc: 0.99104 |  0:06:18s\n",
            "epoch 40 | loss: 0.10389 | train_auc: 0.99163 | valid_auc: 0.99106 |  0:06:27s\n",
            "epoch 41 | loss: 0.10406 | train_auc: 0.99255 | valid_auc: 0.99204 |  0:06:37s\n",
            "epoch 42 | loss: 0.10294 | train_auc: 0.99216 | valid_auc: 0.9917  |  0:06:46s\n",
            "epoch 43 | loss: 0.10334 | train_auc: 0.99282 | valid_auc: 0.99222 |  0:06:55s\n",
            "epoch 44 | loss: 0.10237 | train_auc: 0.99211 | valid_auc: 0.99172 |  0:07:05s\n",
            "epoch 45 | loss: 0.10186 | train_auc: 0.99249 | valid_auc: 0.99192 |  0:07:15s\n",
            "epoch 46 | loss: 0.10078 | train_auc: 0.9932  | valid_auc: 0.99269 |  0:07:24s\n",
            "epoch 47 | loss: 0.10039 | train_auc: 0.99234 | valid_auc: 0.99197 |  0:07:34s\n",
            "epoch 48 | loss: 0.10029 | train_auc: 0.99213 | valid_auc: 0.99172 |  0:07:43s\n",
            "epoch 49 | loss: 0.10082 | train_auc: 0.99211 | valid_auc: 0.99171 |  0:07:53s\n",
            "epoch 50 | loss: 0.10017 | train_auc: 0.99248 | valid_auc: 0.99201 |  0:08:02s\n",
            "epoch 51 | loss: 0.09865 | train_auc: 0.99202 | valid_auc: 0.99134 |  0:08:12s\n",
            "epoch 52 | loss: 0.09925 | train_auc: 0.99089 | valid_auc: 0.99032 |  0:08:22s\n",
            "epoch 53 | loss: 0.09906 | train_auc: 0.99186 | valid_auc: 0.99124 |  0:08:31s\n",
            "epoch 54 | loss: 0.09977 | train_auc: 0.99243 | valid_auc: 0.99196 |  0:08:41s\n",
            "epoch 55 | loss: 0.09954 | train_auc: 0.9931  | valid_auc: 0.99253 |  0:08:50s\n",
            "epoch 56 | loss: 0.09785 | train_auc: 0.99312 | valid_auc: 0.99252 |  0:08:59s\n",
            "epoch 57 | loss: 0.0979  | train_auc: 0.9932  | valid_auc: 0.99272 |  0:09:09s\n",
            "epoch 58 | loss: 0.09746 | train_auc: 0.99339 | valid_auc: 0.99285 |  0:09:18s\n",
            "epoch 59 | loss: 0.0966  | train_auc: 0.99347 | valid_auc: 0.99288 |  0:09:27s\n",
            "epoch 60 | loss: 0.09631 | train_auc: 0.99345 | valid_auc: 0.99293 |  0:09:37s\n",
            "epoch 61 | loss: 0.0958  | train_auc: 0.99339 | valid_auc: 0.99283 |  0:09:46s\n",
            "epoch 62 | loss: 0.09602 | train_auc: 0.99323 | valid_auc: 0.9926  |  0:09:55s\n",
            "epoch 63 | loss: 0.0958  | train_auc: 0.99295 | valid_auc: 0.99233 |  0:10:05s\n",
            "epoch 64 | loss: 0.09479 | train_auc: 0.99355 | valid_auc: 0.99293 |  0:10:15s\n",
            "epoch 65 | loss: 0.09504 | train_auc: 0.99341 | valid_auc: 0.9928  |  0:10:24s\n",
            "epoch 66 | loss: 0.09449 | train_auc: 0.99331 | valid_auc: 0.99273 |  0:10:34s\n",
            "epoch 67 | loss: 0.094   | train_auc: 0.9932  | valid_auc: 0.99266 |  0:10:44s\n",
            "epoch 68 | loss: 0.09414 | train_auc: 0.99311 | valid_auc: 0.9925  |  0:10:53s\n",
            "epoch 69 | loss: 0.09498 | train_auc: 0.99336 | valid_auc: 0.99279 |  0:11:03s\n",
            "epoch 70 | loss: 0.0937  | train_auc: 0.99361 | valid_auc: 0.99314 |  0:11:12s\n",
            "epoch 71 | loss: 0.09397 | train_auc: 0.99379 | valid_auc: 0.99321 |  0:11:22s\n",
            "epoch 72 | loss: 0.09323 | train_auc: 0.99359 | valid_auc: 0.99296 |  0:11:32s\n",
            "epoch 73 | loss: 0.0929  | train_auc: 0.99378 | valid_auc: 0.99314 |  0:11:41s\n",
            "epoch 74 | loss: 0.09379 | train_auc: 0.99367 | valid_auc: 0.99305 |  0:11:50s\n",
            "epoch 75 | loss: 0.09418 | train_auc: 0.99366 | valid_auc: 0.99302 |  0:12:00s\n",
            "epoch 76 | loss: 0.09341 | train_auc: 0.99365 | valid_auc: 0.99309 |  0:12:09s\n",
            "epoch 77 | loss: 0.09239 | train_auc: 0.99372 | valid_auc: 0.99311 |  0:12:18s\n",
            "epoch 78 | loss: 0.09313 | train_auc: 0.99372 | valid_auc: 0.99306 |  0:12:27s\n",
            "epoch 79 | loss: 0.09321 | train_auc: 0.99397 | valid_auc: 0.99337 |  0:12:37s\n",
            "epoch 80 | loss: 0.09235 | train_auc: 0.99394 | valid_auc: 0.99338 |  0:12:46s\n",
            "epoch 81 | loss: 0.09236 | train_auc: 0.99378 | valid_auc: 0.99309 |  0:12:55s\n",
            "epoch 82 | loss: 0.09224 | train_auc: 0.99408 | valid_auc: 0.99346 |  0:13:05s\n",
            "epoch 83 | loss: 0.09073 | train_auc: 0.99415 | valid_auc: 0.99349 |  0:13:15s\n",
            "epoch 84 | loss: 0.09124 | train_auc: 0.99401 | valid_auc: 0.99336 |  0:13:24s\n",
            "epoch 85 | loss: 0.09151 | train_auc: 0.9941  | valid_auc: 0.99349 |  0:13:34s\n",
            "epoch 86 | loss: 0.0915  | train_auc: 0.99404 | valid_auc: 0.99339 |  0:13:44s\n",
            "epoch 87 | loss: 0.0915  | train_auc: 0.99408 | valid_auc: 0.99345 |  0:13:53s\n",
            "epoch 88 | loss: 0.09075 | train_auc: 0.99423 | valid_auc: 0.99358 |  0:14:03s\n",
            "epoch 89 | loss: 0.0913  | train_auc: 0.99395 | valid_auc: 0.99334 |  0:14:13s\n",
            "epoch 90 | loss: 0.09264 | train_auc: 0.99404 | valid_auc: 0.99337 |  0:14:22s\n",
            "epoch 91 | loss: 0.092   | train_auc: 0.99401 | valid_auc: 0.99338 |  0:14:32s\n",
            "epoch 92 | loss: 0.09156 | train_auc: 0.99426 | valid_auc: 0.99358 |  0:14:41s\n",
            "epoch 93 | loss: 0.09104 | train_auc: 0.99419 | valid_auc: 0.99353 |  0:14:51s\n",
            "epoch 94 | loss: 0.09198 | train_auc: 0.99399 | valid_auc: 0.99324 |  0:15:00s\n",
            "epoch 95 | loss: 0.09116 | train_auc: 0.99423 | valid_auc: 0.99355 |  0:15:10s\n",
            "epoch 96 | loss: 0.09044 | train_auc: 0.99407 | valid_auc: 0.99345 |  0:15:19s\n",
            "epoch 97 | loss: 0.09112 | train_auc: 0.99411 | valid_auc: 0.99345 |  0:15:28s\n",
            "epoch 98 | loss: 0.09062 | train_auc: 0.99418 | valid_auc: 0.99354 |  0:15:38s\n",
            "epoch 99 | loss: 0.09025 | train_auc: 0.9942  | valid_auc: 0.99355 |  0:15:47s\n",
            "epoch 100| loss: 0.08986 | train_auc: 0.99411 | valid_auc: 0.99351 |  0:15:56s\n",
            "epoch 101| loss: 0.08955 | train_auc: 0.9942  | valid_auc: 0.99347 |  0:16:06s\n",
            "epoch 102| loss: 0.08982 | train_auc: 0.9943  | valid_auc: 0.99366 |  0:16:16s\n",
            "epoch 103| loss: 0.09032 | train_auc: 0.99431 | valid_auc: 0.9937  |  0:16:25s\n",
            "epoch 104| loss: 0.08958 | train_auc: 0.99431 | valid_auc: 0.99359 |  0:16:35s\n",
            "epoch 105| loss: 0.08968 | train_auc: 0.99428 | valid_auc: 0.99369 |  0:16:45s\n",
            "epoch 106| loss: 0.08946 | train_auc: 0.99409 | valid_auc: 0.99322 |  0:16:54s\n",
            "epoch 107| loss: 0.08932 | train_auc: 0.99424 | valid_auc: 0.99346 |  0:17:04s\n",
            "epoch 108| loss: 0.08957 | train_auc: 0.99433 | valid_auc: 0.99353 |  0:17:14s\n",
            "epoch 109| loss: 0.08942 | train_auc: 0.9942  | valid_auc: 0.99333 |  0:17:23s\n",
            "epoch 110| loss: 0.08898 | train_auc: 0.99369 | valid_auc: 0.99299 |  0:17:33s\n",
            "epoch 111| loss: 0.08919 | train_auc: 0.9942  | valid_auc: 0.99355 |  0:17:42s\n",
            "epoch 112| loss: 0.08859 | train_auc: 0.99435 | valid_auc: 0.99371 |  0:17:52s\n",
            "epoch 113| loss: 0.08941 | train_auc: 0.99442 | valid_auc: 0.99373 |  0:18:01s\n",
            "epoch 114| loss: 0.08877 | train_auc: 0.99436 | valid_auc: 0.99371 |  0:18:10s\n",
            "epoch 115| loss: 0.08859 | train_auc: 0.99454 | valid_auc: 0.9939  |  0:18:20s\n",
            "epoch 116| loss: 0.08782 | train_auc: 0.99438 | valid_auc: 0.99373 |  0:18:29s\n",
            "epoch 117| loss: 0.08785 | train_auc: 0.99444 | valid_auc: 0.99376 |  0:18:38s\n",
            "epoch 118| loss: 0.08785 | train_auc: 0.99432 | valid_auc: 0.99362 |  0:18:48s\n",
            "epoch 119| loss: 0.08809 | train_auc: 0.99436 | valid_auc: 0.99379 |  0:18:57s\n",
            "epoch 120| loss: 0.08775 | train_auc: 0.99448 | valid_auc: 0.99385 |  0:19:07s\n",
            "epoch 121| loss: 0.08775 | train_auc: 0.9945  | valid_auc: 0.99388 |  0:19:16s\n",
            "epoch 122| loss: 0.08734 | train_auc: 0.99453 | valid_auc: 0.99382 |  0:19:25s\n",
            "epoch 123| loss: 0.08808 | train_auc: 0.99451 | valid_auc: 0.99386 |  0:19:35s\n",
            "epoch 124| loss: 0.08858 | train_auc: 0.99452 | valid_auc: 0.99386 |  0:19:45s\n",
            "epoch 125| loss: 0.08771 | train_auc: 0.9944  | valid_auc: 0.99385 |  0:19:54s\n",
            "epoch 126| loss: 0.08769 | train_auc: 0.97576 | valid_auc: 0.97462 |  0:20:04s\n",
            "epoch 127| loss: 0.08738 | train_auc: 0.99455 | valid_auc: 0.99377 |  0:20:14s\n",
            "epoch 128| loss: 0.08734 | train_auc: 0.99453 | valid_auc: 0.99373 |  0:20:23s\n",
            "epoch 129| loss: 0.08729 | train_auc: 0.99461 | valid_auc: 0.99395 |  0:20:33s\n",
            "epoch 130| loss: 0.08694 | train_auc: 0.99447 | valid_auc: 0.99374 |  0:20:43s\n",
            "epoch 131| loss: 0.08753 | train_auc: 0.99454 | valid_auc: 0.99383 |  0:20:53s\n",
            "epoch 132| loss: 0.08689 | train_auc: 0.99465 | valid_auc: 0.99382 |  0:21:03s\n",
            "epoch 133| loss: 0.08714 | train_auc: 0.99463 | valid_auc: 0.99389 |  0:21:12s\n",
            "epoch 134| loss: 0.08646 | train_auc: 0.99475 | valid_auc: 0.99392 |  0:21:21s\n",
            "epoch 135| loss: 0.08653 | train_auc: 0.99463 | valid_auc: 0.99392 |  0:21:31s\n",
            "epoch 136| loss: 0.08627 | train_auc: 0.99462 | valid_auc: 0.9939  |  0:21:40s\n",
            "epoch 137| loss: 0.08612 | train_auc: 0.99474 | valid_auc: 0.99405 |  0:21:49s\n",
            "epoch 138| loss: 0.08651 | train_auc: 0.9945  | valid_auc: 0.99374 |  0:21:59s\n",
            "epoch 139| loss: 0.08709 | train_auc: 0.99452 | valid_auc: 0.99387 |  0:22:09s\n",
            "epoch 140| loss: 0.08677 | train_auc: 0.99459 | valid_auc: 0.99381 |  0:22:19s\n",
            "epoch 141| loss: 0.08706 | train_auc: 0.99461 | valid_auc: 0.99376 |  0:22:28s\n",
            "epoch 142| loss: 0.08672 | train_auc: 0.99452 | valid_auc: 0.99381 |  0:22:38s\n",
            "epoch 143| loss: 0.08582 | train_auc: 0.99459 | valid_auc: 0.99384 |  0:22:47s\n",
            "epoch 144| loss: 0.08592 | train_auc: 0.9947  | valid_auc: 0.99386 |  0:22:57s\n",
            "epoch 145| loss: 0.0865  | train_auc: 0.99476 | valid_auc: 0.994   |  0:23:07s\n",
            "epoch 146| loss: 0.0859  | train_auc: 0.99471 | valid_auc: 0.99397 |  0:23:16s\n",
            "epoch 147| loss: 0.08552 | train_auc: 0.99471 | valid_auc: 0.99407 |  0:23:26s\n",
            "epoch 148| loss: 0.08619 | train_auc: 0.99432 | valid_auc: 0.99355 |  0:23:36s\n",
            "epoch 149| loss: 0.08575 | train_auc: 0.99467 | valid_auc: 0.99398 |  0:23:45s\n",
            "epoch 150| loss: 0.08482 | train_auc: 0.99475 | valid_auc: 0.99407 |  0:23:54s\n",
            "epoch 151| loss: 0.08491 | train_auc: 0.99474 | valid_auc: 0.99402 |  0:24:03s\n",
            "epoch 152| loss: 0.08523 | train_auc: 0.99457 | valid_auc: 0.99377 |  0:24:13s\n",
            "epoch 153| loss: 0.08523 | train_auc: 0.99484 | valid_auc: 0.99418 |  0:24:22s\n",
            "epoch 154| loss: 0.08541 | train_auc: 0.99484 | valid_auc: 0.99418 |  0:24:31s\n",
            "epoch 155| loss: 0.08492 | train_auc: 0.99496 | valid_auc: 0.9943  |  0:24:41s\n",
            "epoch 156| loss: 0.08481 | train_auc: 0.9949  | valid_auc: 0.99411 |  0:24:51s\n",
            "epoch 157| loss: 0.08556 | train_auc: 0.99474 | valid_auc: 0.99411 |  0:25:00s\n",
            "epoch 158| loss: 0.08488 | train_auc: 0.99496 | valid_auc: 0.99424 |  0:25:10s\n",
            "epoch 159| loss: 0.08448 | train_auc: 0.99491 | valid_auc: 0.99413 |  0:25:19s\n",
            "epoch 160| loss: 0.08488 | train_auc: 0.99464 | valid_auc: 0.99388 |  0:25:29s\n",
            "epoch 161| loss: 0.08466 | train_auc: 0.99485 | valid_auc: 0.99417 |  0:25:38s\n",
            "epoch 162| loss: 0.08473 | train_auc: 0.99481 | valid_auc: 0.9941  |  0:25:48s\n",
            "epoch 163| loss: 0.08433 | train_auc: 0.9948  | valid_auc: 0.99408 |  0:25:58s\n",
            "epoch 164| loss: 0.08474 | train_auc: 0.99484 | valid_auc: 0.99404 |  0:26:08s\n",
            "epoch 165| loss: 0.08522 | train_auc: 0.9948  | valid_auc: 0.99406 |  0:26:17s\n",
            "epoch 166| loss: 0.08457 | train_auc: 0.99496 | valid_auc: 0.9942  |  0:26:27s\n",
            "epoch 167| loss: 0.08232 | train_auc: 0.99575 | valid_auc: 0.99502 |  0:26:37s\n",
            "epoch 168| loss: 0.07633 | train_auc: 0.99646 | valid_auc: 0.99582 |  0:26:46s\n",
            "epoch 169| loss: 0.07513 | train_auc: 0.99643 | valid_auc: 0.99589 |  0:26:56s\n",
            "epoch 170| loss: 0.07354 | train_auc: 0.99673 | valid_auc: 0.99611 |  0:27:06s\n",
            "epoch 171| loss: 0.07256 | train_auc: 0.99662 | valid_auc: 0.99609 |  0:27:15s\n",
            "epoch 172| loss: 0.07265 | train_auc: 0.99682 | valid_auc: 0.99622 |  0:27:25s\n",
            "epoch 173| loss: 0.0735  | train_auc: 0.99683 | valid_auc: 0.99622 |  0:27:34s\n",
            "epoch 174| loss: 0.073   | train_auc: 0.99671 | valid_auc: 0.99619 |  0:27:43s\n",
            "epoch 175| loss: 0.07255 | train_auc: 0.99701 | valid_auc: 0.99641 |  0:27:53s\n",
            "epoch 176| loss: 0.07219 | train_auc: 0.9969  | valid_auc: 0.99629 |  0:28:02s\n",
            "epoch 177| loss: 0.07225 | train_auc: 0.99703 | valid_auc: 0.99643 |  0:28:12s\n",
            "epoch 178| loss: 0.07202 | train_auc: 0.99701 | valid_auc: 0.99639 |  0:28:21s\n",
            "epoch 179| loss: 0.07158 | train_auc: 0.99705 | valid_auc: 0.99644 |  0:28:30s\n",
            "epoch 180| loss: 0.07093 | train_auc: 0.99709 | valid_auc: 0.99648 |  0:28:40s\n",
            "epoch 181| loss: 0.07104 | train_auc: 0.99705 | valid_auc: 0.9965  |  0:28:50s\n",
            "epoch 182| loss: 0.07139 | train_auc: 0.99699 | valid_auc: 0.99638 |  0:28:59s\n",
            "epoch 183| loss: 0.07159 | train_auc: 0.99698 | valid_auc: 0.9964  |  0:29:09s\n",
            "epoch 184| loss: 0.07127 | train_auc: 0.9971  | valid_auc: 0.99648 |  0:29:18s\n",
            "epoch 185| loss: 0.07147 | train_auc: 0.997   | valid_auc: 0.99638 |  0:29:28s\n",
            "epoch 186| loss: 0.07284 | train_auc: 0.99712 | valid_auc: 0.99651 |  0:29:38s\n",
            "epoch 187| loss: 0.07112 | train_auc: 0.99708 | valid_auc: 0.99651 |  0:29:47s\n",
            "epoch 188| loss: 0.0708  | train_auc: 0.99715 | valid_auc: 0.99659 |  0:29:57s\n",
            "epoch 189| loss: 0.07034 | train_auc: 0.99721 | valid_auc: 0.99667 |  0:30:07s\n",
            "epoch 190| loss: 0.07064 | train_auc: 0.99716 | valid_auc: 0.99658 |  0:30:16s\n",
            "epoch 191| loss: 0.07045 | train_auc: 0.99712 | valid_auc: 0.99655 |  0:30:26s\n",
            "epoch 192| loss: 0.07089 | train_auc: 0.99708 | valid_auc: 0.99651 |  0:30:35s\n",
            "epoch 193| loss: 0.07034 | train_auc: 0.99723 | valid_auc: 0.99667 |  0:30:45s\n",
            "epoch 194| loss: 0.07058 | train_auc: 0.99723 | valid_auc: 0.99663 |  0:30:54s\n",
            "epoch 195| loss: 0.07043 | train_auc: 0.99725 | valid_auc: 0.9967  |  0:31:03s\n",
            "epoch 196| loss: 0.07001 | train_auc: 0.99721 | valid_auc: 0.99658 |  0:31:13s\n",
            "epoch 197| loss: 0.07053 | train_auc: 0.99715 | valid_auc: 0.99662 |  0:31:22s\n",
            "epoch 198| loss: 0.06994 | train_auc: 0.99724 | valid_auc: 0.99665 |  0:31:31s\n",
            "epoch 199| loss: 0.07044 | train_auc: 0.99716 | valid_auc: 0.99657 |  0:31:41s\n",
            "epoch 200| loss: 0.07118 | train_auc: 0.99726 | valid_auc: 0.9967  |  0:31:51s\n",
            "epoch 201| loss: 0.07001 | train_auc: 0.9972  | valid_auc: 0.99656 |  0:32:00s\n",
            "epoch 202| loss: 0.06998 | train_auc: 0.99727 | valid_auc: 0.99672 |  0:32:10s\n",
            "epoch 203| loss: 0.06983 | train_auc: 0.99726 | valid_auc: 0.9967  |  0:32:20s\n",
            "epoch 204| loss: 0.06931 | train_auc: 0.9973  | valid_auc: 0.99672 |  0:32:29s\n",
            "epoch 205| loss: 0.06952 | train_auc: 0.99728 | valid_auc: 0.99676 |  0:32:39s\n",
            "epoch 206| loss: 0.06992 | train_auc: 0.99721 | valid_auc: 0.99662 |  0:32:49s\n",
            "epoch 207| loss: 0.0696  | train_auc: 0.99718 | valid_auc: 0.99659 |  0:32:58s\n",
            "epoch 208| loss: 0.0697  | train_auc: 0.99715 | valid_auc: 0.99658 |  0:33:08s\n",
            "epoch 209| loss: 0.06971 | train_auc: 0.99713 | valid_auc: 0.99661 |  0:33:18s\n",
            "epoch 210| loss: 0.07002 | train_auc: 0.99722 | valid_auc: 0.99667 |  0:33:28s\n",
            "epoch 211| loss: 0.06916 | train_auc: 0.99719 | valid_auc: 0.99667 |  0:33:37s\n",
            "epoch 212| loss: 0.06891 | train_auc: 0.9973  | valid_auc: 0.99672 |  0:33:47s\n",
            "epoch 213| loss: 0.06897 | train_auc: 0.99731 | valid_auc: 0.99678 |  0:33:56s\n",
            "epoch 214| loss: 0.06888 | train_auc: 0.99734 | valid_auc: 0.9968  |  0:34:05s\n",
            "epoch 215| loss: 0.06876 | train_auc: 0.99731 | valid_auc: 0.99677 |  0:34:14s\n",
            "epoch 216| loss: 0.06881 | train_auc: 0.99727 | valid_auc: 0.99674 |  0:34:24s\n",
            "epoch 217| loss: 0.06947 | train_auc: 0.99721 | valid_auc: 0.99666 |  0:34:33s\n",
            "epoch 218| loss: 0.06952 | train_auc: 0.99733 | valid_auc: 0.99679 |  0:34:42s\n",
            "epoch 219| loss: 0.06837 | train_auc: 0.99734 | valid_auc: 0.99682 |  0:34:52s\n",
            "epoch 220| loss: 0.06849 | train_auc: 0.99736 | valid_auc: 0.99681 |  0:35:01s\n",
            "epoch 221| loss: 0.06835 | train_auc: 0.99733 | valid_auc: 0.99677 |  0:35:11s\n",
            "epoch 222| loss: 0.06969 | train_auc: 0.99725 | valid_auc: 0.99666 |  0:35:21s\n",
            "epoch 223| loss: 0.0695  | train_auc: 0.99721 | valid_auc: 0.99668 |  0:35:31s\n",
            "epoch 224| loss: 0.06868 | train_auc: 0.99736 | valid_auc: 0.99679 |  0:35:41s\n",
            "epoch 225| loss: 0.06822 | train_auc: 0.99727 | valid_auc: 0.99673 |  0:35:50s\n",
            "epoch 226| loss: 0.06804 | train_auc: 0.99583 | valid_auc: 0.99535 |  0:36:00s\n",
            "epoch 227| loss: 0.11858 | train_auc: 0.99436 | valid_auc: 0.99365 |  0:36:10s\n",
            "epoch 228| loss: 0.08809 | train_auc: 0.99507 | valid_auc: 0.99441 |  0:36:19s\n",
            "epoch 229| loss: 0.07994 | train_auc: 0.99633 | valid_auc: 0.99582 |  0:36:29s\n",
            "epoch 230| loss: 0.07562 | train_auc: 0.99673 | valid_auc: 0.99621 |  0:36:39s\n",
            "epoch 231| loss: 0.07341 | train_auc: 0.99697 | valid_auc: 0.99652 |  0:36:49s\n",
            "epoch 232| loss: 0.07195 | train_auc: 0.99711 | valid_auc: 0.99663 |  0:36:58s\n",
            "epoch 233| loss: 0.07108 | train_auc: 0.99708 | valid_auc: 0.99662 |  0:37:08s\n",
            "epoch 234| loss: 0.07078 | train_auc: 0.99704 | valid_auc: 0.99653 |  0:37:18s\n",
            "epoch 235| loss: 0.0701  | train_auc: 0.99705 | valid_auc: 0.99652 |  0:37:28s\n",
            "epoch 236| loss: 0.07012 | train_auc: 0.99713 | valid_auc: 0.99663 |  0:37:38s\n",
            "epoch 237| loss: 0.06966 | train_auc: 0.99716 | valid_auc: 0.99665 |  0:37:47s\n",
            "epoch 238| loss: 0.06926 | train_auc: 0.99713 | valid_auc: 0.9966  |  0:37:57s\n",
            "epoch 239| loss: 0.06973 | train_auc: 0.99713 | valid_auc: 0.99665 |  0:38:07s\n",
            "\n",
            "Early stopping occurred at epoch 239 with best_epoch = 219 and best_valid_auc = 0.99682\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9968169814372859\n",
            "----- 8 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 18162526.71097| val_0_unsup_loss: 5558.56641|  0:00:05s\n",
            "epoch 1  | loss: 4435.77078| val_0_unsup_loss: 3.14834 |  0:00:11s\n",
            "epoch 2  | loss: 8.2033  | val_0_unsup_loss: 2.11372 |  0:00:17s\n",
            "epoch 3  | loss: 2.57989 | val_0_unsup_loss: 1.77417 |  0:00:23s\n",
            "epoch 4  | loss: 6.22542 | val_0_unsup_loss: 1.59285 |  0:00:29s\n",
            "epoch 5  | loss: 9.00485 | val_0_unsup_loss: 1.51991 |  0:00:35s\n",
            "epoch 6  | loss: 4.93759 | val_0_unsup_loss: 1.45885 |  0:00:41s\n",
            "epoch 7  | loss: 1.49544 | val_0_unsup_loss: 1.40747 |  0:00:47s\n",
            "epoch 8  | loss: 1.56576 | val_0_unsup_loss: 1.3767  |  0:00:53s\n",
            "epoch 9  | loss: 1.47704 | val_0_unsup_loss: 1.36864 |  0:00:59s\n",
            "epoch 10 | loss: 1.42461 | val_0_unsup_loss: 1.36296 |  0:01:05s\n",
            "epoch 11 | loss: 4.0381  | val_0_unsup_loss: 1.36037 |  0:01:11s\n",
            "epoch 12 | loss: 1.35983 | val_0_unsup_loss: 1.35874 |  0:01:17s\n",
            "epoch 13 | loss: 1.67691 | val_0_unsup_loss: 1.35805 |  0:01:23s\n",
            "epoch 14 | loss: 1.35788 | val_0_unsup_loss: 1.35712 |  0:01:29s\n",
            "epoch 15 | loss: 1.357   | val_0_unsup_loss: 1.35641 |  0:01:35s\n",
            "epoch 16 | loss: 1.46251 | val_0_unsup_loss: 1.35592 |  0:01:41s\n",
            "epoch 17 | loss: 1.35614 | val_0_unsup_loss: 1.35553 |  0:01:47s\n",
            "epoch 18 | loss: 1.35542 | val_0_unsup_loss: 1.35517 |  0:01:53s\n",
            "epoch 19 | loss: 1.35632 | val_0_unsup_loss: 1.35554 |  0:01:58s\n",
            "epoch 20 | loss: 1.35557 | val_0_unsup_loss: 1.35483 |  0:02:04s\n",
            "epoch 21 | loss: 1.35479 | val_0_unsup_loss: 1.3546  |  0:02:10s\n",
            "epoch 22 | loss: 1.35442 | val_0_unsup_loss: 1.35438 |  0:02:17s\n",
            "epoch 23 | loss: 1.35519 | val_0_unsup_loss: 1.35483 |  0:02:23s\n",
            "epoch 24 | loss: 1.35452 | val_0_unsup_loss: 1.35421 |  0:02:29s\n",
            "epoch 25 | loss: 1.35427 | val_0_unsup_loss: 1.35416 |  0:02:34s\n",
            "epoch 26 | loss: 1.35414 | val_0_unsup_loss: 1.3541  |  0:02:41s\n",
            "epoch 27 | loss: 1.35419 | val_0_unsup_loss: 1.35406 |  0:02:47s\n",
            "epoch 28 | loss: 1.35452 | val_0_unsup_loss: 1.35404 |  0:02:53s\n",
            "epoch 29 | loss: 1.35449 | val_0_unsup_loss: 1.35403 |  0:02:59s\n",
            "epoch 30 | loss: 1.35407 | val_0_unsup_loss: 1.35402 |  0:03:05s\n",
            "epoch 31 | loss: 1.3539  | val_0_unsup_loss: 1.35401 |  0:03:11s\n",
            "epoch 32 | loss: 1.35414 | val_0_unsup_loss: 1.35401 |  0:03:17s\n",
            "epoch 33 | loss: 1.35447 | val_0_unsup_loss: 1.354   |  0:03:23s\n",
            "epoch 34 | loss: 1.35418 | val_0_unsup_loss: 1.354   |  0:03:29s\n",
            "epoch 35 | loss: 1.35427 | val_0_unsup_loss: 1.354   |  0:03:35s\n",
            "epoch 36 | loss: 1.35416 | val_0_unsup_loss: 1.354   |  0:03:41s\n",
            "epoch 37 | loss: 1.35451 | val_0_unsup_loss: 1.354   |  0:03:47s\n",
            "epoch 38 | loss: 1.35418 | val_0_unsup_loss: 1.354   |  0:03:53s\n",
            "epoch 39 | loss: 1.35421 | val_0_unsup_loss: 1.354   |  0:03:59s\n",
            "epoch 40 | loss: 1.35413 | val_0_unsup_loss: 1.354   |  0:04:05s\n",
            "epoch 41 | loss: 1.35419 | val_0_unsup_loss: 1.354   |  0:04:11s\n",
            "epoch 42 | loss: 1.35427 | val_0_unsup_loss: 1.354   |  0:04:17s\n",
            "epoch 43 | loss: 1.35516 | val_0_unsup_loss: 1.35405 |  0:04:23s\n",
            "epoch 44 | loss: 1.35482 | val_0_unsup_loss: 1.35435 |  0:04:29s\n",
            "epoch 45 | loss: 1.3552  | val_0_unsup_loss: 1.35408 |  0:04:35s\n",
            "epoch 46 | loss: 1.35394 | val_0_unsup_loss: 1.35402 |  0:04:41s\n",
            "epoch 47 | loss: 1.35427 | val_0_unsup_loss: 1.35402 |  0:04:47s\n",
            "epoch 48 | loss: 1.3542  | val_0_unsup_loss: 1.354   |  0:04:54s\n",
            "epoch 49 | loss: 1.35456 | val_0_unsup_loss: 1.354   |  0:05:00s\n",
            "epoch 50 | loss: 13.18136| val_0_unsup_loss: 1.35402 |  0:05:05s\n",
            "epoch 51 | loss: 1.35438 | val_0_unsup_loss: 1.35402 |  0:05:11s\n",
            "epoch 52 | loss: 1.36564 | val_0_unsup_loss: 1.35406 |  0:05:18s\n",
            "epoch 53 | loss: 1.7197  | val_0_unsup_loss: 1.35421 |  0:05:23s\n",
            "epoch 54 | loss: 1.35606 | val_0_unsup_loss: 1.35415 |  0:05:29s\n",
            "epoch 55 | loss: 1.35447 | val_0_unsup_loss: 1.35408 |  0:05:35s\n",
            "epoch 56 | loss: 1.35423 | val_0_unsup_loss: 1.35403 |  0:05:41s\n",
            "epoch 57 | loss: 1.35418 | val_0_unsup_loss: 1.35401 |  0:05:47s\n",
            "epoch 58 | loss: 1.35412 | val_0_unsup_loss: 1.354   |  0:05:53s\n",
            "epoch 59 | loss: 1.35401 | val_0_unsup_loss: 1.354   |  0:05:59s\n",
            "epoch 60 | loss: 1.35448 | val_0_unsup_loss: 1.354   |  0:06:05s\n",
            "epoch 61 | loss: 1.35425 | val_0_unsup_loss: 1.354   |  0:06:11s\n",
            "\n",
            "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss = 1.354\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.69141 | train_auc: 0.51946 | valid_auc: 0.51885 |  0:00:09s\n",
            "epoch 1  | loss: 0.62884 | train_auc: 0.56847 | valid_auc: 0.56692 |  0:00:18s\n",
            "epoch 2  | loss: 0.39841 | train_auc: 0.50827 | valid_auc: 0.51125 |  0:00:28s\n",
            "epoch 3  | loss: 0.23734 | train_auc: 0.77554 | valid_auc: 0.77541 |  0:00:37s\n",
            "epoch 4  | loss: 0.18877 | train_auc: 0.83273 | valid_auc: 0.83496 |  0:00:47s\n",
            "epoch 5  | loss: 0.16648 | train_auc: 0.87776 | valid_auc: 0.87812 |  0:00:56s\n",
            "epoch 6  | loss: 0.15766 | train_auc: 0.86278 | valid_auc: 0.86322 |  0:01:06s\n",
            "epoch 7  | loss: 0.15191 | train_auc: 0.93775 | valid_auc: 0.93756 |  0:01:15s\n",
            "epoch 8  | loss: 0.14982 | train_auc: 0.91656 | valid_auc: 0.91738 |  0:01:25s\n",
            "epoch 9  | loss: 0.14579 | train_auc: 0.94476 | valid_auc: 0.94397 |  0:01:35s\n",
            "epoch 10 | loss: 0.1426  | train_auc: 0.96097 | valid_auc: 0.96052 |  0:01:44s\n",
            "epoch 11 | loss: 0.13961 | train_auc: 0.97573 | valid_auc: 0.97521 |  0:01:54s\n",
            "epoch 12 | loss: 0.13617 | train_auc: 0.9774  | valid_auc: 0.97691 |  0:02:04s\n",
            "epoch 13 | loss: 0.13291 | train_auc: 0.98361 | valid_auc: 0.98344 |  0:02:14s\n",
            "epoch 14 | loss: 0.12925 | train_auc: 0.98577 | valid_auc: 0.98579 |  0:02:24s\n",
            "epoch 15 | loss: 0.12569 | train_auc: 0.98548 | valid_auc: 0.9855  |  0:02:34s\n",
            "epoch 16 | loss: 0.12223 | train_auc: 0.98896 | valid_auc: 0.989   |  0:02:44s\n",
            "epoch 17 | loss: 0.1189  | train_auc: 0.98868 | valid_auc: 0.98863 |  0:02:53s\n",
            "epoch 18 | loss: 0.11641 | train_auc: 0.98907 | valid_auc: 0.98903 |  0:03:03s\n",
            "epoch 19 | loss: 0.11412 | train_auc: 0.98873 | valid_auc: 0.9887  |  0:03:13s\n",
            "epoch 20 | loss: 0.11403 | train_auc: 0.99072 | valid_auc: 0.99055 |  0:03:23s\n",
            "epoch 21 | loss: 0.11119 | train_auc: 0.9898  | valid_auc: 0.98969 |  0:03:33s\n",
            "epoch 22 | loss: 0.11018 | train_auc: 0.99018 | valid_auc: 0.99008 |  0:03:43s\n",
            "epoch 23 | loss: 0.11107 | train_auc: 0.9909  | valid_auc: 0.99071 |  0:03:52s\n",
            "epoch 24 | loss: 0.10937 | train_auc: 0.99151 | valid_auc: 0.99142 |  0:04:01s\n",
            "epoch 25 | loss: 0.10809 | train_auc: 0.99011 | valid_auc: 0.98987 |  0:04:11s\n",
            "epoch 26 | loss: 0.10715 | train_auc: 0.99126 | valid_auc: 0.99117 |  0:04:20s\n",
            "epoch 27 | loss: 0.10557 | train_auc: 0.99184 | valid_auc: 0.99184 |  0:04:30s\n",
            "epoch 28 | loss: 0.10467 | train_auc: 0.99179 | valid_auc: 0.99178 |  0:04:40s\n",
            "epoch 29 | loss: 0.10379 | train_auc: 0.99126 | valid_auc: 0.99109 |  0:04:49s\n",
            "epoch 30 | loss: 0.10251 | train_auc: 0.99079 | valid_auc: 0.9906  |  0:04:59s\n",
            "epoch 31 | loss: 0.10313 | train_auc: 0.99205 | valid_auc: 0.992   |  0:05:09s\n",
            "epoch 32 | loss: 0.1045  | train_auc: 0.99195 | valid_auc: 0.9919  |  0:05:18s\n",
            "epoch 33 | loss: 0.10346 | train_auc: 0.99225 | valid_auc: 0.99205 |  0:05:28s\n",
            "epoch 34 | loss: 0.10139 | train_auc: 0.99302 | valid_auc: 0.9929  |  0:05:38s\n",
            "epoch 35 | loss: 0.10102 | train_auc: 0.99283 | valid_auc: 0.99259 |  0:05:48s\n",
            "epoch 36 | loss: 0.09984 | train_auc: 0.99324 | valid_auc: 0.9931  |  0:05:58s\n",
            "epoch 37 | loss: 0.10005 | train_auc: 0.99317 | valid_auc: 0.99304 |  0:06:08s\n",
            "epoch 38 | loss: 0.10003 | train_auc: 0.99268 | valid_auc: 0.9925  |  0:06:18s\n",
            "epoch 39 | loss: 0.10021 | train_auc: 0.99284 | valid_auc: 0.9925  |  0:06:28s\n",
            "epoch 40 | loss: 0.09949 | train_auc: 0.99318 | valid_auc: 0.99287 |  0:06:38s\n",
            "epoch 41 | loss: 0.09852 | train_auc: 0.99325 | valid_auc: 0.99316 |  0:06:47s\n",
            "epoch 42 | loss: 0.09743 | train_auc: 0.99346 | valid_auc: 0.99322 |  0:06:57s\n",
            "epoch 43 | loss: 0.09673 | train_auc: 0.99353 | valid_auc: 0.99324 |  0:07:07s\n",
            "epoch 44 | loss: 0.10206 | train_auc: 0.99188 | valid_auc: 0.99146 |  0:07:16s\n",
            "epoch 45 | loss: 0.10356 | train_auc: 0.99317 | valid_auc: 0.99275 |  0:07:25s\n",
            "epoch 46 | loss: 0.09935 | train_auc: 0.9935  | valid_auc: 0.99326 |  0:07:35s\n",
            "epoch 47 | loss: 0.097   | train_auc: 0.99333 | valid_auc: 0.99304 |  0:07:44s\n",
            "epoch 48 | loss: 0.09671 | train_auc: 0.99353 | valid_auc: 0.99321 |  0:07:54s\n",
            "epoch 49 | loss: 0.09638 | train_auc: 0.99384 | valid_auc: 0.99355 |  0:08:04s\n",
            "epoch 50 | loss: 0.09492 | train_auc: 0.99387 | valid_auc: 0.9936  |  0:08:13s\n",
            "epoch 51 | loss: 0.09482 | train_auc: 0.99367 | valid_auc: 0.99331 |  0:08:23s\n",
            "epoch 52 | loss: 0.09559 | train_auc: 0.99378 | valid_auc: 0.99339 |  0:08:33s\n",
            "epoch 53 | loss: 0.09576 | train_auc: 0.99378 | valid_auc: 0.99348 |  0:08:43s\n",
            "epoch 54 | loss: 0.09428 | train_auc: 0.9938  | valid_auc: 0.99338 |  0:08:53s\n",
            "epoch 55 | loss: 0.09363 | train_auc: 0.99392 | valid_auc: 0.99364 |  0:09:03s\n",
            "epoch 56 | loss: 0.09498 | train_auc: 0.99372 | valid_auc: 0.99337 |  0:09:13s\n",
            "epoch 57 | loss: 0.0947  | train_auc: 0.99406 | valid_auc: 0.99374 |  0:09:23s\n",
            "epoch 58 | loss: 0.09397 | train_auc: 0.99394 | valid_auc: 0.99358 |  0:09:33s\n",
            "epoch 59 | loss: 0.09307 | train_auc: 0.99403 | valid_auc: 0.99369 |  0:09:43s\n",
            "epoch 60 | loss: 0.0929  | train_auc: 0.99394 | valid_auc: 0.99354 |  0:09:53s\n",
            "epoch 61 | loss: 0.09252 | train_auc: 0.99401 | valid_auc: 0.99368 |  0:10:03s\n",
            "epoch 62 | loss: 0.09312 | train_auc: 0.99405 | valid_auc: 0.99367 |  0:10:13s\n",
            "epoch 63 | loss: 0.09233 | train_auc: 0.99433 | valid_auc: 0.99393 |  0:10:22s\n",
            "epoch 64 | loss: 0.09171 | train_auc: 0.99422 | valid_auc: 0.99393 |  0:10:32s\n",
            "epoch 65 | loss: 0.09201 | train_auc: 0.99425 | valid_auc: 0.99385 |  0:10:41s\n",
            "epoch 66 | loss: 0.09201 | train_auc: 0.99403 | valid_auc: 0.99371 |  0:10:51s\n",
            "epoch 67 | loss: 0.0917  | train_auc: 0.99415 | valid_auc: 0.99374 |  0:11:00s\n",
            "epoch 68 | loss: 0.09162 | train_auc: 0.99417 | valid_auc: 0.99386 |  0:11:10s\n",
            "epoch 69 | loss: 0.09196 | train_auc: 0.99433 | valid_auc: 0.99397 |  0:11:19s\n",
            "epoch 70 | loss: 0.09119 | train_auc: 0.99408 | valid_auc: 0.99365 |  0:11:29s\n",
            "epoch 71 | loss: 0.09181 | train_auc: 0.99425 | valid_auc: 0.99396 |  0:11:38s\n",
            "epoch 72 | loss: 0.09195 | train_auc: 0.99406 | valid_auc: 0.99364 |  0:11:48s\n",
            "epoch 73 | loss: 0.09196 | train_auc: 0.99413 | valid_auc: 0.99372 |  0:11:58s\n",
            "epoch 74 | loss: 0.09112 | train_auc: 0.99426 | valid_auc: 0.99401 |  0:12:08s\n",
            "epoch 75 | loss: 0.09    | train_auc: 0.9945  | valid_auc: 0.99416 |  0:12:18s\n",
            "epoch 76 | loss: 0.08952 | train_auc: 0.9944  | valid_auc: 0.9941  |  0:12:28s\n",
            "epoch 77 | loss: 0.08888 | train_auc: 0.99444 | valid_auc: 0.99408 |  0:12:37s\n",
            "epoch 78 | loss: 0.08896 | train_auc: 0.9945  | valid_auc: 0.99412 |  0:12:47s\n",
            "epoch 79 | loss: 0.08816 | train_auc: 0.99445 | valid_auc: 0.99401 |  0:12:57s\n",
            "epoch 80 | loss: 0.0886  | train_auc: 0.99453 | valid_auc: 0.9942  |  0:13:07s\n",
            "epoch 81 | loss: 0.08972 | train_auc: 0.99453 | valid_auc: 0.99412 |  0:13:16s\n",
            "epoch 82 | loss: 0.08924 | train_auc: 0.99453 | valid_auc: 0.99424 |  0:13:26s\n",
            "epoch 83 | loss: 0.08831 | train_auc: 0.9944  | valid_auc: 0.99396 |  0:13:36s\n",
            "epoch 84 | loss: 0.08866 | train_auc: 0.99439 | valid_auc: 0.99408 |  0:13:46s\n",
            "epoch 85 | loss: 0.08811 | train_auc: 0.99458 | valid_auc: 0.99433 |  0:13:56s\n",
            "epoch 86 | loss: 0.0881  | train_auc: 0.99472 | valid_auc: 0.99435 |  0:14:05s\n",
            "epoch 87 | loss: 0.08852 | train_auc: 0.99456 | valid_auc: 0.99422 |  0:14:15s\n",
            "epoch 88 | loss: 0.08792 | train_auc: 0.99445 | valid_auc: 0.99404 |  0:14:24s\n",
            "epoch 89 | loss: 0.08765 | train_auc: 0.9947  | valid_auc: 0.99447 |  0:14:34s\n",
            "epoch 90 | loss: 0.08748 | train_auc: 0.99457 | valid_auc: 0.99427 |  0:14:43s\n",
            "epoch 91 | loss: 0.08844 | train_auc: 0.9946  | valid_auc: 0.99425 |  0:14:53s\n",
            "epoch 92 | loss: 0.08761 | train_auc: 0.99466 | valid_auc: 0.99433 |  0:15:03s\n",
            "epoch 93 | loss: 0.08713 | train_auc: 0.99465 | valid_auc: 0.99426 |  0:15:13s\n",
            "epoch 94 | loss: 0.08692 | train_auc: 0.99473 | valid_auc: 0.99442 |  0:15:22s\n",
            "epoch 95 | loss: 0.08693 | train_auc: 0.99465 | valid_auc: 0.99426 |  0:15:32s\n",
            "epoch 96 | loss: 0.08701 | train_auc: 0.99482 | valid_auc: 0.99442 |  0:15:42s\n",
            "epoch 97 | loss: 0.08701 | train_auc: 0.99485 | valid_auc: 0.99447 |  0:15:52s\n",
            "epoch 98 | loss: 0.0867  | train_auc: 0.99472 | valid_auc: 0.99438 |  0:16:02s\n",
            "epoch 99 | loss: 0.08655 | train_auc: 0.99469 | valid_auc: 0.99436 |  0:16:12s\n",
            "epoch 100| loss: 0.08681 | train_auc: 0.99479 | valid_auc: 0.99443 |  0:16:22s\n",
            "epoch 101| loss: 0.08658 | train_auc: 0.99474 | valid_auc: 0.99437 |  0:16:32s\n",
            "epoch 102| loss: 0.08592 | train_auc: 0.99489 | valid_auc: 0.99452 |  0:16:42s\n",
            "epoch 103| loss: 0.08561 | train_auc: 0.9948  | valid_auc: 0.9944  |  0:16:52s\n",
            "epoch 104| loss: 0.08675 | train_auc: 0.99493 | valid_auc: 0.99455 |  0:17:02s\n",
            "epoch 105| loss: 0.08364 | train_auc: 0.99612 | valid_auc: 0.99579 |  0:17:11s\n",
            "epoch 106| loss: 0.08    | train_auc: 0.99601 | valid_auc: 0.99559 |  0:17:21s\n",
            "epoch 107| loss: 0.07844 | train_auc: 0.99631 | valid_auc: 0.99596 |  0:17:30s\n",
            "epoch 108| loss: 0.07684 | train_auc: 0.99656 | valid_auc: 0.99621 |  0:17:40s\n",
            "epoch 109| loss: 0.07626 | train_auc: 0.99668 | valid_auc: 0.99626 |  0:17:50s\n",
            "epoch 110| loss: 0.0754  | train_auc: 0.99664 | valid_auc: 0.99632 |  0:17:59s\n",
            "epoch 111| loss: 0.07515 | train_auc: 0.99665 | valid_auc: 0.99629 |  0:18:09s\n",
            "epoch 112| loss: 0.07519 | train_auc: 0.99657 | valid_auc: 0.99622 |  0:18:19s\n",
            "epoch 113| loss: 0.07577 | train_auc: 0.99669 | valid_auc: 0.99636 |  0:18:29s\n",
            "epoch 114| loss: 0.07575 | train_auc: 0.99653 | valid_auc: 0.99612 |  0:18:39s\n",
            "epoch 115| loss: 0.07536 | train_auc: 0.99685 | valid_auc: 0.99642 |  0:18:49s\n",
            "epoch 116| loss: 0.07477 | train_auc: 0.99688 | valid_auc: 0.99651 |  0:18:59s\n",
            "epoch 117| loss: 0.07353 | train_auc: 0.99698 | valid_auc: 0.99662 |  0:19:09s\n",
            "epoch 118| loss: 0.07394 | train_auc: 0.99692 | valid_auc: 0.99652 |  0:19:19s\n",
            "epoch 119| loss: 0.07356 | train_auc: 0.99694 | valid_auc: 0.99661 |  0:19:29s\n",
            "epoch 120| loss: 0.07358 | train_auc: 0.99692 | valid_auc: 0.9966  |  0:19:39s\n",
            "epoch 121| loss: 0.07228 | train_auc: 0.99674 | valid_auc: 0.99636 |  0:19:49s\n",
            "epoch 122| loss: 0.07231 | train_auc: 0.99684 | valid_auc: 0.99651 |  0:19:59s\n",
            "epoch 123| loss: 0.07346 | train_auc: 0.99679 | valid_auc: 0.99644 |  0:20:09s\n",
            "epoch 124| loss: 0.07254 | train_auc: 0.99704 | valid_auc: 0.99669 |  0:20:19s\n",
            "epoch 125| loss: 0.07158 | train_auc: 0.99707 | valid_auc: 0.99672 |  0:20:29s\n",
            "epoch 126| loss: 0.07171 | train_auc: 0.99692 | valid_auc: 0.99659 |  0:20:38s\n",
            "epoch 127| loss: 0.07303 | train_auc: 0.99693 | valid_auc: 0.99658 |  0:20:48s\n",
            "epoch 128| loss: 0.07185 | train_auc: 0.99692 | valid_auc: 0.99653 |  0:20:57s\n",
            "epoch 129| loss: 0.07233 | train_auc: 0.99709 | valid_auc: 0.99676 |  0:21:06s\n",
            "epoch 130| loss: 0.0715  | train_auc: 0.99704 | valid_auc: 0.99672 |  0:21:16s\n",
            "epoch 131| loss: 0.07094 | train_auc: 0.99714 | valid_auc: 0.9968  |  0:21:26s\n",
            "epoch 132| loss: 0.07076 | train_auc: 0.99704 | valid_auc: 0.99668 |  0:21:35s\n",
            "epoch 133| loss: 0.07162 | train_auc: 0.99717 | valid_auc: 0.9968  |  0:21:45s\n",
            "epoch 134| loss: 0.07101 | train_auc: 0.99714 | valid_auc: 0.9968  |  0:21:55s\n",
            "epoch 135| loss: 0.07087 | train_auc: 0.99721 | valid_auc: 0.99685 |  0:22:05s\n",
            "epoch 136| loss: 0.07151 | train_auc: 0.9972  | valid_auc: 0.99685 |  0:22:15s\n",
            "epoch 137| loss: 0.07083 | train_auc: 0.99717 | valid_auc: 0.99684 |  0:22:25s\n",
            "epoch 138| loss: 0.07082 | train_auc: 0.99709 | valid_auc: 0.99676 |  0:22:34s\n",
            "epoch 139| loss: 0.07125 | train_auc: 0.99708 | valid_auc: 0.99667 |  0:22:45s\n",
            "epoch 140| loss: 0.07098 | train_auc: 0.99715 | valid_auc: 0.99682 |  0:22:54s\n",
            "epoch 141| loss: 0.07095 | train_auc: 0.99714 | valid_auc: 0.9968  |  0:23:04s\n",
            "epoch 142| loss: 0.0705  | train_auc: 0.99719 | valid_auc: 0.99679 |  0:23:14s\n",
            "epoch 143| loss: 0.07016 | train_auc: 0.99721 | valid_auc: 0.99686 |  0:23:24s\n",
            "epoch 144| loss: 0.06988 | train_auc: 0.99725 | valid_auc: 0.99687 |  0:23:34s\n",
            "epoch 145| loss: 0.06948 | train_auc: 0.99721 | valid_auc: 0.99686 |  0:23:43s\n",
            "epoch 146| loss: 0.06986 | train_auc: 0.99723 | valid_auc: 0.99687 |  0:23:53s\n",
            "epoch 147| loss: 0.07053 | train_auc: 0.99721 | valid_auc: 0.99688 |  0:24:02s\n",
            "epoch 148| loss: 0.07021 | train_auc: 0.99722 | valid_auc: 0.99685 |  0:24:12s\n",
            "epoch 149| loss: 0.06962 | train_auc: 0.99728 | valid_auc: 0.99692 |  0:24:21s\n",
            "epoch 150| loss: 0.06995 | train_auc: 0.99725 | valid_auc: 0.99686 |  0:24:30s\n",
            "epoch 151| loss: 0.06976 | train_auc: 0.99723 | valid_auc: 0.99686 |  0:24:40s\n",
            "epoch 152| loss: 0.07021 | train_auc: 0.99711 | valid_auc: 0.99673 |  0:24:50s\n",
            "epoch 153| loss: 0.06953 | train_auc: 0.99723 | valid_auc: 0.99689 |  0:25:00s\n",
            "epoch 154| loss: 0.0694  | train_auc: 0.99724 | valid_auc: 0.99684 |  0:25:09s\n",
            "epoch 155| loss: 0.06955 | train_auc: 0.99725 | valid_auc: 0.99684 |  0:25:19s\n",
            "epoch 156| loss: 0.06939 | train_auc: 0.99723 | valid_auc: 0.99684 |  0:25:29s\n",
            "epoch 157| loss: 0.06908 | train_auc: 0.99729 | valid_auc: 0.9969  |  0:25:39s\n",
            "epoch 158| loss: 0.06922 | train_auc: 0.99729 | valid_auc: 0.99691 |  0:25:48s\n",
            "epoch 159| loss: 0.06905 | train_auc: 0.99723 | valid_auc: 0.99684 |  0:25:58s\n",
            "epoch 160| loss: 0.07017 | train_auc: 0.99725 | valid_auc: 0.99687 |  0:26:08s\n",
            "epoch 161| loss: 0.06963 | train_auc: 0.99732 | valid_auc: 0.9969  |  0:26:18s\n",
            "epoch 162| loss: 0.06881 | train_auc: 0.99729 | valid_auc: 0.99693 |  0:26:28s\n",
            "epoch 163| loss: 0.06858 | train_auc: 0.99729 | valid_auc: 0.99693 |  0:26:38s\n",
            "epoch 164| loss: 0.06891 | train_auc: 0.99721 | valid_auc: 0.99679 |  0:26:47s\n",
            "epoch 165| loss: 0.06975 | train_auc: 0.99731 | valid_auc: 0.99689 |  0:26:56s\n",
            "epoch 166| loss: 0.06898 | train_auc: 0.99725 | valid_auc: 0.99687 |  0:27:06s\n",
            "epoch 167| loss: 0.06922 | train_auc: 0.99733 | valid_auc: 0.99693 |  0:27:15s\n",
            "epoch 168| loss: 0.06834 | train_auc: 0.99733 | valid_auc: 0.9969  |  0:27:25s\n",
            "epoch 169| loss: 0.06828 | train_auc: 0.99727 | valid_auc: 0.99686 |  0:27:34s\n",
            "epoch 170| loss: 0.06889 | train_auc: 0.99732 | valid_auc: 0.99689 |  0:27:44s\n",
            "epoch 171| loss: 0.06834 | train_auc: 0.9973  | valid_auc: 0.99685 |  0:27:54s\n",
            "epoch 172| loss: 0.06832 | train_auc: 0.99735 | valid_auc: 0.9969  |  0:28:04s\n",
            "epoch 173| loss: 0.06804 | train_auc: 0.99737 | valid_auc: 0.99699 |  0:28:14s\n",
            "epoch 174| loss: 0.06849 | train_auc: 0.99736 | valid_auc: 0.99695 |  0:28:24s\n",
            "epoch 175| loss: 0.06851 | train_auc: 0.99737 | valid_auc: 0.99695 |  0:28:34s\n",
            "epoch 176| loss: 0.06837 | train_auc: 0.99737 | valid_auc: 0.99695 |  0:28:44s\n",
            "epoch 177| loss: 0.06816 | train_auc: 0.99733 | valid_auc: 0.9969  |  0:28:54s\n",
            "epoch 178| loss: 0.0682  | train_auc: 0.99737 | valid_auc: 0.99697 |  0:29:04s\n",
            "epoch 179| loss: 0.06814 | train_auc: 0.99738 | valid_auc: 0.99698 |  0:29:14s\n",
            "epoch 180| loss: 0.0683  | train_auc: 0.99735 | valid_auc: 0.99692 |  0:29:24s\n",
            "epoch 181| loss: 0.0679  | train_auc: 0.99735 | valid_auc: 0.99696 |  0:29:33s\n",
            "epoch 182| loss: 0.06842 | train_auc: 0.99731 | valid_auc: 0.9969  |  0:29:43s\n",
            "epoch 183| loss: 0.06833 | train_auc: 0.99735 | valid_auc: 0.99694 |  0:29:53s\n",
            "epoch 184| loss: 0.06825 | train_auc: 0.99725 | valid_auc: 0.99675 |  0:30:03s\n",
            "epoch 185| loss: 0.06809 | train_auc: 0.99736 | valid_auc: 0.99693 |  0:30:13s\n",
            "epoch 186| loss: 0.0679  | train_auc: 0.99745 | valid_auc: 0.99702 |  0:30:22s\n",
            "epoch 187| loss: 0.06734 | train_auc: 0.99739 | valid_auc: 0.99693 |  0:30:32s\n",
            "epoch 188| loss: 0.06767 | train_auc: 0.99739 | valid_auc: 0.99699 |  0:30:41s\n",
            "epoch 189| loss: 0.06748 | train_auc: 0.99742 | valid_auc: 0.99699 |  0:30:50s\n",
            "epoch 190| loss: 0.06757 | train_auc: 0.99741 | valid_auc: 0.99694 |  0:31:00s\n",
            "epoch 191| loss: 0.06711 | train_auc: 0.99742 | valid_auc: 0.99698 |  0:31:10s\n",
            "epoch 192| loss: 0.06725 | train_auc: 0.99734 | valid_auc: 0.99689 |  0:31:19s\n",
            "epoch 193| loss: 0.06732 | train_auc: 0.99742 | valid_auc: 0.99697 |  0:31:28s\n",
            "epoch 194| loss: 0.06741 | train_auc: 0.9974  | valid_auc: 0.99696 |  0:31:38s\n",
            "epoch 195| loss: 0.06722 | train_auc: 0.9974  | valid_auc: 0.99695 |  0:31:48s\n",
            "epoch 196| loss: 0.06704 | train_auc: 0.99745 | valid_auc: 0.99705 |  0:31:58s\n",
            "epoch 197| loss: 0.06688 | train_auc: 0.99746 | valid_auc: 0.99702 |  0:32:07s\n",
            "epoch 198| loss: 0.06695 | train_auc: 0.99743 | valid_auc: 0.99698 |  0:32:17s\n",
            "epoch 199| loss: 0.06697 | train_auc: 0.9974  | valid_auc: 0.99694 |  0:32:27s\n",
            "epoch 200| loss: 0.06717 | train_auc: 0.99742 | valid_auc: 0.997   |  0:32:37s\n",
            "epoch 201| loss: 0.06656 | train_auc: 0.99746 | valid_auc: 0.99702 |  0:32:47s\n",
            "epoch 202| loss: 0.06661 | train_auc: 0.99744 | valid_auc: 0.99702 |  0:32:57s\n",
            "epoch 203| loss: 0.06697 | train_auc: 0.99744 | valid_auc: 0.99702 |  0:33:07s\n",
            "epoch 204| loss: 0.06708 | train_auc: 0.99746 | valid_auc: 0.99705 |  0:33:17s\n",
            "epoch 205| loss: 0.06681 | train_auc: 0.99741 | valid_auc: 0.99695 |  0:33:27s\n",
            "epoch 206| loss: 0.06727 | train_auc: 0.99746 | valid_auc: 0.99704 |  0:33:36s\n",
            "epoch 207| loss: 0.06693 | train_auc: 0.99747 | valid_auc: 0.99704 |  0:33:46s\n",
            "epoch 208| loss: 0.06667 | train_auc: 0.99748 | valid_auc: 0.99705 |  0:33:55s\n",
            "epoch 209| loss: 0.06661 | train_auc: 0.99747 | valid_auc: 0.99702 |  0:34:05s\n",
            "epoch 210| loss: 0.06642 | train_auc: 0.99747 | valid_auc: 0.997   |  0:34:14s\n",
            "epoch 211| loss: 0.06638 | train_auc: 0.99746 | valid_auc: 0.99702 |  0:34:24s\n",
            "epoch 212| loss: 0.06638 | train_auc: 0.99748 | valid_auc: 0.99706 |  0:34:33s\n",
            "epoch 213| loss: 0.06662 | train_auc: 0.99748 | valid_auc: 0.99704 |  0:34:43s\n",
            "epoch 214| loss: 0.06645 | train_auc: 0.9975  | valid_auc: 0.99704 |  0:34:52s\n",
            "epoch 215| loss: 0.06615 | train_auc: 0.99748 | valid_auc: 0.99706 |  0:35:02s\n",
            "epoch 216| loss: 0.06636 | train_auc: 0.99749 | valid_auc: 0.99707 |  0:35:12s\n",
            "epoch 217| loss: 0.06696 | train_auc: 0.99747 | valid_auc: 0.99705 |  0:35:22s\n",
            "epoch 218| loss: 0.06631 | train_auc: 0.99751 | valid_auc: 0.99707 |  0:35:31s\n",
            "epoch 219| loss: 0.06647 | train_auc: 0.99748 | valid_auc: 0.99708 |  0:35:41s\n",
            "epoch 220| loss: 0.06649 | train_auc: 0.99751 | valid_auc: 0.99707 |  0:35:51s\n",
            "epoch 221| loss: 0.06634 | train_auc: 0.99748 | valid_auc: 0.99704 |  0:36:01s\n",
            "epoch 222| loss: 0.06618 | train_auc: 0.99752 | valid_auc: 0.99709 |  0:36:11s\n",
            "epoch 223| loss: 0.06596 | train_auc: 0.9975  | valid_auc: 0.99707 |  0:36:20s\n",
            "epoch 224| loss: 0.06606 | train_auc: 0.99749 | valid_auc: 0.99702 |  0:36:30s\n",
            "epoch 225| loss: 0.06589 | train_auc: 0.99753 | valid_auc: 0.99707 |  0:36:40s\n",
            "epoch 226| loss: 0.066   | train_auc: 0.99753 | valid_auc: 0.9971  |  0:36:50s\n",
            "epoch 227| loss: 0.06588 | train_auc: 0.99748 | valid_auc: 0.99701 |  0:37:00s\n",
            "epoch 228| loss: 0.06671 | train_auc: 0.99747 | valid_auc: 0.99701 |  0:37:09s\n",
            "epoch 229| loss: 0.0665  | train_auc: 0.99751 | valid_auc: 0.99707 |  0:37:19s\n",
            "epoch 230| loss: 0.06578 | train_auc: 0.99743 | valid_auc: 0.99699 |  0:37:29s\n",
            "epoch 231| loss: 0.0663  | train_auc: 0.99751 | valid_auc: 0.99705 |  0:37:38s\n",
            "epoch 232| loss: 0.06618 | train_auc: 0.9975  | valid_auc: 0.99711 |  0:37:48s\n",
            "epoch 233| loss: 0.0663  | train_auc: 0.99752 | valid_auc: 0.99705 |  0:37:58s\n",
            "epoch 234| loss: 0.06633 | train_auc: 0.99752 | valid_auc: 0.99708 |  0:38:07s\n",
            "epoch 235| loss: 0.06613 | train_auc: 0.99751 | valid_auc: 0.99705 |  0:38:16s\n",
            "epoch 236| loss: 0.06606 | train_auc: 0.99749 | valid_auc: 0.99705 |  0:38:26s\n",
            "epoch 237| loss: 0.06583 | train_auc: 0.99753 | valid_auc: 0.99706 |  0:38:36s\n",
            "epoch 238| loss: 0.06558 | train_auc: 0.99753 | valid_auc: 0.99707 |  0:38:46s\n",
            "epoch 239| loss: 0.06568 | train_auc: 0.99755 | valid_auc: 0.99711 |  0:38:56s\n",
            "epoch 240| loss: 0.06589 | train_auc: 0.99753 | valid_auc: 0.99709 |  0:39:06s\n",
            "epoch 241| loss: 0.06569 | train_auc: 0.99755 | valid_auc: 0.99711 |  0:39:16s\n",
            "epoch 242| loss: 0.06577 | train_auc: 0.99753 | valid_auc: 0.9971  |  0:39:26s\n",
            "epoch 243| loss: 0.06604 | train_auc: 0.99751 | valid_auc: 0.99701 |  0:39:36s\n",
            "epoch 244| loss: 0.06549 | train_auc: 0.99751 | valid_auc: 0.99707 |  0:39:46s\n",
            "epoch 245| loss: 0.06545 | train_auc: 0.99754 | valid_auc: 0.9971  |  0:39:56s\n",
            "epoch 246| loss: 0.0654  | train_auc: 0.99755 | valid_auc: 0.99712 |  0:40:06s\n",
            "epoch 247| loss: 0.06551 | train_auc: 0.99757 | valid_auc: 0.9971  |  0:40:16s\n",
            "epoch 248| loss: 0.06565 | train_auc: 0.99754 | valid_auc: 0.9971  |  0:40:26s\n",
            "epoch 249| loss: 0.06566 | train_auc: 0.99753 | valid_auc: 0.99708 |  0:40:35s\n",
            "epoch 250| loss: 0.06549 | train_auc: 0.99753 | valid_auc: 0.99709 |  0:40:45s\n",
            "epoch 251| loss: 0.06589 | train_auc: 0.99751 | valid_auc: 0.99707 |  0:40:55s\n",
            "epoch 252| loss: 0.0662  | train_auc: 0.99752 | valid_auc: 0.99708 |  0:41:04s\n",
            "epoch 253| loss: 0.06563 | train_auc: 0.99756 | valid_auc: 0.99713 |  0:41:14s\n",
            "epoch 254| loss: 0.06531 | train_auc: 0.99756 | valid_auc: 0.99713 |  0:41:23s\n",
            "epoch 255| loss: 0.06555 | train_auc: 0.99749 | valid_auc: 0.99705 |  0:41:33s\n",
            "epoch 256| loss: 0.06554 | train_auc: 0.99758 | valid_auc: 0.99714 |  0:41:43s\n",
            "epoch 257| loss: 0.06562 | train_auc: 0.99756 | valid_auc: 0.99712 |  0:41:52s\n",
            "epoch 258| loss: 0.0658  | train_auc: 0.99754 | valid_auc: 0.9971  |  0:42:02s\n",
            "epoch 259| loss: 0.0662  | train_auc: 0.99757 | valid_auc: 0.99714 |  0:42:11s\n",
            "epoch 260| loss: 0.06542 | train_auc: 0.99753 | valid_auc: 0.99707 |  0:42:22s\n",
            "epoch 261| loss: 0.06526 | train_auc: 0.99758 | valid_auc: 0.99713 |  0:42:31s\n",
            "epoch 262| loss: 0.0651  | train_auc: 0.99759 | valid_auc: 0.99714 |  0:42:41s\n",
            "epoch 263| loss: 0.06527 | train_auc: 0.9976  | valid_auc: 0.99716 |  0:42:51s\n",
            "epoch 264| loss: 0.0649  | train_auc: 0.99757 | valid_auc: 0.9971  |  0:43:01s\n",
            "epoch 265| loss: 0.06486 | train_auc: 0.99758 | valid_auc: 0.9971  |  0:43:11s\n",
            "epoch 266| loss: 0.06479 | train_auc: 0.99756 | valid_auc: 0.99713 |  0:43:21s\n",
            "epoch 267| loss: 0.06514 | train_auc: 0.9976  | valid_auc: 0.99714 |  0:43:31s\n",
            "epoch 268| loss: 0.06492 | train_auc: 0.9976  | valid_auc: 0.99714 |  0:43:41s\n",
            "epoch 269| loss: 0.06511 | train_auc: 0.99757 | valid_auc: 0.99713 |  0:43:51s\n",
            "epoch 270| loss: 0.06505 | train_auc: 0.99758 | valid_auc: 0.99712 |  0:44:00s\n",
            "epoch 271| loss: 0.06481 | train_auc: 0.99759 | valid_auc: 0.99716 |  0:44:10s\n",
            "epoch 272| loss: 0.06479 | train_auc: 0.99759 | valid_auc: 0.99711 |  0:44:20s\n",
            "epoch 273| loss: 0.06482 | train_auc: 0.9976  | valid_auc: 0.99718 |  0:44:30s\n",
            "epoch 274| loss: 0.06458 | train_auc: 0.9976  | valid_auc: 0.99718 |  0:44:40s\n",
            "epoch 275| loss: 0.06496 | train_auc: 0.9976  | valid_auc: 0.99716 |  0:44:50s\n",
            "epoch 276| loss: 0.06513 | train_auc: 0.99762 | valid_auc: 0.99717 |  0:44:59s\n",
            "epoch 277| loss: 0.06452 | train_auc: 0.99763 | valid_auc: 0.99719 |  0:45:09s\n",
            "epoch 278| loss: 0.06519 | train_auc: 0.99761 | valid_auc: 0.99715 |  0:45:18s\n",
            "epoch 279| loss: 0.06489 | train_auc: 0.99756 | valid_auc: 0.99713 |  0:45:28s\n",
            "epoch 280| loss: 0.06507 | train_auc: 0.9976  | valid_auc: 0.99716 |  0:45:38s\n",
            "epoch 281| loss: 0.06487 | train_auc: 0.99761 | valid_auc: 0.99715 |  0:45:47s\n",
            "epoch 282| loss: 0.06466 | train_auc: 0.9976  | valid_auc: 0.99713 |  0:45:57s\n",
            "epoch 283| loss: 0.06447 | train_auc: 0.99762 | valid_auc: 0.99715 |  0:46:07s\n",
            "epoch 284| loss: 0.06437 | train_auc: 0.99763 | valid_auc: 0.9972  |  0:46:17s\n",
            "epoch 285| loss: 0.06464 | train_auc: 0.99762 | valid_auc: 0.99718 |  0:46:27s\n",
            "epoch 286| loss: 0.06441 | train_auc: 0.99762 | valid_auc: 0.99716 |  0:46:36s\n",
            "epoch 287| loss: 0.06471 | train_auc: 0.99758 | valid_auc: 0.99712 |  0:46:46s\n",
            "epoch 288| loss: 0.06456 | train_auc: 0.99758 | valid_auc: 0.99712 |  0:46:56s\n",
            "epoch 289| loss: 0.06435 | train_auc: 0.99763 | valid_auc: 0.99719 |  0:47:06s\n",
            "epoch 290| loss: 0.06453 | train_auc: 0.99763 | valid_auc: 0.99716 |  0:47:16s\n",
            "epoch 291| loss: 0.06432 | train_auc: 0.99763 | valid_auc: 0.99718 |  0:47:26s\n",
            "epoch 292| loss: 0.06453 | train_auc: 0.99762 | valid_auc: 0.99716 |  0:47:36s\n",
            "epoch 293| loss: 0.06422 | train_auc: 0.99764 | valid_auc: 0.99718 |  0:47:45s\n",
            "epoch 294| loss: 0.06456 | train_auc: 0.99764 | valid_auc: 0.99716 |  0:47:55s\n",
            "epoch 295| loss: 0.06428 | train_auc: 0.99765 | valid_auc: 0.99717 |  0:48:05s\n",
            "epoch 296| loss: 0.06435 | train_auc: 0.99763 | valid_auc: 0.99716 |  0:48:15s\n",
            "epoch 297| loss: 0.06436 | train_auc: 0.99761 | valid_auc: 0.99712 |  0:48:25s\n",
            "epoch 298| loss: 0.0642  | train_auc: 0.9976  | valid_auc: 0.99713 |  0:48:34s\n",
            "epoch 299| loss: 0.06429 | train_auc: 0.99759 | valid_auc: 0.99711 |  0:48:44s\n",
            "epoch 300| loss: 0.06441 | train_auc: 0.99764 | valid_auc: 0.99717 |  0:48:53s\n",
            "epoch 301| loss: 0.06436 | train_auc: 0.99764 | valid_auc: 0.99717 |  0:49:03s\n",
            "epoch 302| loss: 0.06444 | train_auc: 0.99764 | valid_auc: 0.99716 |  0:49:12s\n",
            "epoch 303| loss: 0.06431 | train_auc: 0.99765 | valid_auc: 0.99717 |  0:49:22s\n",
            "epoch 304| loss: 0.06433 | train_auc: 0.99765 | valid_auc: 0.99715 |  0:49:32s\n",
            "\n",
            "Early stopping occurred at epoch 304 with best_epoch = 284 and best_valid_auc = 0.9972\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9971959636023487\n",
            "----- 9 -----\n",
            "unsupervised_model\n",
            "Device used : cuda\n",
            "epoch 0  | loss: 17901722.02645| val_0_unsup_loss: 12959.74121|  0:00:05s\n",
            "epoch 1  | loss: 8876.30607| val_0_unsup_loss: 2.80173 |  0:00:11s\n",
            "epoch 2  | loss: 25.21882| val_0_unsup_loss: 2.08877 |  0:00:17s\n",
            "epoch 3  | loss: 2.43199 | val_0_unsup_loss: 1.70008 |  0:00:24s\n",
            "epoch 4  | loss: 4.2841  | val_0_unsup_loss: 1.52665 |  0:00:30s\n",
            "epoch 5  | loss: 1.90566 | val_0_unsup_loss: 1.45021 |  0:00:36s\n",
            "epoch 6  | loss: 1.88671 | val_0_unsup_loss: 1.40724 |  0:00:42s\n",
            "epoch 7  | loss: 1.8307  | val_0_unsup_loss: 1.3837  |  0:00:48s\n",
            "epoch 8  | loss: 1.37829 | val_0_unsup_loss: 1.37385 |  0:00:54s\n",
            "epoch 9  | loss: 1.37231 | val_0_unsup_loss: 1.36838 |  0:01:00s\n",
            "epoch 10 | loss: 1.41582 | val_0_unsup_loss: 1.36468 |  0:01:07s\n",
            "epoch 11 | loss: 2.03738 | val_0_unsup_loss: 1.36258 |  0:01:13s\n",
            "epoch 12 | loss: 1.36166 | val_0_unsup_loss: 1.3612  |  0:01:19s\n",
            "epoch 13 | loss: 4.47868 | val_0_unsup_loss: 1.36017 |  0:01:25s\n",
            "epoch 14 | loss: 1.35913 | val_0_unsup_loss: 1.35936 |  0:01:31s\n",
            "epoch 15 | loss: 1.35808 | val_0_unsup_loss: 1.35872 |  0:01:37s\n",
            "epoch 16 | loss: 1.75635 | val_0_unsup_loss: 1.35848 |  0:01:43s\n",
            "epoch 17 | loss: 1.35724 | val_0_unsup_loss: 1.35802 |  0:01:49s\n",
            "epoch 18 | loss: 1.3573  | val_0_unsup_loss: 1.35759 |  0:01:56s\n",
            "epoch 19 | loss: 1.35738 | val_0_unsup_loss: 1.35727 |  0:02:02s\n",
            "epoch 20 | loss: 1.35605 | val_0_unsup_loss: 1.35696 |  0:02:08s\n",
            "epoch 21 | loss: 4.03809 | val_0_unsup_loss: 1.35707 |  0:02:14s\n",
            "epoch 22 | loss: 1.35637 | val_0_unsup_loss: 1.35682 |  0:02:20s\n",
            "epoch 23 | loss: 1.35559 | val_0_unsup_loss: 1.35642 |  0:02:26s\n",
            "epoch 24 | loss: 1.77264 | val_0_unsup_loss: 1.35615 |  0:02:32s\n",
            "epoch 25 | loss: 1.45043 | val_0_unsup_loss: 1.35618 |  0:02:38s\n",
            "epoch 26 | loss: 1.37429 | val_0_unsup_loss: 1.35619 |  0:02:44s\n",
            "epoch 27 | loss: 1.35564 | val_0_unsup_loss: 1.35597 |  0:02:50s\n",
            "epoch 28 | loss: 1.3553  | val_0_unsup_loss: 1.35572 |  0:02:56s\n",
            "epoch 29 | loss: 1.35501 | val_0_unsup_loss: 1.35554 |  0:03:02s\n",
            "epoch 30 | loss: 1.35435 | val_0_unsup_loss: 1.35541 |  0:03:08s\n",
            "epoch 31 | loss: 1.35441 | val_0_unsup_loss: 1.35531 |  0:03:14s\n",
            "epoch 32 | loss: 1.35889 | val_0_unsup_loss: 1.35525 |  0:03:20s\n",
            "epoch 33 | loss: 1.43433 | val_0_unsup_loss: 1.35532 |  0:03:26s\n",
            "epoch 34 | loss: 1.36642 | val_0_unsup_loss: 1.35526 |  0:03:32s\n",
            "epoch 35 | loss: 1.3779  | val_0_unsup_loss: 1.35545 |  0:03:38s\n",
            "epoch 36 | loss: 1.35515 | val_0_unsup_loss: 1.35559 |  0:03:45s\n",
            "epoch 37 | loss: 1.35472 | val_0_unsup_loss: 1.35527 |  0:03:51s\n",
            "epoch 38 | loss: 1.35427 | val_0_unsup_loss: 1.35509 |  0:03:57s\n",
            "epoch 39 | loss: 1.3541  | val_0_unsup_loss: 1.35502 |  0:04:03s\n",
            "epoch 40 | loss: 1.35424 | val_0_unsup_loss: 1.35499 |  0:04:09s\n",
            "epoch 41 | loss: 1.35389 | val_0_unsup_loss: 1.35496 |  0:04:15s\n",
            "epoch 42 | loss: 1.35426 | val_0_unsup_loss: 1.35495 |  0:04:21s\n",
            "epoch 43 | loss: 1.35418 | val_0_unsup_loss: 1.35493 |  0:04:27s\n",
            "epoch 44 | loss: 1.38078 | val_0_unsup_loss: 1.35509 |  0:04:33s\n",
            "epoch 45 | loss: 1.35429 | val_0_unsup_loss: 1.35497 |  0:04:40s\n",
            "epoch 46 | loss: 1.35366 | val_0_unsup_loss: 1.35493 |  0:04:46s\n",
            "epoch 47 | loss: 1.35415 | val_0_unsup_loss: 1.35491 |  0:04:52s\n",
            "epoch 48 | loss: 1.35431 | val_0_unsup_loss: 1.3549  |  0:04:58s\n",
            "epoch 49 | loss: 1.42705 | val_0_unsup_loss: 1.35582 |  0:05:04s\n",
            "epoch 50 | loss: 1.35494 | val_0_unsup_loss: 1.35541 |  0:05:10s\n",
            "epoch 51 | loss: 1.40005 | val_0_unsup_loss: 1.35512 |  0:05:16s\n",
            "epoch 52 | loss: 1.35495 | val_0_unsup_loss: 1.35577 |  0:05:22s\n",
            "epoch 53 | loss: 1.35428 | val_0_unsup_loss: 1.35503 |  0:05:28s\n",
            "epoch 54 | loss: 1.35406 | val_0_unsup_loss: 1.35492 |  0:05:35s\n",
            "epoch 55 | loss: 1.35414 | val_0_unsup_loss: 1.35491 |  0:05:41s\n",
            "epoch 56 | loss: 1.35432 | val_0_unsup_loss: 1.3549  |  0:05:47s\n",
            "epoch 57 | loss: 1.35385 | val_0_unsup_loss: 1.35489 |  0:05:53s\n",
            "epoch 58 | loss: 1.35383 | val_0_unsup_loss: 1.35489 |  0:05:59s\n",
            "epoch 59 | loss: 2.38115 | val_0_unsup_loss: 1.35496 |  0:06:05s\n",
            "epoch 60 | loss: 1.35404 | val_0_unsup_loss: 1.35502 |  0:06:11s\n",
            "epoch 61 | loss: 1.35408 | val_0_unsup_loss: 1.35494 |  0:06:17s\n",
            "epoch 62 | loss: 1.35375 | val_0_unsup_loss: 1.35491 |  0:06:24s\n",
            "epoch 63 | loss: 1.3544  | val_0_unsup_loss: 1.3549  |  0:06:30s\n",
            "epoch 64 | loss: 1.35427 | val_0_unsup_loss: 1.3549  |  0:06:36s\n",
            "epoch 65 | loss: 1.35405 | val_0_unsup_loss: 1.35518 |  0:06:42s\n",
            "epoch 66 | loss: 1.35466 | val_0_unsup_loss: 1.35532 |  0:06:48s\n",
            "epoch 67 | loss: 1.35428 | val_0_unsup_loss: 1.35499 |  0:06:54s\n",
            "epoch 68 | loss: 1.35391 | val_0_unsup_loss: 1.3549  |  0:07:00s\n",
            "epoch 69 | loss: 1.35408 | val_0_unsup_loss: 1.3549  |  0:07:06s\n",
            "epoch 70 | loss: 1.35425 | val_0_unsup_loss: 1.3549  |  0:07:12s\n",
            "epoch 71 | loss: 1.35381 | val_0_unsup_loss: 1.35489 |  0:07:18s\n",
            "epoch 72 | loss: 1.3542  | val_0_unsup_loss: 1.35489 |  0:07:24s\n",
            "epoch 73 | loss: 1.35437 | val_0_unsup_loss: 1.35489 |  0:07:31s\n",
            "epoch 74 | loss: 1.35408 | val_0_unsup_loss: 1.35489 |  0:07:37s\n",
            "epoch 75 | loss: 1.35394 | val_0_unsup_loss: 1.35489 |  0:07:43s\n",
            "epoch 76 | loss: 1.35414 | val_0_unsup_loss: 1.35489 |  0:07:49s\n",
            "epoch 77 | loss: 1.35406 | val_0_unsup_loss: 1.35489 |  0:07:55s\n",
            "epoch 78 | loss: 1.35413 | val_0_unsup_loss: 1.35489 |  0:08:01s\n",
            "epoch 79 | loss: 1.40255 | val_0_unsup_loss: 1.35514 |  0:08:07s\n",
            "epoch 80 | loss: 1.35429 | val_0_unsup_loss: 1.35494 |  0:08:13s\n",
            "epoch 81 | loss: 45.64629| val_0_unsup_loss: 1.3551  |  0:08:19s\n",
            "epoch 82 | loss: 1.35433 | val_0_unsup_loss: 1.35502 |  0:08:25s\n",
            "epoch 83 | loss: 1.35408 | val_0_unsup_loss: 1.35493 |  0:08:31s\n",
            "epoch 84 | loss: 1.35417 | val_0_unsup_loss: 1.3549  |  0:08:37s\n",
            "epoch 85 | loss: 1.35429 | val_0_unsup_loss: 1.3549  |  0:08:44s\n",
            "epoch 86 | loss: 1.35396 | val_0_unsup_loss: 1.3549  |  0:08:50s\n",
            "epoch 87 | loss: 1.35445 | val_0_unsup_loss: 1.35489 |  0:08:56s\n",
            "epoch 88 | loss: 1.3544  | val_0_unsup_loss: 1.3549  |  0:09:02s\n",
            "epoch 89 | loss: 1.3541  | val_0_unsup_loss: 1.35489 |  0:09:08s\n",
            "epoch 90 | loss: 1.3538  | val_0_unsup_loss: 1.35489 |  0:09:14s\n",
            "epoch 91 | loss: 1.35393 | val_0_unsup_loss: 1.35489 |  0:09:20s\n",
            "epoch 92 | loss: 1.35398 | val_0_unsup_loss: 1.35489 |  0:09:26s\n",
            "epoch 93 | loss: 1.35413 | val_0_unsup_loss: 1.35489 |  0:09:33s\n",
            "epoch 94 | loss: 1.35386 | val_0_unsup_loss: 1.35489 |  0:09:39s\n",
            "epoch 95 | loss: 1.35365 | val_0_unsup_loss: 1.35489 |  0:09:45s\n",
            "epoch 96 | loss: 1.35398 | val_0_unsup_loss: 1.35489 |  0:09:51s\n",
            "epoch 97 | loss: 1.35423 | val_0_unsup_loss: 1.35489 |  0:09:57s\n",
            "epoch 98 | loss: 1.35397 | val_0_unsup_loss: 1.35489 |  0:10:03s\n",
            "epoch 99 | loss: 1.35406 | val_0_unsup_loss: 1.35489 |  0:10:09s\n",
            "epoch 100| loss: 1.35449 | val_0_unsup_loss: 1.35489 |  0:10:15s\n",
            "epoch 101| loss: 1.35399 | val_0_unsup_loss: 1.35489 |  0:10:21s\n",
            "epoch 102| loss: 1.35419 | val_0_unsup_loss: 1.35489 |  0:10:28s\n",
            "epoch 103| loss: 1.35411 | val_0_unsup_loss: 1.35489 |  0:10:34s\n",
            "epoch 104| loss: 1.35426 | val_0_unsup_loss: 1.35489 |  0:10:40s\n",
            "epoch 105| loss: 1.35442 | val_0_unsup_loss: 1.35489 |  0:10:46s\n",
            "epoch 106| loss: 1.3543  | val_0_unsup_loss: 1.35489 |  0:10:52s\n",
            "epoch 107| loss: 1.35418 | val_0_unsup_loss: 1.35489 |  0:10:58s\n",
            "epoch 108| loss: 1.35426 | val_0_unsup_loss: 1.35489 |  0:11:04s\n",
            "epoch 109| loss: 1.3542  | val_0_unsup_loss: 1.3549  |  0:11:10s\n",
            "\n",
            "Early stopping occurred at epoch 109 with best_epoch = 89 and best_val_0_unsup_loss = 1.35489\n",
            "Best weights from best epoch are automatically used!\n",
            "main model\n",
            "Device used : cuda\n",
            "Loading weights from unsupervised pretraining\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_a changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_d changed from 64 to 8\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_independent changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_shared changed from 3 to 2\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: n_steps changed from 5 to 3\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.6932  | train_auc: 0.52224 | valid_auc: 0.52224 |  0:00:09s\n",
            "epoch 1  | loss: 0.65872 | train_auc: 0.63761 | valid_auc: 0.63712 |  0:00:20s\n",
            "epoch 2  | loss: 0.43798 | train_auc: 0.83925 | valid_auc: 0.83856 |  0:00:29s\n",
            "epoch 3  | loss: 0.29705 | train_auc: 0.9066  | valid_auc: 0.90558 |  0:00:39s\n",
            "epoch 4  | loss: 0.25598 | train_auc: 0.92337 | valid_auc: 0.92348 |  0:00:48s\n",
            "epoch 5  | loss: 0.2301  | train_auc: 0.95802 | valid_auc: 0.95788 |  0:00:58s\n",
            "epoch 6  | loss: 0.20735 | train_auc: 0.95387 | valid_auc: 0.95401 |  0:01:08s\n",
            "epoch 7  | loss: 0.18015 | train_auc: 0.9561  | valid_auc: 0.95629 |  0:01:18s\n",
            "epoch 8  | loss: 0.16587 | train_auc: 0.97559 | valid_auc: 0.97586 |  0:01:28s\n",
            "epoch 9  | loss: 0.15481 | train_auc: 0.97998 | valid_auc: 0.97965 |  0:01:39s\n",
            "epoch 10 | loss: 0.14325 | train_auc: 0.98286 | valid_auc: 0.98245 |  0:01:49s\n",
            "epoch 11 | loss: 0.1358  | train_auc: 0.98451 | valid_auc: 0.98424 |  0:01:59s\n",
            "epoch 12 | loss: 0.13274 | train_auc: 0.98701 | valid_auc: 0.98666 |  0:02:09s\n",
            "epoch 13 | loss: 0.12983 | train_auc: 0.98686 | valid_auc: 0.98658 |  0:02:18s\n",
            "epoch 14 | loss: 0.12624 | train_auc: 0.98499 | valid_auc: 0.9847  |  0:02:29s\n",
            "epoch 15 | loss: 0.12301 | train_auc: 0.98582 | valid_auc: 0.98554 |  0:02:39s\n",
            "epoch 16 | loss: 0.12006 | train_auc: 0.9866  | valid_auc: 0.98636 |  0:02:48s\n",
            "epoch 17 | loss: 0.11848 | train_auc: 0.98491 | valid_auc: 0.98471 |  0:02:58s\n",
            "epoch 18 | loss: 0.11522 | train_auc: 0.98454 | valid_auc: 0.98445 |  0:03:08s\n",
            "epoch 19 | loss: 0.11366 | train_auc: 0.98963 | valid_auc: 0.98949 |  0:03:18s\n",
            "epoch 20 | loss: 0.11178 | train_auc: 0.99125 | valid_auc: 0.99099 |  0:03:28s\n",
            "epoch 21 | loss: 0.11098 | train_auc: 0.98928 | valid_auc: 0.98905 |  0:03:38s\n",
            "epoch 22 | loss: 0.10805 | train_auc: 0.9922  | valid_auc: 0.99195 |  0:03:48s\n",
            "epoch 23 | loss: 0.10613 | train_auc: 0.99174 | valid_auc: 0.9914  |  0:03:58s\n",
            "epoch 24 | loss: 0.10481 | train_auc: 0.99231 | valid_auc: 0.99195 |  0:04:07s\n",
            "epoch 25 | loss: 0.1057  | train_auc: 0.99263 | valid_auc: 0.99229 |  0:04:17s\n",
            "epoch 26 | loss: 0.10393 | train_auc: 0.99346 | valid_auc: 0.99311 |  0:04:27s\n",
            "epoch 27 | loss: 0.10185 | train_auc: 0.99341 | valid_auc: 0.99309 |  0:04:37s\n",
            "epoch 28 | loss: 0.10043 | train_auc: 0.99358 | valid_auc: 0.99319 |  0:04:46s\n",
            "epoch 29 | loss: 0.09953 | train_auc: 0.99413 | valid_auc: 0.99375 |  0:04:56s\n",
            "epoch 30 | loss: 0.09955 | train_auc: 0.99412 | valid_auc: 0.99379 |  0:05:06s\n",
            "epoch 31 | loss: 0.09949 | train_auc: 0.99428 | valid_auc: 0.99395 |  0:05:16s\n",
            "epoch 32 | loss: 0.09812 | train_auc: 0.99447 | valid_auc: 0.99416 |  0:05:26s\n",
            "epoch 33 | loss: 0.09636 | train_auc: 0.99439 | valid_auc: 0.99404 |  0:05:36s\n",
            "epoch 34 | loss: 0.09535 | train_auc: 0.99431 | valid_auc: 0.99399 |  0:05:46s\n",
            "epoch 35 | loss: 0.09631 | train_auc: 0.99467 | valid_auc: 0.9943  |  0:05:56s\n",
            "epoch 36 | loss: 0.0944  | train_auc: 0.99451 | valid_auc: 0.99414 |  0:06:06s\n",
            "epoch 37 | loss: 0.09476 | train_auc: 0.99491 | valid_auc: 0.99451 |  0:06:16s\n",
            "epoch 38 | loss: 0.09419 | train_auc: 0.99485 | valid_auc: 0.99447 |  0:06:25s\n",
            "epoch 39 | loss: 0.09285 | train_auc: 0.99485 | valid_auc: 0.99445 |  0:06:35s\n",
            "epoch 40 | loss: 0.09271 | train_auc: 0.99473 | valid_auc: 0.99433 |  0:06:45s\n",
            "epoch 41 | loss: 0.09163 | train_auc: 0.99508 | valid_auc: 0.99466 |  0:06:55s\n",
            "epoch 42 | loss: 0.09165 | train_auc: 0.99519 | valid_auc: 0.99484 |  0:07:05s\n",
            "epoch 43 | loss: 0.09162 | train_auc: 0.99514 | valid_auc: 0.99469 |  0:07:15s\n",
            "epoch 44 | loss: 0.09087 | train_auc: 0.99496 | valid_auc: 0.99453 |  0:07:25s\n",
            "epoch 45 | loss: 0.09192 | train_auc: 0.99495 | valid_auc: 0.99451 |  0:07:35s\n",
            "epoch 46 | loss: 0.09119 | train_auc: 0.99514 | valid_auc: 0.99477 |  0:07:45s\n",
            "epoch 47 | loss: 0.09068 | train_auc: 0.99488 | valid_auc: 0.99448 |  0:07:56s\n",
            "epoch 48 | loss: 0.09051 | train_auc: 0.9952  | valid_auc: 0.99477 |  0:08:05s\n",
            "epoch 49 | loss: 0.08939 | train_auc: 0.99541 | valid_auc: 0.995   |  0:08:15s\n",
            "epoch 50 | loss: 0.08923 | train_auc: 0.99536 | valid_auc: 0.99493 |  0:08:25s\n",
            "epoch 51 | loss: 0.0881  | train_auc: 0.99521 | valid_auc: 0.99479 |  0:08:34s\n",
            "epoch 52 | loss: 0.08867 | train_auc: 0.99524 | valid_auc: 0.9948  |  0:08:44s\n",
            "epoch 53 | loss: 0.08872 | train_auc: 0.99548 | valid_auc: 0.99503 |  0:08:54s\n",
            "epoch 54 | loss: 0.08818 | train_auc: 0.9955  | valid_auc: 0.99505 |  0:09:04s\n",
            "epoch 55 | loss: 0.08716 | train_auc: 0.99545 | valid_auc: 0.99501 |  0:09:14s\n",
            "epoch 56 | loss: 0.08767 | train_auc: 0.99562 | valid_auc: 0.99518 |  0:09:24s\n",
            "epoch 57 | loss: 0.08787 | train_auc: 0.99544 | valid_auc: 0.99497 |  0:09:34s\n",
            "epoch 58 | loss: 0.0875  | train_auc: 0.99563 | valid_auc: 0.99525 |  0:09:44s\n",
            "epoch 59 | loss: 0.0871  | train_auc: 0.99572 | valid_auc: 0.99532 |  0:09:54s\n",
            "epoch 60 | loss: 0.08653 | train_auc: 0.99566 | valid_auc: 0.99532 |  0:10:04s\n",
            "epoch 61 | loss: 0.08606 | train_auc: 0.99575 | valid_auc: 0.99536 |  0:10:14s\n",
            "epoch 62 | loss: 0.08603 | train_auc: 0.99584 | valid_auc: 0.99544 |  0:10:24s\n",
            "epoch 63 | loss: 0.08523 | train_auc: 0.99577 | valid_auc: 0.9954  |  0:10:34s\n",
            "epoch 64 | loss: 0.08593 | train_auc: 0.99568 | valid_auc: 0.9952  |  0:10:44s\n",
            "epoch 65 | loss: 0.08557 | train_auc: 0.9958  | valid_auc: 0.99539 |  0:10:54s\n",
            "epoch 66 | loss: 0.08572 | train_auc: 0.99571 | valid_auc: 0.99534 |  0:11:04s\n",
            "epoch 67 | loss: 0.08431 | train_auc: 0.99582 | valid_auc: 0.99543 |  0:11:14s\n",
            "epoch 68 | loss: 0.08432 | train_auc: 0.99583 | valid_auc: 0.99541 |  0:11:24s\n",
            "epoch 69 | loss: 0.08407 | train_auc: 0.99566 | valid_auc: 0.99531 |  0:11:34s\n",
            "epoch 70 | loss: 0.08446 | train_auc: 0.99597 | valid_auc: 0.9956  |  0:11:44s\n",
            "epoch 71 | loss: 0.08346 | train_auc: 0.99602 | valid_auc: 0.99558 |  0:11:54s\n",
            "epoch 72 | loss: 0.08345 | train_auc: 0.99605 | valid_auc: 0.99561 |  0:12:04s\n",
            "epoch 73 | loss: 0.08327 | train_auc: 0.99605 | valid_auc: 0.99565 |  0:12:13s\n",
            "epoch 74 | loss: 0.08255 | train_auc: 0.99592 | valid_auc: 0.99555 |  0:12:23s\n",
            "epoch 75 | loss: 0.08332 | train_auc: 0.99606 | valid_auc: 0.99566 |  0:12:33s\n",
            "epoch 76 | loss: 0.08295 | train_auc: 0.99594 | valid_auc: 0.99556 |  0:12:43s\n",
            "epoch 77 | loss: 0.08378 | train_auc: 0.99582 | valid_auc: 0.99545 |  0:12:53s\n",
            "epoch 78 | loss: 0.08316 | train_auc: 0.99596 | valid_auc: 0.99551 |  0:13:03s\n",
            "epoch 79 | loss: 0.08313 | train_auc: 0.9959  | valid_auc: 0.99551 |  0:13:13s\n",
            "epoch 80 | loss: 0.08352 | train_auc: 0.9957  | valid_auc: 0.99532 |  0:13:23s\n",
            "epoch 81 | loss: 0.08359 | train_auc: 0.99585 | valid_auc: 0.99544 |  0:13:33s\n",
            "epoch 82 | loss: 0.08165 | train_auc: 0.99596 | valid_auc: 0.99552 |  0:13:43s\n",
            "epoch 83 | loss: 0.08148 | train_auc: 0.99604 | valid_auc: 0.9956  |  0:13:53s\n",
            "epoch 84 | loss: 0.08091 | train_auc: 0.99624 | valid_auc: 0.99584 |  0:14:03s\n",
            "epoch 85 | loss: 0.08103 | train_auc: 0.99619 | valid_auc: 0.99579 |  0:14:13s\n",
            "epoch 86 | loss: 0.08133 | train_auc: 0.99629 | valid_auc: 0.99584 |  0:14:23s\n",
            "epoch 87 | loss: 0.0803  | train_auc: 0.99628 | valid_auc: 0.99587 |  0:14:33s\n",
            "epoch 88 | loss: 0.07996 | train_auc: 0.99634 | valid_auc: 0.996   |  0:14:43s\n",
            "epoch 89 | loss: 0.07974 | train_auc: 0.99631 | valid_auc: 0.99583 |  0:14:52s\n",
            "epoch 90 | loss: 0.08033 | train_auc: 0.99614 | valid_auc: 0.99567 |  0:15:02s\n",
            "epoch 91 | loss: 0.07963 | train_auc: 0.99627 | valid_auc: 0.99583 |  0:15:12s\n",
            "epoch 92 | loss: 0.07986 | train_auc: 0.9964  | valid_auc: 0.99589 |  0:15:22s\n",
            "epoch 93 | loss: 0.07936 | train_auc: 0.99634 | valid_auc: 0.99587 |  0:15:32s\n",
            "epoch 94 | loss: 0.0794  | train_auc: 0.9963  | valid_auc: 0.9959  |  0:15:42s\n",
            "epoch 95 | loss: 0.07895 | train_auc: 0.99635 | valid_auc: 0.99582 |  0:15:52s\n",
            "epoch 96 | loss: 0.07894 | train_auc: 0.99629 | valid_auc: 0.99578 |  0:16:02s\n",
            "epoch 97 | loss: 0.07991 | train_auc: 0.99608 | valid_auc: 0.99564 |  0:16:11s\n",
            "epoch 98 | loss: 0.08033 | train_auc: 0.99641 | valid_auc: 0.99599 |  0:16:21s\n",
            "epoch 99 | loss: 0.07928 | train_auc: 0.99642 | valid_auc: 0.99598 |  0:16:31s\n",
            "epoch 100| loss: 0.07866 | train_auc: 0.99633 | valid_auc: 0.99586 |  0:16:41s\n",
            "epoch 101| loss: 0.07867 | train_auc: 0.99607 | valid_auc: 0.99556 |  0:16:51s\n",
            "epoch 102| loss: 0.0784  | train_auc: 0.99643 | valid_auc: 0.99593 |  0:17:01s\n",
            "epoch 103| loss: 0.07797 | train_auc: 0.99647 | valid_auc: 0.99599 |  0:17:11s\n",
            "epoch 104| loss: 0.07778 | train_auc: 0.99652 | valid_auc: 0.99604 |  0:17:21s\n",
            "epoch 105| loss: 0.0772  | train_auc: 0.99652 | valid_auc: 0.99611 |  0:17:31s\n",
            "epoch 106| loss: 0.07769 | train_auc: 0.99649 | valid_auc: 0.99599 |  0:17:41s\n",
            "epoch 107| loss: 0.07689 | train_auc: 0.99663 | valid_auc: 0.99614 |  0:17:51s\n",
            "epoch 108| loss: 0.07698 | train_auc: 0.99662 | valid_auc: 0.99614 |  0:18:00s\n",
            "epoch 109| loss: 0.0774  | train_auc: 0.99649 | valid_auc: 0.996   |  0:18:10s\n",
            "epoch 110| loss: 0.07769 | train_auc: 0.99659 | valid_auc: 0.99612 |  0:18:20s\n",
            "epoch 111| loss: 0.07689 | train_auc: 0.99643 | valid_auc: 0.99594 |  0:18:31s\n",
            "epoch 112| loss: 0.07726 | train_auc: 0.9966  | valid_auc: 0.99612 |  0:18:41s\n",
            "epoch 113| loss: 0.07705 | train_auc: 0.99666 | valid_auc: 0.99623 |  0:18:51s\n",
            "epoch 114| loss: 0.07652 | train_auc: 0.99653 | valid_auc: 0.99609 |  0:19:00s\n",
            "epoch 115| loss: 0.07612 | train_auc: 0.99658 | valid_auc: 0.99607 |  0:19:10s\n",
            "epoch 116| loss: 0.07662 | train_auc: 0.99659 | valid_auc: 0.99612 |  0:19:20s\n",
            "epoch 117| loss: 0.07697 | train_auc: 0.9966  | valid_auc: 0.99611 |  0:19:30s\n",
            "epoch 118| loss: 0.07723 | train_auc: 0.9966  | valid_auc: 0.99618 |  0:19:40s\n",
            "epoch 119| loss: 0.07681 | train_auc: 0.99661 | valid_auc: 0.99617 |  0:19:50s\n",
            "epoch 120| loss: 0.07621 | train_auc: 0.99668 | valid_auc: 0.99625 |  0:20:00s\n",
            "epoch 121| loss: 0.07559 | train_auc: 0.99668 | valid_auc: 0.9962  |  0:20:10s\n",
            "epoch 122| loss: 0.0766  | train_auc: 0.99666 | valid_auc: 0.99624 |  0:20:20s\n",
            "epoch 123| loss: 0.07574 | train_auc: 0.99672 | valid_auc: 0.99625 |  0:20:29s\n",
            "epoch 124| loss: 0.07593 | train_auc: 0.99669 | valid_auc: 0.99625 |  0:20:39s\n",
            "epoch 125| loss: 0.07518 | train_auc: 0.99671 | valid_auc: 0.99627 |  0:20:49s\n",
            "epoch 126| loss: 0.07519 | train_auc: 0.99677 | valid_auc: 0.99634 |  0:20:59s\n",
            "epoch 127| loss: 0.0756  | train_auc: 0.99673 | valid_auc: 0.99623 |  0:21:09s\n",
            "epoch 128| loss: 0.07601 | train_auc: 0.99678 | valid_auc: 0.99634 |  0:21:20s\n",
            "epoch 129| loss: 0.07577 | train_auc: 0.99676 | valid_auc: 0.99632 |  0:21:30s\n",
            "epoch 130| loss: 0.0757  | train_auc: 0.99673 | valid_auc: 0.99631 |  0:21:39s\n",
            "epoch 131| loss: 0.07603 | train_auc: 0.99675 | valid_auc: 0.99629 |  0:21:49s\n",
            "epoch 132| loss: 0.0757  | train_auc: 0.99676 | valid_auc: 0.99629 |  0:21:59s\n",
            "epoch 133| loss: 0.07455 | train_auc: 0.99686 | valid_auc: 0.99645 |  0:22:09s\n",
            "epoch 134| loss: 0.07464 | train_auc: 0.99678 | valid_auc: 0.99636 |  0:22:19s\n",
            "epoch 135| loss: 0.07528 | train_auc: 0.99654 | valid_auc: 0.99612 |  0:22:30s\n",
            "epoch 136| loss: 0.07537 | train_auc: 0.99685 | valid_auc: 0.99643 |  0:22:39s\n",
            "epoch 137| loss: 0.07432 | train_auc: 0.99683 | valid_auc: 0.99636 |  0:22:50s\n",
            "epoch 138| loss: 0.07465 | train_auc: 0.99682 | valid_auc: 0.99639 |  0:22:59s\n",
            "epoch 139| loss: 0.07462 | train_auc: 0.9969  | valid_auc: 0.99643 |  0:23:09s\n",
            "epoch 140| loss: 0.10475 | train_auc: 0.98471 | valid_auc: 0.9846  |  0:23:19s\n",
            "epoch 141| loss: 0.10145 | train_auc: 0.9866  | valid_auc: 0.98633 |  0:23:29s\n",
            "epoch 142| loss: 0.08873 | train_auc: 0.99416 | valid_auc: 0.99378 |  0:23:39s\n",
            "epoch 143| loss: 0.08335 | train_auc: 0.99523 | valid_auc: 0.99482 |  0:23:49s\n",
            "epoch 144| loss: 0.08043 | train_auc: 0.99589 | valid_auc: 0.99546 |  0:23:59s\n",
            "epoch 145| loss: 0.07795 | train_auc: 0.99641 | valid_auc: 0.99598 |  0:24:09s\n",
            "epoch 146| loss: 0.07754 | train_auc: 0.99645 | valid_auc: 0.99603 |  0:24:18s\n",
            "epoch 147| loss: 0.0776  | train_auc: 0.99659 | valid_auc: 0.99618 |  0:24:28s\n",
            "epoch 148| loss: 0.07705 | train_auc: 0.99669 | valid_auc: 0.99632 |  0:24:38s\n",
            "epoch 149| loss: 0.07722 | train_auc: 0.99657 | valid_auc: 0.99617 |  0:24:48s\n",
            "epoch 150| loss: 0.07628 | train_auc: 0.99675 | valid_auc: 0.99636 |  0:24:58s\n",
            "epoch 151| loss: 0.07567 | train_auc: 0.99675 | valid_auc: 0.99636 |  0:25:08s\n",
            "epoch 152| loss: 0.07599 | train_auc: 0.99683 | valid_auc: 0.99643 |  0:25:18s\n",
            "epoch 153| loss: 0.07519 | train_auc: 0.99673 | valid_auc: 0.99639 |  0:25:28s\n",
            "\n",
            "Early stopping occurred at epoch 153 with best_epoch = 133 and best_valid_auc = 0.99645\n",
            "Best weights from best epoch are automatically used!\n",
            "AUC: 0.9964460136938154\n"
          ]
        }
      ],
      "source": [
        "df_train = df_train.sample(frac=1).reset_index(drop=True)  # df_trainをシャッフル\n",
        "\n",
        "skf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\n",
        "\n",
        "for fold, (train_index, valid_index) in enumerate(skf.split(df_train, df_train[CFG.target])):\n",
        "    print('-----', fold, '-----')\n",
        "\n",
        "    X_train = df_train[all_features].to_numpy()[train_index]\n",
        "    y_train = df_train[CFG.target].to_numpy()[train_index]\n",
        "    X_valid = df_train[all_features].to_numpy()[valid_index]\n",
        "    y_valid = df_train[CFG.target].to_numpy()[valid_index]\n",
        "    X_test = df_test[all_features].to_numpy()\n",
        "    \n",
        "    # fold毎に初期化する設定\n",
        "    # https://github.com/dreamquark-ai/tabnet\n",
        "    tabnet_params = dict(\n",
        "        # TODO ここら辺初期値にしてみる\n",
        "        n_d=64,  # Width of the decision prediction layer. Bigger values gives more capacity to the model with the risk of overfitting. Values typically range from 8 to 64.\n",
        "        n_a=64,  # Width of the attention embedding for each mask. According to the paper n_d=n_a is usually a good choice. (default=8)\n",
        "        n_steps=5,  # Number of steps in the architecture (usually between 3 and 10)\n",
        "        gamma=1.3,  # This is the coefficient for feature reusage in the masks. A value close to 1 will make mask selection least correlated between layers. Values range from 1.0 to 2.0.\n",
        "        cat_idxs=cat_idxs, # list of int (default=[] - Mandatory for embeddings). List of categorical features indices.\n",
        "        cat_dims=cat_dims,  # list of int (default=[] - Mandatory for embeddings). List of categorical features number of modalities (number of unique values for a categorical feature) /!\\ no new modalities can be predicted\n",
        "        cat_emb_dim=2,  # list of int (optional). List of embeddings size for each categorical features. (default =1)\n",
        "        n_independent=3,  # Number of independent Gated Linear Units layers at each step. Usual values range from 1 to 5.\n",
        "        n_shared=3,  # Number of shared Gated Linear Units at each step Usual values range from 1 to 5.\n",
        "        # epsilon=1e-15   # Should be left untouched.\n",
        "        seed=CFG.seed,\n",
        "        momentum=0.01,  # Momentum for batch normalization, typically ranges from 0.01 to 0.4 (default=0.02)\n",
        "        clip_value=None,  # float (default None). If a float is given this will clip the gradient at clip_value.\n",
        "        lambda_sparse=1e-6,  # float (default = 1e-3). This is the extra sparsity loss coefficient as proposed in the original paper. The bigger this coefficient is, the sparser your model will be in terms of feature selection. Depending on the difficulty of your problem, reducing this value could help.\n",
        "\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(\n",
        "            lr=CFG.lr,\n",
        "            weight_decay=1e-7\n",
        "        ),\n",
        "        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "        scheduler_params=dict(\n",
        "            mode='max',\n",
        "            factor=0.9,\n",
        "            patience=10,\n",
        "            min_lr=CFG.min_lr,\n",
        "        ),\n",
        "        verbose=1,  # Verbosity for notebooks plots, set to 1 to see every epoch, 0 to get None.\n",
        "        device_name='auto',  #  str (default='auto') 'cpu' for cpu training, 'gpu' for gpu training, 'auto' to automatically detect gpu\n",
        "        mask_type='sparsemax',  # (default='sparsemax') Either \"sparsemax\" or \"entmax\" : this is the masking function to use for selecting features.\n",
        "    )\n",
        "\n",
        "    print('unsupervised_model')\n",
        "    unsupervised_model = TabNetPretrainer(\n",
        "        cat_idxs=cat_idxs,\n",
        "        cat_dims=cat_dims,\n",
        "        cat_emb_dim=2,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        device_name='auto',  #  str (default='auto') 'cpu' for cpu training, 'gpu' for gpu training, 'auto' to automatically detect gpu\n",
        "        seed=CFG.seed,\n",
        "    )  # schedulerとかがうまくいかないので、tabnet_paramsは使わないでみる\n",
        "\n",
        "    unsupervised_model.fit(\n",
        "        X_train=X_train,\n",
        "        eval_set=[X_valid],\n",
        "        pretraining_ratio=0.8,\n",
        "        batch_size=CFG.batch_size,\n",
        "        virtual_batch_size=CFG.virtual_batch_size,\n",
        "        patience=20,\n",
        "        max_epochs=CFG.epochs,\n",
        "        pin_memory=True,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    print('main model')\n",
        "    # Defining TabNet model\n",
        "    model = TabNetClassifier(**tabnet_params)\n",
        "    model.fit(\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "        eval_name=[\"train\", \"valid\"],\n",
        "        eval_metric=[\"auc\"],\n",
        "        batch_size=CFG.batch_size,\n",
        "        virtual_batch_size=CFG.virtual_batch_size,\n",
        "        max_epochs=CFG.epochs,\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "        patience=20,\n",
        "        num_workers=2,\n",
        "        from_unsupervised=unsupervised_model,\n",
        "        # weights=1,  # サンプリングするか否かっポイ default:0  # TODO weights=0の時との挙動の違いを見たい\n",
        "    )\n",
        "    df_train.loc[valid_index, CFG.pred] = model.predict_proba(X_valid)[:, -1]\n",
        "    auc = roc_auc_score(y_true=y_valid, y_score=df_train.loc[valid_index, CFG.pred])\n",
        "    print('AUC:', auc)\n",
        "    df_test[CFG.pred + str(fold)] = model.predict_proba(X_test)[:, -1]\n",
        "\n",
        "    # TEST\n",
        "    # print('----- this is test run -----')\n",
        "    # break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyGMLJUKeKLx",
        "papermill": {
          "duration": 2.424314,
          "end_time": "2022-05-10T11:32:47.067282",
          "exception": false,
          "start_time": "2022-05-10T11:32:44.642968",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "execution": {
          "iopub.status.busy": "2022-05-12T04:10:48.250775Z",
          "iopub.status.idle": "2022-05-12T04:10:48.251813Z",
          "shell.execute_reply": "2022-05-12T04:10:48.251521Z",
          "shell.execute_reply.started": "2022-05-12T04:10:48.251477Z"
        },
        "id": "VyKu1kSaeKLy",
        "outputId": "7e243e8d-f824-4ca8-b71f-18e761b49493",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dc3a7d90-2536-4211-b525-1326a30adcf1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>900000</td>\n",
              "      <td>0.999975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>900001</td>\n",
              "      <td>0.999870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>900002</td>\n",
              "      <td>0.000027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>900003</td>\n",
              "      <td>0.000125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>900004</td>\n",
              "      <td>0.999809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699995</th>\n",
              "      <td>1599995</td>\n",
              "      <td>0.645901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699996</th>\n",
              "      <td>1599996</td>\n",
              "      <td>0.999885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699997</th>\n",
              "      <td>1599997</td>\n",
              "      <td>0.107184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699998</th>\n",
              "      <td>1599998</td>\n",
              "      <td>0.000138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699999</th>\n",
              "      <td>1599999</td>\n",
              "      <td>0.000037</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>700000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc3a7d90-2536-4211-b525-1326a30adcf1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc3a7d90-2536-4211-b525-1326a30adcf1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc3a7d90-2536-4211-b525-1326a30adcf1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             id    target\n",
              "0        900000  0.999975\n",
              "1        900001  0.999870\n",
              "2        900002  0.000027\n",
              "3        900003  0.000125\n",
              "4        900004  0.999809\n",
              "...         ...       ...\n",
              "699995  1599995  0.645901\n",
              "699996  1599996  0.999885\n",
              "699997  1599997  0.107184\n",
              "699998  1599998  0.000138\n",
              "699999  1599999  0.000037\n",
              "\n",
              "[700000 rows x 2 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cols = [col for col in df_test.columns if CFG.pred in col]\n",
        "\n",
        "df_sub[CFG.target] = df_test[cols].mean(axis=1)\n",
        "df_sub.to_csv(\"submission.csv\", index=False)\n",
        "df_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-12T04:10:48.252985Z",
          "iopub.status.idle": "2022-05-12T04:10:48.253377Z",
          "shell.execute_reply": "2022-05-12T04:10:48.253210Z",
          "shell.execute_reply.started": "2022-05-12T04:10:48.253191Z"
        },
        "id": "aQhcVuareKLy",
        "papermill": {
          "duration": 2.291903,
          "end_time": "2022-05-10T11:33:10.519366",
          "exception": false,
          "start_time": "2022-05-10T11:33:08.227463",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# df_sub[CFG.target] = (df_sub[CFG.target] > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-12T04:10:48.255656Z",
          "iopub.status.idle": "2022-05-12T04:10:48.256299Z",
          "shell.execute_reply": "2022-05-12T04:10:48.255976Z",
          "shell.execute_reply.started": "2022-05-12T04:10:48.255949Z"
        },
        "id": "8e104E84eKLz",
        "papermill": {
          "duration": 4.162619,
          "end_time": "2022-05-10T11:33:16.942301",
          "exception": false,
          "start_time": "2022-05-10T11:33:12.779682",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_sub.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "execution": {
          "iopub.status.busy": "2022-05-12T04:10:48.258539Z",
          "iopub.status.idle": "2022-05-12T04:10:48.259473Z",
          "shell.execute_reply": "2022-05-12T04:10:48.259194Z",
          "shell.execute_reply.started": "2022-05-12T04:10:48.259161Z"
        },
        "id": "u9fwMF2ReKLz",
        "outputId": "ebdbfe1e-8170-4105-d043-a51be9091328",
        "papermill": {
          "duration": 2.236115,
          "end_time": "2022-05-10T11:33:21.406575",
          "exception": false,
          "start_time": "2022-05-10T11:33:19.17046",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-01342104-f446-420a-9b32-49da4dc0bc6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>900000</td>\n",
              "      <td>0.999975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>900001</td>\n",
              "      <td>0.999870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>900002</td>\n",
              "      <td>0.000027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>900003</td>\n",
              "      <td>0.000125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>900004</td>\n",
              "      <td>0.999809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>900005</td>\n",
              "      <td>0.036031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>900006</td>\n",
              "      <td>0.287342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>900007</td>\n",
              "      <td>0.999987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>900008</td>\n",
              "      <td>0.316022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>900009</td>\n",
              "      <td>0.999435</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01342104-f446-420a-9b32-49da4dc0bc6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01342104-f446-420a-9b32-49da4dc0bc6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01342104-f446-420a-9b32-49da4dc0bc6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id    target\n",
              "0  900000  0.999975\n",
              "1  900001  0.999870\n",
              "2  900002  0.000027\n",
              "3  900003  0.000125\n",
              "4  900004  0.999809\n",
              "5  900005  0.036031\n",
              "6  900006  0.287342\n",
              "7  900007  0.999987\n",
              "8  900008  0.316022\n",
              "9  900009  0.999435"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sub.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2022-05-12T04:10:48.261346Z",
          "iopub.status.idle": "2022-05-12T04:10:48.261928Z",
          "shell.execute_reply": "2022-05-12T04:10:48.261615Z",
          "shell.execute_reply.started": "2022-05-12T04:10:48.261588Z"
        },
        "id": "joNwrPeVeKL0",
        "outputId": "38950f31-7bc0-42ef-c040-4e30f2f683d5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.39335595e-02,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 3.27798903e-01, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       7.30361231e-02, 0.00000000e+00, 4.51399591e-05, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       1.98674114e-07, 0.00000000e+00, 3.22039652e-02, 0.00000000e+00,\n",
              "       3.13393894e-01, 0.00000000e+00, 8.10878666e-04, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 4.20102789e-03, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 1.59386021e-01, 0.00000000e+00, 5.19028868e-03,\n",
              "       0.00000000e+00])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "execution": {
          "iopub.status.busy": "2022-05-12T04:10:48.264643Z",
          "iopub.status.idle": "2022-05-12T04:10:48.265293Z",
          "shell.execute_reply": "2022-05-12T04:10:48.264998Z",
          "shell.execute_reply.started": "2022-05-12T04:10:48.264969Z"
        },
        "id": "f62NtDRleKL0",
        "outputId": "9528630d-4553-4f9a-b859-0179333eb538",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f74bdee6450>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "importance = pd.DataFrame()\n",
        "importance['feature'] = df_train[all_features].columns\n",
        "importance['importance'] = model.feature_importances_\n",
        "\n",
        "sns.barplot(x='importance', y='feature', data=importance.sort_values(by='importance', ascending=False))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "tabnet_TPS2205.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "60f8cdbf2a96461788475085dd1e9d6dd7137de331e19aa3c17c37cb4f0963a7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('yourenvname')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
