{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorchによる分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Takanori/Desktop/Kaggle/SIGNATE2205')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time, gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    target = 'charges'\n",
    "    batch_size = 128\n",
    "    apex=True\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    max_grad_norm=1000\n",
    "    batch_scheduler=True\n",
    "    print_freq=100000000\n",
    "    num_workers=0\n",
    "    trn_fold=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    lr = 1e-2\n",
    "    lr_gamma = 0.9\n",
    "    epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('sample_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>male</td>\n",
       "      <td>46.561704</td>\n",
       "      <td>5</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>23.572081</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southwest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>female</td>\n",
       "      <td>38.670352</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>female</td>\n",
       "      <td>45.614196</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southwest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>38.769610</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age     sex        bmi  children smoker     region  charges\n",
       "0   1   45    male  46.561704         5     no  southeast        0\n",
       "1   2   18  female  23.572081         3     no  southwest        0\n",
       "2   4   28  female  38.670352         0     no  southeast        0\n",
       "3   5   46  female  45.614196         0     no  southwest        0\n",
       "4  10   27    male  38.769610         0     no  northeast        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[CFG.target] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        1200 non-null   int64  \n",
      " 1   age       1200 non-null   int64  \n",
      " 2   sex       1200 non-null   object \n",
      " 3   bmi       1200 non-null   float64\n",
      " 4   children  1200 non-null   int64  \n",
      " 5   smoker    1200 non-null   object \n",
      " 6   region    1200 non-null   object \n",
      " 7   charges   1200 non-null   int64  \n",
      "dtypes: float64(1), int64(4), object(3)\n",
      "memory usage: 75.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "# nullは存在しない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1009.728333</td>\n",
       "      <td>38.238333</td>\n",
       "      <td>33.665249</td>\n",
       "      <td>0.884167</td>\n",
       "      <td>0.255833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>581.366414</td>\n",
       "      <td>12.708571</td>\n",
       "      <td>5.866870</td>\n",
       "      <td>1.093959</td>\n",
       "      <td>0.586517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>22.997608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>503.750000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.741881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1011.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.441095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1522.250000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>38.575390</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>46.755010</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id          age          bmi     children      charges\n",
       "count  1200.000000  1200.000000  1200.000000  1200.000000  1200.000000\n",
       "mean   1009.728333    38.238333    33.665249     0.884167     0.255833\n",
       "std     581.366414    12.708571     5.866870     1.093959     0.586517\n",
       "min       1.000000    18.000000    22.997608     0.000000     0.000000\n",
       "25%     503.750000    28.000000    29.741881     0.000000     0.000000\n",
       "50%    1011.000000    38.000000    33.441095     0.000000     0.000000\n",
       "75%    1522.250000    49.000000    38.575390     2.000000     0.000000\n",
       "max    1999.000000    63.000000    46.755010     5.000000     2.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          1200\n",
       "age           45\n",
       "sex            2\n",
       "bmi         1200\n",
       "children       6\n",
       "smoker         2\n",
       "region         4\n",
       "charges        3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字列を数値に変換\n",
    "for col in df_all.columns:\n",
    "    if df_all[col].dtype == 'object':\n",
    "        l_enc = LabelEncoder()\n",
    "        df_all[col] = l_enc.fit_transform(df_all[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_all.iloc[:len(train)]\n",
    "test = df_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in [CFG.target, 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_24892\\3679118474.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train.loc[val_index, 'fold'] = int(n)\n",
      "C:\\Users\\Takanori\\AppData\\Local\\Temp\\ipykernel_24892\\3679118474.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['fold'] = train['fold'].astype(int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    240\n",
       "1    240\n",
       "2    240\n",
       "3    240\n",
       "4    240\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CV Split\n",
    "skf = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(skf.split(train, train[CFG.target])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        self.model_name = 'Net1'\n",
    "        # 4層\n",
    "        super(Net1, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.first_bn = nn.BatchNorm1d(self.input_size, momentum=0.01)  # とりあえず入れてみた。momentumが小さいといいことあるんかな・・？\n",
    "        self.fc1 = nn.Linear(self.input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 384)\n",
    "        self.bn2 = nn.BatchNorm1d(384)\n",
    "        self.fc3 = nn.Linear(384, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.fc6 = nn.Linear(64, 32)\n",
    "        self.bn6 = nn.BatchNorm1d(32)\n",
    "        self.fc7 = nn.Linear(32, 3)\n",
    "        self.bn7 = nn.BatchNorm1d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # dropoutの後にbnを置いてはならない\n",
    "        # nbは、活性化関数の前に置く\n",
    "        x = self.first_bn(x)\n",
    "        x = F.silu(self.bn1((self.fc1(x))))\n",
    "        x = F.silu(self.bn2((self.fc2(x))))\n",
    "        x = F.silu(self.bn3((self.fc3(x))))\n",
    "        x = F.silu(self.bn4((self.fc4(x))))\n",
    "        x = F.silu(self.bn5((self.fc5(x))))\n",
    "        x = F.silu(self.bn6((self.fc6(x))))\n",
    "        x = self.bn7((self.fc7(x)))\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataSet\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, features, target):\n",
    "        self.target = df[target].values\n",
    "        self.data = df[features].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = torch.tensor(self.data[item], dtype=torch.float32)\n",
    "        label = torch.tensor(self.target[item], dtype=torch.long)\n",
    "        return inputs, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = accuracy_score(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler()  # これが何か調べる\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):  # これが何か調べる\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm =torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, epoch, criterion, device):\n",
    "    model.eval()\n",
    "    scaler = torch.cuda.amp.GradScaler()  # これが何か調べる\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    preds = []\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        grad_norm =torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  .format(epoch+1, step, len(valid_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,))\n",
    "\n",
    "        _, predicted = torch.max(y_preds.data, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        preds.append(predicted)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        _, predicted = torch.max(y_preds.data, 1)\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        preds.append(predictions)\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    print(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target].values\n",
    "    \n",
    "    train_dataset = TrainDataset(train_folds, features, CFG.target)\n",
    "    valid_dataset = TrainDataset(valid_folds, features, CFG.target)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = Net1(len(features)).to(device)\n",
    "\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    # optimizer = AdamW(lr=CFG.lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "\n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        # lr_gammna**n = 1e-3\n",
    "        # n = log(lr_gammna) / log(1e-3)\n",
    "        n = math.log(1e-3) / math.log(cfg.lr_gamma)\n",
    "        step_size = int(num_train_steps / n)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=cfg.lr_gamma)\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_score = -float('inf')\n",
    "\n",
    "    model_file_name = model.model_name + f\"_fold{fold}_best.pth\"\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, epoch, criterion, device)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        print(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        model_file_name)\n",
    "\n",
    "    predictions = torch.load(model_file_name, \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds['pred'] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Epoch: [1][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 1.1429(1.1429) Grad: 197728.2656  LR: 0.01000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8898(0.9776) Grad: 15101.3955  LR: 0.01000000  \n",
      "Epoch: [1][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4968(1.4968) Grad: 0.0000  \n",
      "Epoch: [1][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4443(1.4723) Grad: 0.0000  \n",
      "Epoch 1 - avg_train_loss: 0.9776  avg_val_loss: 1.4723  time: 0s\n",
      "Epoch 1 - Score: 0.0792\n",
      "Epoch 1 - Save Best Score: 0.0792 Model\n",
      "Epoch: [2][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8914(0.8914) Grad: 14361.1318  LR: 0.01000000  \n",
      "Epoch: [2][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8374(0.8707) Grad: 14571.2705  LR: 0.01000000  \n",
      "Epoch: [2][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4968(1.4968) Grad: 0.0000  \n",
      "Epoch: [2][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4443(1.4723) Grad: 0.0000  \n",
      "Epoch 2 - avg_train_loss: 0.8707  avg_val_loss: 1.4723  time: 0s\n",
      "Epoch 2 - Score: 0.0792\n",
      "Epoch: [3][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8400(0.8400) Grad: 15233.6172  LR: 0.01000000  \n",
      "Epoch: [3][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7874(0.8298) Grad: 12495.4082  LR: 0.01000000  \n",
      "Epoch: [3][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4952(1.4952) Grad: 0.0000  \n",
      "Epoch: [3][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4431(1.4709) Grad: 0.0000  \n",
      "Epoch 3 - avg_train_loss: 0.8298  avg_val_loss: 1.4709  time: 0s\n",
      "Epoch 3 - Score: 0.0792\n",
      "Epoch: [4][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8134(0.8134) Grad: 13124.4014  LR: 0.00810000  \n",
      "Epoch: [4][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8010(0.8046) Grad: 10802.2520  LR: 0.00900000  \n",
      "Epoch: [4][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.3943(1.3943) Grad: 0.0000  \n",
      "Epoch: [4][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.3709(1.3834) Grad: 0.0000  \n",
      "Epoch 4 - avg_train_loss: 0.8046  avg_val_loss: 1.3834  time: 0s\n",
      "Epoch 4 - Score: 0.0917\n",
      "Epoch 4 - Save Best Score: 0.0917 Model\n",
      "Epoch: [5][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7844(0.7844) Grad: 12190.7148  LR: 0.00900000  \n",
      "Epoch: [5][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7936(0.7728) Grad: 11069.2598  LR: 0.00900000  \n",
      "Epoch: [5][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.0818(1.0818) Grad: 0.0000  \n",
      "Epoch: [5][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.0950(1.0879) Grad: 0.0000  \n",
      "Epoch 5 - avg_train_loss: 0.7728  avg_val_loss: 1.0879  time: 0s\n",
      "Epoch 5 - Score: 0.4375\n",
      "Epoch 5 - Save Best Score: 0.4375 Model\n",
      "Epoch: [6][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7699(0.7699) Grad: 10372.1738  LR: 0.00900000  \n",
      "Epoch: [6][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7396(0.7538) Grad: 8359.2773  LR: 0.00900000  \n",
      "Epoch: [6][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9094(0.9094) Grad: 0.0000  \n",
      "Epoch: [6][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9273(0.9177) Grad: 0.0000  \n",
      "Epoch 6 - avg_train_loss: 0.7538  avg_val_loss: 0.9177  time: 0s\n",
      "Epoch 6 - Score: 0.7458\n",
      "Epoch 6 - Save Best Score: 0.7458 Model\n",
      "Epoch: [7][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7432(0.7432) Grad: 8644.6416  LR: 0.00900000  \n",
      "Epoch: [7][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7433(0.7365) Grad: 9298.5625  LR: 0.00810000  \n",
      "Epoch: [7][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8101(0.8101) Grad: 0.0000  \n",
      "Epoch: [7][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8197(0.8145) Grad: 0.0000  \n",
      "Epoch 7 - avg_train_loss: 0.7365  avg_val_loss: 0.8145  time: 0s\n",
      "Epoch 7 - Score: 0.8625\n",
      "Epoch 7 - Save Best Score: 0.8625 Model\n",
      "Epoch: [8][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6942(0.6942) Grad: 8014.7915  LR: 0.00810000  \n",
      "Epoch: [8][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7161(0.7167) Grad: 7438.5010  LR: 0.00810000  \n",
      "Epoch: [8][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7587(0.7587) Grad: 0.0000  \n",
      "Epoch: [8][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7726(0.7652) Grad: 0.0000  \n",
      "Epoch 8 - avg_train_loss: 0.7167  avg_val_loss: 0.7652  time: 0s\n",
      "Epoch 8 - Score: 0.8625\n",
      "Epoch: [9][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7092(0.7092) Grad: 5840.3433  LR: 0.00810000  \n",
      "Epoch: [9][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7484(0.7091) Grad: 7185.8032  LR: 0.00810000  \n",
      "Epoch: [9][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7511(0.7511) Grad: 0.0000  \n",
      "Epoch: [9][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7746(0.7620) Grad: 0.0000  \n",
      "Epoch 9 - avg_train_loss: 0.7091  avg_val_loss: 0.7620  time: 0s\n",
      "Epoch 9 - Score: 0.8292\n",
      "Epoch: [10][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6993(0.6993) Grad: 5635.7227  LR: 0.00810000  \n",
      "Epoch: [10][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6898(0.6952) Grad: 6031.9106  LR: 0.00729000  \n",
      "Epoch: [10][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7446(0.7446) Grad: 0.0000  \n",
      "Epoch: [10][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7496(0.7469) Grad: 0.0000  \n",
      "Epoch 10 - avg_train_loss: 0.6952  avg_val_loss: 0.7469  time: 0s\n",
      "Epoch 10 - Score: 0.8583\n",
      "Epoch: [11][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6863(0.6863) Grad: 5624.3286  LR: 0.00729000  \n",
      "Epoch: [11][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7027(0.6871) Grad: 7183.9922  LR: 0.00729000  \n",
      "Epoch: [11][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7425(0.7425) Grad: 0.0000  \n",
      "Epoch: [11][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7752(0.7578) Grad: 0.0000  \n",
      "Epoch 11 - avg_train_loss: 0.6871  avg_val_loss: 0.7578  time: 0s\n",
      "Epoch 11 - Score: 0.8208\n",
      "Epoch: [12][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6929(0.6929) Grad: 4192.1055  LR: 0.00729000  \n",
      "Epoch: [12][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6859(0.6816) Grad: 5709.6528  LR: 0.00729000  \n",
      "Epoch: [12][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7328(0.7328) Grad: 0.0000  \n",
      "Epoch: [12][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7362(0.7344) Grad: 0.0000  \n",
      "Epoch 12 - avg_train_loss: 0.6816  avg_val_loss: 0.7344  time: 0s\n",
      "Epoch 12 - Score: 0.8542\n",
      "Epoch: [13][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6800(0.6800) Grad: 3706.6367  LR: 0.00729000  \n",
      "Epoch: [13][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6819(0.6756) Grad: 3473.3113  LR: 0.00656100  \n",
      "Epoch: [13][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7985(0.7985) Grad: 0.0000  \n",
      "Epoch: [13][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8029(0.8006) Grad: 0.0000  \n",
      "Epoch 13 - avg_train_loss: 0.6756  avg_val_loss: 0.8006  time: 0s\n",
      "Epoch 13 - Score: 0.8167\n",
      "Epoch: [14][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6782(0.6782) Grad: 4216.7422  LR: 0.00656100  \n",
      "Epoch: [14][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7091(0.6684) Grad: 3891.8149  LR: 0.00656100  \n",
      "Epoch: [14][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7348(0.7348) Grad: 0.0000  \n",
      "Epoch: [14][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7440(0.7391) Grad: 0.0000  \n",
      "Epoch 14 - avg_train_loss: 0.6684  avg_val_loss: 0.7391  time: 0s\n",
      "Epoch 14 - Score: 0.8625\n",
      "Epoch: [15][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6576(0.6576) Grad: 4054.3379  LR: 0.00656100  \n",
      "Epoch: [15][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6477(0.6635) Grad: 4398.9922  LR: 0.00656100  \n",
      "Epoch: [15][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7165(0.7165) Grad: 0.0000  \n",
      "Epoch: [15][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7243(0.7201) Grad: 0.0000  \n",
      "Epoch 15 - avg_train_loss: 0.6635  avg_val_loss: 0.7201  time: 0s\n",
      "Epoch 15 - Score: 0.8625\n",
      "Epoch: [16][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6443(0.6443) Grad: 3643.8137  LR: 0.00656100  \n",
      "Epoch: [16][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7012(0.6584) Grad: 3687.7788  LR: 0.00590490  \n",
      "Epoch: [16][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7101(0.7101) Grad: 0.0000  \n",
      "Epoch: [16][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7206(0.7150) Grad: 0.0000  \n",
      "Epoch 16 - avg_train_loss: 0.6584  avg_val_loss: 0.7150  time: 0s\n",
      "Epoch 16 - Score: 0.8750\n",
      "Epoch 16 - Save Best Score: 0.8750 Model\n",
      "Epoch: [17][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6340(0.6340) Grad: 3385.6116  LR: 0.00590490  \n",
      "Epoch: [17][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6454(0.6497) Grad: 3976.8972  LR: 0.00590490  \n",
      "Epoch: [17][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7023(0.7023) Grad: 0.0000  \n",
      "Epoch: [17][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7105(0.7061) Grad: 0.0000  \n",
      "Epoch 17 - avg_train_loss: 0.6497  avg_val_loss: 0.7061  time: 0s\n",
      "Epoch 17 - Score: 0.8667\n",
      "Epoch: [18][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6425(0.6425) Grad: 2433.5881  LR: 0.00590490  \n",
      "Epoch: [18][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6764(0.6502) Grad: 3764.4861  LR: 0.00590490  \n",
      "Epoch: [18][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7007(0.7007) Grad: 0.0000  \n",
      "Epoch: [18][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7128(0.7064) Grad: 0.0000  \n",
      "Epoch 18 - avg_train_loss: 0.6502  avg_val_loss: 0.7064  time: 0s\n",
      "Epoch 18 - Score: 0.8583\n",
      "Epoch: [19][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6114(0.6114) Grad: 2451.8547  LR: 0.00590490  \n",
      "Epoch: [19][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7072(0.6437) Grad: 3206.5972  LR: 0.00531441  \n",
      "Epoch: [19][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6874(0.6874) Grad: 0.0000  \n",
      "Epoch: [19][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7062(0.6962) Grad: 0.0000  \n",
      "Epoch 19 - avg_train_loss: 0.6437  avg_val_loss: 0.6962  time: 0s\n",
      "Epoch 19 - Score: 0.8667\n",
      "Epoch: [20][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6699(0.6699) Grad: 1538.9960  LR: 0.00531441  \n",
      "Epoch: [20][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6455) Grad: 2878.0532  LR: 0.00531441  \n",
      "Epoch: [20][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6955(0.6955) Grad: 0.0000  \n",
      "Epoch: [20][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7093(0.7019) Grad: 0.0000  \n",
      "Epoch 20 - avg_train_loss: 0.6455  avg_val_loss: 0.7019  time: 0s\n",
      "Epoch 20 - Score: 0.8583\n",
      "Epoch: [21][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6305(0.6305) Grad: 2905.3445  LR: 0.00531441  \n",
      "Epoch: [21][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6641(0.6408) Grad: 2169.9167  LR: 0.00531441  \n",
      "Epoch: [21][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6967(0.6967) Grad: 0.0000  \n",
      "Epoch: [21][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7144(0.7050) Grad: 0.0000  \n",
      "Epoch 21 - avg_train_loss: 0.6408  avg_val_loss: 0.7050  time: 0s\n",
      "Epoch 21 - Score: 0.8583\n",
      "Epoch: [22][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6269(0.6269) Grad: 2674.0466  LR: 0.00531441  \n",
      "Epoch: [22][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6519(0.6361) Grad: 2382.7000  LR: 0.00430467  \n",
      "Epoch: [22][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6997(0.6997) Grad: 0.0000  \n",
      "Epoch: [22][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7170(0.7078) Grad: 0.0000  \n",
      "Epoch 22 - avg_train_loss: 0.6361  avg_val_loss: 0.7078  time: 0s\n",
      "Epoch 22 - Score: 0.8583\n",
      "Epoch: [23][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6389(0.6389) Grad: 2071.5706  LR: 0.00478297  \n",
      "Epoch: [23][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6554(0.6348) Grad: 2858.3906  LR: 0.00478297  \n",
      "Epoch: [23][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6993(0.6993) Grad: 0.0000  \n",
      "Epoch: [23][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7094(0.7040) Grad: 0.0000  \n",
      "Epoch 23 - avg_train_loss: 0.6348  avg_val_loss: 0.7040  time: 0s\n",
      "Epoch 23 - Score: 0.8625\n",
      "Epoch: [24][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6322(0.6322) Grad: 1878.7963  LR: 0.00478297  \n",
      "Epoch: [24][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6515(0.6384) Grad: 2612.4839  LR: 0.00478297  \n",
      "Epoch: [24][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6748(0.6748) Grad: 0.0000  \n",
      "Epoch: [24][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6988(0.6860) Grad: 0.0000  \n",
      "Epoch 24 - avg_train_loss: 0.6384  avg_val_loss: 0.6860  time: 0s\n",
      "Epoch 24 - Score: 0.8667\n",
      "Epoch: [25][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6147(0.6147) Grad: 2132.0525  LR: 0.00478297  \n",
      "Epoch: [25][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6248(0.6272) Grad: 2114.5586  LR: 0.00478297  \n",
      "Epoch: [25][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6829(0.6829) Grad: 0.0000  \n",
      "Epoch: [25][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7032(0.6924) Grad: 0.0000  \n",
      "Epoch 25 - avg_train_loss: 0.6272  avg_val_loss: 0.6924  time: 0s\n",
      "Epoch 25 - Score: 0.8667\n",
      "Epoch: [26][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6260(0.6260) Grad: 1405.5748  LR: 0.00387420  \n",
      "Epoch: [26][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6062(0.6343) Grad: 2445.0020  LR: 0.00430467  \n",
      "Epoch: [26][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6737(0.6737) Grad: 0.0000  \n",
      "Epoch: [26][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7000(0.6859) Grad: 0.0000  \n",
      "Epoch 26 - avg_train_loss: 0.6343  avg_val_loss: 0.6859  time: 0s\n",
      "Epoch 26 - Score: 0.8708\n",
      "Epoch: [27][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6466(0.6466) Grad: 1112.6316  LR: 0.00430467  \n",
      "Epoch: [27][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6285(0.6324) Grad: 3667.6387  LR: 0.00430467  \n",
      "Epoch: [27][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6804(0.6804) Grad: 0.0000  \n",
      "Epoch: [27][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7029(0.6909) Grad: 0.0000  \n",
      "Epoch 27 - avg_train_loss: 0.6324  avg_val_loss: 0.6909  time: 0s\n",
      "Epoch 27 - Score: 0.8708\n",
      "Epoch: [28][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6740(0.6740) Grad: 2471.0615  LR: 0.00430467  \n",
      "Epoch: [28][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6354(0.6337) Grad: 2423.5337  LR: 0.00430467  \n",
      "Epoch: [28][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6781(0.6781) Grad: 0.0000  \n",
      "Epoch: [28][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6979(0.6873) Grad: 0.0000  \n",
      "Epoch 28 - avg_train_loss: 0.6337  avg_val_loss: 0.6873  time: 0s\n",
      "Epoch 28 - Score: 0.8708\n",
      "Epoch: [29][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6203(0.6203) Grad: 1579.6637  LR: 0.00430467  \n",
      "Epoch: [29][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6211(0.6250) Grad: 2706.5920  LR: 0.00387420  \n",
      "Epoch: [29][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6665(0.6665) Grad: 0.0000  \n",
      "Epoch: [29][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6932(0.6790) Grad: 0.0000  \n",
      "Epoch 29 - avg_train_loss: 0.6250  avg_val_loss: 0.6790  time: 0s\n",
      "Epoch 29 - Score: 0.8792\n",
      "Epoch 29 - Save Best Score: 0.8792 Model\n",
      "Epoch: [30][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6163(0.6163) Grad: 2256.9692  LR: 0.00387420  \n",
      "Epoch: [30][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6335(0.6373) Grad: 2375.7429  LR: 0.00387420  \n",
      "Epoch: [30][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [30][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6678(0.6585) Grad: 0.0000  \n",
      "Epoch 30 - avg_train_loss: 0.6373  avg_val_loss: 0.6585  time: 0s\n",
      "Epoch 30 - Score: 0.9083\n",
      "Epoch 30 - Save Best Score: 0.9083 Model\n",
      "Epoch: [31][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6447) Grad: 2023.2053  LR: 0.00387420  \n",
      "Epoch: [31][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6130(0.6276) Grad: 1446.4847  LR: 0.00387420  \n",
      "Epoch: [31][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6586(0.6586) Grad: 0.0000  \n",
      "Epoch: [31][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6975(0.6767) Grad: 0.0000  \n",
      "Epoch 31 - avg_train_loss: 0.6276  avg_val_loss: 0.6767  time: 0s\n",
      "Epoch 31 - Score: 0.8792\n",
      "Epoch: [32][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6106(0.6106) Grad: 3263.2542  LR: 0.00387420  \n",
      "Epoch: [32][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6517(0.6301) Grad: 3589.8152  LR: 0.00348678  \n",
      "Epoch: [32][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6532(0.6532) Grad: 0.0000  \n",
      "Epoch: [32][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6944(0.6724) Grad: 0.0000  \n",
      "Epoch 32 - avg_train_loss: 0.6301  avg_val_loss: 0.6724  time: 0s\n",
      "Epoch 32 - Score: 0.8833\n",
      "Epoch: [33][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6387(0.6387) Grad: 860.3690  LR: 0.00348678  \n",
      "Epoch: [33][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5939(0.6200) Grad: 2373.1204  LR: 0.00348678  \n",
      "Epoch: [33][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6477(0.6477) Grad: 0.0000  \n",
      "Epoch: [33][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6696(0.6579) Grad: 0.0000  \n",
      "Epoch 33 - avg_train_loss: 0.6200  avg_val_loss: 0.6579  time: 0s\n",
      "Epoch 33 - Score: 0.9042\n",
      "Epoch: [34][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6370(0.6370) Grad: 1397.7456  LR: 0.00348678  \n",
      "Epoch: [34][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6124(0.6223) Grad: 1961.0983  LR: 0.00348678  \n",
      "Epoch: [34][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6506(0.6506) Grad: 0.0000  \n",
      "Epoch: [34][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6695(0.6595) Grad: 0.0000  \n",
      "Epoch 34 - avg_train_loss: 0.6223  avg_val_loss: 0.6595  time: 0s\n",
      "Epoch 34 - Score: 0.9042\n",
      "Epoch: [35][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6141(0.6141) Grad: 1478.0576  LR: 0.00348678  \n",
      "Epoch: [35][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6292(0.6275) Grad: 1488.0995  LR: 0.00313811  \n",
      "Epoch: [35][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6564(0.6564) Grad: 0.0000  \n",
      "Epoch: [35][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6858(0.6701) Grad: 0.0000  \n",
      "Epoch 35 - avg_train_loss: 0.6275  avg_val_loss: 0.6701  time: 0s\n",
      "Epoch 35 - Score: 0.9000\n",
      "Epoch: [36][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6471(0.6471) Grad: 1258.1522  LR: 0.00313811  \n",
      "Epoch: [36][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6264(0.6302) Grad: 3578.7585  LR: 0.00313811  \n",
      "Epoch: [36][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6473(0.6473) Grad: 0.0000  \n",
      "Epoch: [36][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6654(0.6557) Grad: 0.0000  \n",
      "Epoch 36 - avg_train_loss: 0.6302  avg_val_loss: 0.6557  time: 0s\n",
      "Epoch 36 - Score: 0.9042\n",
      "Epoch: [37][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6316(0.6316) Grad: 6878.0635  LR: 0.00313811  \n",
      "Epoch: [37][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5860(0.6253) Grad: 948.5955  LR: 0.00313811  \n",
      "Epoch: [37][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6446) Grad: 0.0000  \n",
      "Epoch: [37][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6592(0.6514) Grad: 0.0000  \n",
      "Epoch 37 - avg_train_loss: 0.6253  avg_val_loss: 0.6514  time: 0s\n",
      "Epoch 37 - Score: 0.9083\n",
      "Epoch: [38][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6307(0.6307) Grad: 1981.4576  LR: 0.00313811  \n",
      "Epoch: [38][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6191(0.6262) Grad: 3465.3958  LR: 0.00282430  \n",
      "Epoch: [38][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6449) Grad: 0.0000  \n",
      "Epoch: [38][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6589(0.6515) Grad: 0.0000  \n",
      "Epoch 38 - avg_train_loss: 0.6262  avg_val_loss: 0.6515  time: 0s\n",
      "Epoch 38 - Score: 0.9083\n",
      "Epoch: [39][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6470(0.6470) Grad: 1445.8834  LR: 0.00282430  \n",
      "Epoch: [39][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6218(0.6296) Grad: 758.0441  LR: 0.00282430  \n",
      "Epoch: [39][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6452(0.6452) Grad: 0.0000  \n",
      "Epoch: [39][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6657(0.6547) Grad: 0.0000  \n",
      "Epoch 39 - avg_train_loss: 0.6296  avg_val_loss: 0.6547  time: 0s\n",
      "Epoch 39 - Score: 0.9083\n",
      "Epoch: [40][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6329(0.6329) Grad: 1349.9841  LR: 0.00282430  \n",
      "Epoch: [40][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6090(0.6243) Grad: 1358.4264  LR: 0.00282430  \n",
      "Epoch: [40][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [40][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6578(0.6535) Grad: 0.0000  \n",
      "Epoch 40 - avg_train_loss: 0.6243  avg_val_loss: 0.6535  time: 0s\n",
      "Epoch 40 - Score: 0.9042\n",
      "Epoch: [41][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6368(0.6368) Grad: 2113.9744  LR: 0.00282430  \n",
      "Epoch: [41][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6223) Grad: 4362.2451  LR: 0.00254187  \n",
      "Epoch: [41][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6477(0.6477) Grad: 0.0000  \n",
      "Epoch: [41][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6656(0.6561) Grad: 0.0000  \n",
      "Epoch 41 - avg_train_loss: 0.6223  avg_val_loss: 0.6561  time: 0s\n",
      "Epoch 41 - Score: 0.9000\n",
      "Epoch: [42][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6573(0.6573) Grad: 2362.1426  LR: 0.00254187  \n",
      "Epoch: [42][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6219) Grad: 1970.0045  LR: 0.00254187  \n",
      "Epoch: [42][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6530(0.6530) Grad: 0.0000  \n",
      "Epoch: [42][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6600(0.6562) Grad: 0.0000  \n",
      "Epoch 42 - avg_train_loss: 0.6219  avg_val_loss: 0.6562  time: 0s\n",
      "Epoch 42 - Score: 0.9042\n",
      "Epoch: [43][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5983(0.5983) Grad: 712.1282  LR: 0.00254187  \n",
      "Epoch: [43][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6200) Grad: 6347.8057  LR: 0.00254187  \n",
      "Epoch: [43][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6557(0.6557) Grad: 0.0000  \n",
      "Epoch: [43][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6581(0.6568) Grad: 0.0000  \n",
      "Epoch 43 - avg_train_loss: 0.6200  avg_val_loss: 0.6568  time: 0s\n",
      "Epoch 43 - Score: 0.9000\n",
      "Epoch: [44][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6152(0.6152) Grad: 7885.6367  LR: 0.00254187  \n",
      "Epoch: [44][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6267) Grad: 1420.9391  LR: 0.00205891  \n",
      "Epoch: [44][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [44][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6574(0.6534) Grad: 0.0000  \n",
      "Epoch 44 - avg_train_loss: 0.6267  avg_val_loss: 0.6534  time: 0s\n",
      "Epoch 44 - Score: 0.9042\n",
      "Epoch: [45][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6389(0.6389) Grad: 3131.2869  LR: 0.00228768  \n",
      "Epoch: [45][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6132(0.6221) Grad: 1511.2996  LR: 0.00228768  \n",
      "Epoch: [45][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6475(0.6475) Grad: 0.0000  \n",
      "Epoch: [45][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6579(0.6524) Grad: 0.0000  \n",
      "Epoch 45 - avg_train_loss: 0.6221  avg_val_loss: 0.6524  time: 0s\n",
      "Epoch 45 - Score: 0.9042\n",
      "Epoch: [46][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6332(0.6332) Grad: 1869.4813  LR: 0.00228768  \n",
      "Epoch: [46][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5856(0.6259) Grad: 1035.4641  LR: 0.00228768  \n",
      "Epoch: [46][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6579(0.6579) Grad: 0.0000  \n",
      "Epoch: [46][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6550(0.6566) Grad: 0.0000  \n",
      "Epoch 46 - avg_train_loss: 0.6259  avg_val_loss: 0.6566  time: 0s\n",
      "Epoch 46 - Score: 0.9000\n",
      "Epoch: [47][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6126(0.6126) Grad: 2499.4221  LR: 0.00228768  \n",
      "Epoch: [47][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6131(0.6223) Grad: 1207.0801  LR: 0.00228768  \n",
      "Epoch: [47][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6577(0.6577) Grad: 0.0000  \n",
      "Epoch: [47][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6554(0.6567) Grad: 0.0000  \n",
      "Epoch 47 - avg_train_loss: 0.6223  avg_val_loss: 0.6567  time: 0s\n",
      "Epoch 47 - Score: 0.9000\n",
      "Epoch: [48][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6503) Grad: 1070.7053  LR: 0.00185302  \n",
      "Epoch: [48][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6014(0.6244) Grad: 4158.6235  LR: 0.00205891  \n",
      "Epoch: [48][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6583(0.6583) Grad: 0.0000  \n",
      "Epoch: [48][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6530(0.6559) Grad: 0.0000  \n",
      "Epoch 48 - avg_train_loss: 0.6244  avg_val_loss: 0.6559  time: 0s\n",
      "Epoch 48 - Score: 0.9000\n",
      "Epoch: [49][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 716.4454  LR: 0.00205891  \n",
      "Epoch: [49][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6278(0.6213) Grad: 1066.2966  LR: 0.00205891  \n",
      "Epoch: [49][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6525(0.6525) Grad: 0.0000  \n",
      "Epoch: [49][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6572(0.6547) Grad: 0.0000  \n",
      "Epoch 49 - avg_train_loss: 0.6213  avg_val_loss: 0.6547  time: 0s\n",
      "Epoch 49 - Score: 0.9042\n",
      "Epoch: [50][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6408(0.6408) Grad: 3102.8767  LR: 0.00205891  \n",
      "Epoch: [50][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6152) Grad: 683.2374  LR: 0.00205891  \n",
      "Epoch: [50][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6459(0.6459) Grad: 0.0000  \n",
      "Epoch: [50][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6611(0.6530) Grad: 0.0000  \n",
      "Epoch 50 - avg_train_loss: 0.6152  avg_val_loss: 0.6530  time: 0s\n",
      "Epoch 50 - Score: 0.9042\n",
      "Epoch: [51][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6134(0.6134) Grad: 610.3054  LR: 0.00205891  \n",
      "Epoch: [51][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6333(0.6174) Grad: 3277.5850  LR: 0.00185302  \n",
      "Epoch: [51][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6531(0.6531) Grad: 0.0000  \n",
      "Epoch: [51][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6557(0.6543) Grad: 0.0000  \n",
      "Epoch 51 - avg_train_loss: 0.6174  avg_val_loss: 0.6543  time: 0s\n",
      "Epoch 51 - Score: 0.9000\n",
      "Epoch: [52][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6129(0.6129) Grad: 2850.7878  LR: 0.00185302  \n",
      "Epoch: [52][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6314(0.6180) Grad: 3704.1697  LR: 0.00185302  \n",
      "Epoch: [52][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6551(0.6551) Grad: 0.0000  \n",
      "Epoch: [52][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6556(0.6553) Grad: 0.0000  \n",
      "Epoch 52 - avg_train_loss: 0.6180  avg_val_loss: 0.6553  time: 0s\n",
      "Epoch 52 - Score: 0.9000\n",
      "Epoch: [53][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6339(0.6339) Grad: 3362.8096  LR: 0.00185302  \n",
      "Epoch: [53][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6346(0.6203) Grad: 2217.0640  LR: 0.00185302  \n",
      "Epoch: [53][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [53][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6576(0.6534) Grad: 0.0000  \n",
      "Epoch 53 - avg_train_loss: 0.6203  avg_val_loss: 0.6534  time: 0s\n",
      "Epoch 53 - Score: 0.9042\n",
      "Epoch: [54][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6267(0.6267) Grad: 1138.0642  LR: 0.00185302  \n",
      "Epoch: [54][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6306(0.6153) Grad: 1581.5048  LR: 0.00166772  \n",
      "Epoch: [54][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6432(0.6432) Grad: 0.0000  \n",
      "Epoch: [54][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6619(0.6519) Grad: 0.0000  \n",
      "Epoch 54 - avg_train_loss: 0.6153  avg_val_loss: 0.6519  time: 0s\n",
      "Epoch 54 - Score: 0.9042\n",
      "Epoch: [55][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5820(0.5820) Grad: 601.5770  LR: 0.00166772  \n",
      "Epoch: [55][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6539(0.6216) Grad: 2242.4175  LR: 0.00166772  \n",
      "Epoch: [55][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6428(0.6428) Grad: 0.0000  \n",
      "Epoch: [55][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6657(0.6535) Grad: 0.0000  \n",
      "Epoch 55 - avg_train_loss: 0.6216  avg_val_loss: 0.6535  time: 0s\n",
      "Epoch 55 - Score: 0.9042\n",
      "Epoch: [56][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6269(0.6269) Grad: 534.3848  LR: 0.00166772  \n",
      "Epoch: [56][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6659(0.6204) Grad: 520.1289  LR: 0.00166772  \n",
      "Epoch: [56][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6487(0.6487) Grad: 0.0000  \n",
      "Epoch: [56][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6509(0.6497) Grad: 0.0000  \n",
      "Epoch 56 - avg_train_loss: 0.6204  avg_val_loss: 0.6497  time: 0s\n",
      "Epoch 56 - Score: 0.9042\n",
      "Epoch: [57][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6529(0.6529) Grad: 2153.2263  LR: 0.00166772  \n",
      "Epoch: [57][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6371(0.6182) Grad: 2609.4524  LR: 0.00150095  \n",
      "Epoch: [57][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6576(0.6576) Grad: 0.0000  \n",
      "Epoch: [57][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6528(0.6553) Grad: 0.0000  \n",
      "Epoch 57 - avg_train_loss: 0.6182  avg_val_loss: 0.6553  time: 0s\n",
      "Epoch 57 - Score: 0.9000\n",
      "Epoch: [58][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5718(0.5718) Grad: 2465.1628  LR: 0.00150095  \n",
      "Epoch: [58][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6366(0.6170) Grad: 1677.1139  LR: 0.00150095  \n",
      "Epoch: [58][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6576(0.6576) Grad: 0.0000  \n",
      "Epoch: [58][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6469(0.6526) Grad: 0.0000  \n",
      "Epoch 58 - avg_train_loss: 0.6170  avg_val_loss: 0.6526  time: 0s\n",
      "Epoch 58 - Score: 0.9042\n",
      "Epoch: [59][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6046) Grad: 934.9315  LR: 0.00150095  \n",
      "Epoch: [59][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6414(0.6204) Grad: 911.1135  LR: 0.00150095  \n",
      "Epoch: [59][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6577(0.6577) Grad: 0.0000  \n",
      "Epoch: [59][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6529(0.6555) Grad: 0.0000  \n",
      "Epoch 59 - avg_train_loss: 0.6204  avg_val_loss: 0.6555  time: 0s\n",
      "Epoch 59 - Score: 0.9000\n",
      "Epoch: [60][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6430(0.6430) Grad: 1692.2792  LR: 0.00150095  \n",
      "Epoch: [60][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6215(0.6223) Grad: 1740.1316  LR: 0.00135085  \n",
      "Epoch: [60][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6556(0.6556) Grad: 0.0000  \n",
      "Epoch: [60][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6551(0.6554) Grad: 0.0000  \n",
      "Epoch 60 - avg_train_loss: 0.6223  avg_val_loss: 0.6554  time: 0s\n",
      "Epoch 60 - Score: 0.9000\n",
      "Epoch: [61][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6018(0.6018) Grad: 868.1171  LR: 0.00135085  \n",
      "Epoch: [61][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6270(0.6217) Grad: 453.7028  LR: 0.00135085  \n",
      "Epoch: [61][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6574(0.6574) Grad: 0.0000  \n",
      "Epoch: [61][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6549(0.6562) Grad: 0.0000  \n",
      "Epoch 61 - avg_train_loss: 0.6217  avg_val_loss: 0.6562  time: 0s\n",
      "Epoch 61 - Score: 0.9000\n",
      "Epoch: [62][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6054(0.6054) Grad: 1641.7443  LR: 0.00135085  \n",
      "Epoch: [62][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6067(0.6182) Grad: 6218.5718  LR: 0.00135085  \n",
      "Epoch: [62][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6561(0.6561) Grad: 0.0000  \n",
      "Epoch: [62][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6558(0.6559) Grad: 0.0000  \n",
      "Epoch 62 - avg_train_loss: 0.6182  avg_val_loss: 0.6559  time: 0s\n",
      "Epoch 62 - Score: 0.9000\n",
      "Epoch: [63][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6199(0.6199) Grad: 527.3584  LR: 0.00135085  \n",
      "Epoch: [63][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6122(0.6223) Grad: 568.5543  LR: 0.00121577  \n",
      "Epoch: [63][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6540(0.6540) Grad: 0.0000  \n",
      "Epoch: [63][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6561(0.6550) Grad: 0.0000  \n",
      "Epoch 63 - avg_train_loss: 0.6223  avg_val_loss: 0.6550  time: 0s\n",
      "Epoch 63 - Score: 0.9000\n",
      "Epoch: [64][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6267(0.6267) Grad: 473.5342  LR: 0.00121577  \n",
      "Epoch: [64][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6173(0.6223) Grad: 2371.2151  LR: 0.00121577  \n",
      "Epoch: [64][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6533(0.6533) Grad: 0.0000  \n",
      "Epoch: [64][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6552(0.6542) Grad: 0.0000  \n",
      "Epoch 64 - avg_train_loss: 0.6223  avg_val_loss: 0.6542  time: 0s\n",
      "Epoch 64 - Score: 0.9000\n",
      "Epoch: [65][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6328(0.6328) Grad: 467.0656  LR: 0.00121577  \n",
      "Epoch: [65][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6074(0.6212) Grad: 3337.1060  LR: 0.00121577  \n",
      "Epoch: [65][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6481(0.6481) Grad: 0.0000  \n",
      "Epoch: [65][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6553(0.6515) Grad: 0.0000  \n",
      "Epoch 65 - avg_train_loss: 0.6212  avg_val_loss: 0.6515  time: 0s\n",
      "Epoch 65 - Score: 0.9042\n",
      "Epoch: [66][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5835(0.5835) Grad: 2485.9268  LR: 0.00121577  \n",
      "Epoch: [66][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6637(0.6207) Grad: 1465.4746  LR: 0.00098477  \n",
      "Epoch: [66][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6447) Grad: 0.0000  \n",
      "Epoch: [66][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6552(0.6496) Grad: 0.0000  \n",
      "Epoch 66 - avg_train_loss: 0.6207  avg_val_loss: 0.6496  time: 0s\n",
      "Epoch 66 - Score: 0.9083\n",
      "Epoch: [67][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6046) Grad: 3377.3372  LR: 0.00109419  \n",
      "Epoch: [67][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6228(0.6186) Grad: 2263.0872  LR: 0.00109419  \n",
      "Epoch: [67][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6494) Grad: 0.0000  \n",
      "Epoch: [67][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6551(0.6521) Grad: 0.0000  \n",
      "Epoch 67 - avg_train_loss: 0.6186  avg_val_loss: 0.6521  time: 0s\n",
      "Epoch 67 - Score: 0.9042\n",
      "Epoch: [68][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5747(0.5747) Grad: 1195.3540  LR: 0.00109419  \n",
      "Epoch: [68][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6147) Grad: 566.3525  LR: 0.00109419  \n",
      "Epoch: [68][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6545(0.6545) Grad: 0.0000  \n",
      "Epoch: [68][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6609(0.6575) Grad: 0.0000  \n",
      "Epoch 68 - avg_train_loss: 0.6147  avg_val_loss: 0.6575  time: 0s\n",
      "Epoch 68 - Score: 0.8958\n",
      "Epoch: [69][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6194) Grad: 1087.1479  LR: 0.00109419  \n",
      "Epoch: [69][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6230) Grad: 3321.1653  LR: 0.00109419  \n",
      "Epoch: [69][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6556(0.6556) Grad: 0.0000  \n",
      "Epoch: [69][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6576(0.6565) Grad: 0.0000  \n",
      "Epoch 69 - avg_train_loss: 0.6230  avg_val_loss: 0.6565  time: 0s\n",
      "Epoch 69 - Score: 0.9000\n",
      "Epoch: [70][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6342(0.6342) Grad: 525.6002  LR: 0.00088629  \n",
      "Epoch: [70][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6289(0.6182) Grad: 3272.7727  LR: 0.00098477  \n",
      "Epoch: [70][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6550(0.6550) Grad: 0.0000  \n",
      "Epoch: [70][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6550(0.6550) Grad: 0.0000  \n",
      "Epoch 70 - avg_train_loss: 0.6182  avg_val_loss: 0.6550  time: 0s\n",
      "Epoch 70 - Score: 0.9000\n",
      "Epoch: [71][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5963(0.5963) Grad: 698.1263  LR: 0.00098477  \n",
      "Epoch: [71][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6045(0.6171) Grad: 656.3714  LR: 0.00098477  \n",
      "Epoch: [71][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6562(0.6562) Grad: 0.0000  \n",
      "Epoch: [71][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6548(0.6555) Grad: 0.0000  \n",
      "Epoch 71 - avg_train_loss: 0.6171  avg_val_loss: 0.6555  time: 0s\n",
      "Epoch 71 - Score: 0.9000\n",
      "Epoch: [72][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6097) Grad: 399.1891  LR: 0.00098477  \n",
      "Epoch: [72][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6575(0.6195) Grad: 1135.3407  LR: 0.00098477  \n",
      "Epoch: [72][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6572(0.6572) Grad: 0.0000  \n",
      "Epoch: [72][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6562(0.6567) Grad: 0.0000  \n",
      "Epoch 72 - avg_train_loss: 0.6195  avg_val_loss: 0.6567  time: 0s\n",
      "Epoch 72 - Score: 0.9000\n",
      "Epoch: [73][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6178) Grad: 1831.0746  LR: 0.00098477  \n",
      "Epoch: [73][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6204(0.6200) Grad: 4476.2920  LR: 0.00088629  \n",
      "Epoch: [73][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6566(0.6566) Grad: 0.0000  \n",
      "Epoch: [73][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6622(0.6592) Grad: 0.0000  \n",
      "Epoch 73 - avg_train_loss: 0.6200  avg_val_loss: 0.6592  time: 0s\n",
      "Epoch 73 - Score: 0.8958\n",
      "Epoch: [74][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6397(0.6397) Grad: 1302.1182  LR: 0.00088629  \n",
      "Epoch: [74][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6603(0.6218) Grad: 3589.4753  LR: 0.00088629  \n",
      "Epoch: [74][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6570(0.6570) Grad: 0.0000  \n",
      "Epoch: [74][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6592(0.6580) Grad: 0.0000  \n",
      "Epoch 74 - avg_train_loss: 0.6218  avg_val_loss: 0.6580  time: 0s\n",
      "Epoch 74 - Score: 0.8958\n",
      "Epoch: [75][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6040(0.6040) Grad: 5524.9434  LR: 0.00088629  \n",
      "Epoch: [75][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6424(0.6148) Grad: 515.3325  LR: 0.00088629  \n",
      "Epoch: [75][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6568(0.6568) Grad: 0.0000  \n",
      "Epoch: [75][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6558(0.6563) Grad: 0.0000  \n",
      "Epoch 75 - avg_train_loss: 0.6148  avg_val_loss: 0.6563  time: 0s\n",
      "Epoch 75 - Score: 0.9000\n",
      "Epoch: [76][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6163(0.6163) Grad: 2572.3447  LR: 0.00088629  \n",
      "Epoch: [76][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5959(0.6142) Grad: 482.4532  LR: 0.00079766  \n",
      "Epoch: [76][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6567(0.6567) Grad: 0.0000  \n",
      "Epoch: [76][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6563(0.6565) Grad: 0.0000  \n",
      "Epoch 76 - avg_train_loss: 0.6142  avg_val_loss: 0.6565  time: 0s\n",
      "Epoch 76 - Score: 0.9000\n",
      "Epoch: [77][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6209(0.6209) Grad: 866.8632  LR: 0.00079766  \n",
      "Epoch: [77][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6112(0.6147) Grad: 1159.6763  LR: 0.00079766  \n",
      "Epoch: [77][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6566(0.6566) Grad: 0.0000  \n",
      "Epoch: [77][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6558(0.6562) Grad: 0.0000  \n",
      "Epoch 77 - avg_train_loss: 0.6147  avg_val_loss: 0.6562  time: 0s\n",
      "Epoch 77 - Score: 0.9000\n",
      "Epoch: [78][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6286(0.6286) Grad: 591.9678  LR: 0.00079766  \n",
      "Epoch: [78][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6166(0.6194) Grad: 4233.8579  LR: 0.00079766  \n",
      "Epoch: [78][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6568(0.6568) Grad: 0.0000  \n",
      "Epoch: [78][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6578(0.6573) Grad: 0.0000  \n",
      "Epoch 78 - avg_train_loss: 0.6194  avg_val_loss: 0.6573  time: 0s\n",
      "Epoch 78 - Score: 0.8958\n",
      "Epoch: [79][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6176(0.6176) Grad: 514.9292  LR: 0.00079766  \n",
      "Epoch: [79][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6290(0.6134) Grad: 1013.8430  LR: 0.00071790  \n",
      "Epoch: [79][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6557(0.6557) Grad: 0.0000  \n",
      "Epoch: [79][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6613(0.6583) Grad: 0.0000  \n",
      "Epoch 79 - avg_train_loss: 0.6134  avg_val_loss: 0.6583  time: 0s\n",
      "Epoch 79 - Score: 0.8958\n",
      "Epoch: [80][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6366(0.6366) Grad: 4764.1621  LR: 0.00071790  \n",
      "Epoch: [80][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6171(0.6163) Grad: 516.6671  LR: 0.00071790  \n",
      "Epoch: [80][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6568(0.6568) Grad: 0.0000  \n",
      "Epoch: [80][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6567(0.6567) Grad: 0.0000  \n",
      "Epoch 80 - avg_train_loss: 0.6163  avg_val_loss: 0.6567  time: 0s\n",
      "Epoch 80 - Score: 0.9000\n",
      "Epoch: [81][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6118(0.6118) Grad: 530.7965  LR: 0.00071790  \n",
      "Epoch: [81][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6023(0.6174) Grad: 558.4255  LR: 0.00071790  \n",
      "Epoch: [81][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6554(0.6554) Grad: 0.0000  \n",
      "Epoch: [81][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6550(0.6552) Grad: 0.0000  \n",
      "Epoch 81 - avg_train_loss: 0.6174  avg_val_loss: 0.6552  time: 0s\n",
      "Epoch 81 - Score: 0.9000\n",
      "Epoch: [82][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6241(0.6241) Grad: 2267.8755  LR: 0.00071790  \n",
      "Epoch: [82][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5975(0.6215) Grad: 2035.7754  LR: 0.00064611  \n",
      "Epoch: [82][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6516(0.6516) Grad: 0.0000  \n",
      "Epoch: [82][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6551(0.6533) Grad: 0.0000  \n",
      "Epoch 82 - avg_train_loss: 0.6215  avg_val_loss: 0.6533  time: 0s\n",
      "Epoch 82 - Score: 0.9000\n",
      "Epoch: [83][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 655.3164  LR: 0.00064611  \n",
      "Epoch: [83][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5947(0.6149) Grad: 494.9979  LR: 0.00064611  \n",
      "Epoch: [83][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6534(0.6534) Grad: 0.0000  \n",
      "Epoch: [83][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6546(0.6540) Grad: 0.0000  \n",
      "Epoch 83 - avg_train_loss: 0.6149  avg_val_loss: 0.6540  time: 0s\n",
      "Epoch 83 - Score: 0.9000\n",
      "Epoch: [84][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6271(0.6271) Grad: 1222.4115  LR: 0.00064611  \n",
      "Epoch: [84][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6241(0.6194) Grad: 1584.5957  LR: 0.00064611  \n",
      "Epoch: [84][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6540(0.6540) Grad: 0.0000  \n",
      "Epoch: [84][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6545(0.6542) Grad: 0.0000  \n",
      "Epoch 84 - avg_train_loss: 0.6194  avg_val_loss: 0.6542  time: 0s\n",
      "Epoch 84 - Score: 0.9000\n",
      "Epoch: [85][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6190(0.6190) Grad: 6061.5815  LR: 0.00064611  \n",
      "Epoch: [85][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6105(0.6127) Grad: 1409.1625  LR: 0.00058150  \n",
      "Epoch: [85][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6552(0.6552) Grad: 0.0000  \n",
      "Epoch: [85][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6548(0.6550) Grad: 0.0000  \n",
      "Epoch 85 - avg_train_loss: 0.6127  avg_val_loss: 0.6550  time: 0s\n",
      "Epoch 85 - Score: 0.9000\n",
      "Epoch: [86][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6045(0.6045) Grad: 855.9221  LR: 0.00058150  \n",
      "Epoch: [86][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5822(0.6194) Grad: 1822.2159  LR: 0.00058150  \n",
      "Epoch: [86][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6556(0.6556) Grad: 0.0000  \n",
      "Epoch: [86][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6571(0.6563) Grad: 0.0000  \n",
      "Epoch 86 - avg_train_loss: 0.6194  avg_val_loss: 0.6563  time: 0s\n",
      "Epoch 86 - Score: 0.9000\n",
      "Epoch: [87][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5891(0.5891) Grad: 1878.2646  LR: 0.00058150  \n",
      "Epoch: [87][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6156) Grad: 1227.8240  LR: 0.00058150  \n",
      "Epoch: [87][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6551(0.6551) Grad: 0.0000  \n",
      "Epoch: [87][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6582(0.6565) Grad: 0.0000  \n",
      "Epoch 87 - avg_train_loss: 0.6156  avg_val_loss: 0.6565  time: 0s\n",
      "Epoch 87 - Score: 0.8958\n",
      "Epoch: [88][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6127(0.6127) Grad: 1234.1832  LR: 0.00058150  \n",
      "Epoch: [88][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6131) Grad: 930.5858  LR: 0.00047101  \n",
      "Epoch: [88][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6536) Grad: 0.0000  \n",
      "Epoch: [88][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6594(0.6563) Grad: 0.0000  \n",
      "Epoch 88 - avg_train_loss: 0.6131  avg_val_loss: 0.6563  time: 0s\n",
      "Epoch 88 - Score: 0.8958\n",
      "Epoch: [89][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5982(0.5982) Grad: 2369.0627  LR: 0.00052335  \n",
      "Epoch: [89][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6245(0.6115) Grad: 3775.6614  LR: 0.00052335  \n",
      "Epoch: [89][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6542(0.6542) Grad: 0.0000  \n",
      "Epoch: [89][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6566(0.6553) Grad: 0.0000  \n",
      "Epoch 89 - avg_train_loss: 0.6115  avg_val_loss: 0.6553  time: 0s\n",
      "Epoch 89 - Score: 0.9000\n",
      "Epoch: [90][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5907(0.5907) Grad: 3372.7012  LR: 0.00052335  \n",
      "Epoch: [90][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6168) Grad: 3216.8755  LR: 0.00052335  \n",
      "Epoch: [90][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6530(0.6530) Grad: 0.0000  \n",
      "Epoch: [90][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6577(0.6552) Grad: 0.0000  \n",
      "Epoch 90 - avg_train_loss: 0.6168  avg_val_loss: 0.6552  time: 0s\n",
      "Epoch 90 - Score: 0.9000\n",
      "Epoch: [91][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6160(0.6160) Grad: 3560.1406  LR: 0.00052335  \n",
      "Epoch: [91][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5846(0.6201) Grad: 1994.7052  LR: 0.00052335  \n",
      "Epoch: [91][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6519(0.6519) Grad: 0.0000  \n",
      "Epoch: [91][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6578(0.6547) Grad: 0.0000  \n",
      "Epoch 91 - avg_train_loss: 0.6201  avg_val_loss: 0.6547  time: 0s\n",
      "Epoch 91 - Score: 0.9000\n",
      "Epoch: [92][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6152(0.6152) Grad: 3363.9075  LR: 0.00042391  \n",
      "Epoch: [92][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6038(0.6150) Grad: 5235.4209  LR: 0.00047101  \n",
      "Epoch: [92][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6502) Grad: 0.0000  \n",
      "Epoch: [92][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6622(0.6558) Grad: 0.0000  \n",
      "Epoch 92 - avg_train_loss: 0.6150  avg_val_loss: 0.6558  time: 0s\n",
      "Epoch 92 - Score: 0.8958\n",
      "Epoch: [93][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6197(0.6197) Grad: 1928.3176  LR: 0.00047101  \n",
      "Epoch: [93][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6140(0.6118) Grad: 2403.6643  LR: 0.00047101  \n",
      "Epoch: [93][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6548(0.6548) Grad: 0.0000  \n",
      "Epoch: [93][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6593(0.6569) Grad: 0.0000  \n",
      "Epoch 93 - avg_train_loss: 0.6118  avg_val_loss: 0.6569  time: 0s\n",
      "Epoch 93 - Score: 0.8958\n",
      "Epoch: [94][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6111(0.6111) Grad: 697.2611  LR: 0.00047101  \n",
      "Epoch: [94][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6368(0.6141) Grad: 6220.7905  LR: 0.00047101  \n",
      "Epoch: [94][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6563(0.6563) Grad: 0.0000  \n",
      "Epoch: [94][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6608(0.6584) Grad: 0.0000  \n",
      "Epoch 94 - avg_train_loss: 0.6141  avg_val_loss: 0.6584  time: 0s\n",
      "Epoch 94 - Score: 0.8958\n",
      "Epoch: [95][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6090(0.6090) Grad: 2907.3081  LR: 0.00047101  \n",
      "Epoch: [95][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6039(0.6183) Grad: 668.0383  LR: 0.00042391  \n",
      "Epoch: [95][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6558(0.6558) Grad: 0.0000  \n",
      "Epoch: [95][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6605(0.6580) Grad: 0.0000  \n",
      "Epoch 95 - avg_train_loss: 0.6183  avg_val_loss: 0.6580  time: 0s\n",
      "Epoch 95 - Score: 0.8958\n",
      "Epoch: [96][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6033(0.6033) Grad: 452.6859  LR: 0.00042391  \n",
      "Epoch: [96][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6345(0.6177) Grad: 827.9858  LR: 0.00042391  \n",
      "Epoch: [96][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6540(0.6540) Grad: 0.0000  \n",
      "Epoch: [96][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6575(0.6556) Grad: 0.0000  \n",
      "Epoch 96 - avg_train_loss: 0.6177  avg_val_loss: 0.6556  time: 0s\n",
      "Epoch 96 - Score: 0.9000\n",
      "Epoch: [97][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6040(0.6040) Grad: 4054.8022  LR: 0.00042391  \n",
      "Epoch: [97][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6509(0.6143) Grad: 3030.7371  LR: 0.00042391  \n",
      "Epoch: [97][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6532(0.6532) Grad: 0.0000  \n",
      "Epoch: [97][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6570(0.6550) Grad: 0.0000  \n",
      "Epoch 97 - avg_train_loss: 0.6143  avg_val_loss: 0.6550  time: 0s\n",
      "Epoch 97 - Score: 0.9000\n",
      "Epoch: [98][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6241(0.6241) Grad: 3365.3376  LR: 0.00042391  \n",
      "Epoch: [98][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6306(0.6162) Grad: 2693.8735  LR: 0.00038152  \n",
      "Epoch: [98][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6513) Grad: 0.0000  \n",
      "Epoch: [98][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6607(0.6557) Grad: 0.0000  \n",
      "Epoch 98 - avg_train_loss: 0.6162  avg_val_loss: 0.6557  time: 0s\n",
      "Epoch 98 - Score: 0.9042\n",
      "Epoch: [99][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6114(0.6114) Grad: 1776.6132  LR: 0.00038152  \n",
      "Epoch: [99][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6736(0.6165) Grad: 3354.0515  LR: 0.00038152  \n",
      "Epoch: [99][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6533(0.6533) Grad: 0.0000  \n",
      "Epoch: [99][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6576(0.6553) Grad: 0.0000  \n",
      "Epoch 99 - avg_train_loss: 0.6165  avg_val_loss: 0.6553  time: 0s\n",
      "Epoch 99 - Score: 0.9000\n",
      "Epoch: [100][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6223(0.6223) Grad: 3539.1138  LR: 0.00038152  \n",
      "Epoch: [100][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5872(0.6163) Grad: 1047.4951  LR: 0.00038152  \n",
      "Epoch: [100][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6545(0.6545) Grad: 0.0000  \n",
      "Epoch: [100][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6587(0.6565) Grad: 0.0000  \n",
      "Epoch 100 - avg_train_loss: 0.6163  avg_val_loss: 0.6565  time: 0s\n",
      "Epoch 100 - Score: 0.9000\n",
      "Epoch: [101][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6328(0.6328) Grad: 4932.0112  LR: 0.00038152  \n",
      "Epoch: [101][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6266(0.6125) Grad: 3742.4915  LR: 0.00034337  \n",
      "Epoch: [101][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6536) Grad: 0.0000  \n",
      "Epoch: [101][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6611(0.6571) Grad: 0.0000  \n",
      "Epoch 101 - avg_train_loss: 0.6125  avg_val_loss: 0.6571  time: 0s\n",
      "Epoch 101 - Score: 0.8958\n",
      "Epoch: [102][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5911(0.5911) Grad: 2994.9268  LR: 0.00034337  \n",
      "Epoch: [102][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6248(0.6104) Grad: 3122.2637  LR: 0.00034337  \n",
      "Epoch: [102][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6533(0.6533) Grad: 0.0000  \n",
      "Epoch: [102][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6624(0.6576) Grad: 0.0000  \n",
      "Epoch 102 - avg_train_loss: 0.6104  avg_val_loss: 0.6576  time: 0s\n",
      "Epoch 102 - Score: 0.8958\n",
      "Epoch: [103][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5829(0.5829) Grad: 1828.5190  LR: 0.00034337  \n",
      "Epoch: [103][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6054(0.6147) Grad: 2558.4165  LR: 0.00034337  \n",
      "Epoch: [103][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6549(0.6549) Grad: 0.0000  \n",
      "Epoch: [103][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6628(0.6586) Grad: 0.0000  \n",
      "Epoch 103 - avg_train_loss: 0.6147  avg_val_loss: 0.6586  time: 0s\n",
      "Epoch 103 - Score: 0.8917\n",
      "Epoch: [104][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5952(0.5952) Grad: 3287.4387  LR: 0.00034337  \n",
      "Epoch: [104][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6396(0.6165) Grad: 2623.1238  LR: 0.00030903  \n",
      "Epoch: [104][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6556(0.6556) Grad: 0.0000  \n",
      "Epoch: [104][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6611(0.6582) Grad: 0.0000  \n",
      "Epoch 104 - avg_train_loss: 0.6165  avg_val_loss: 0.6582  time: 0s\n",
      "Epoch 104 - Score: 0.8958\n",
      "Epoch: [105][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6162(0.6162) Grad: 3289.3350  LR: 0.00030903  \n",
      "Epoch: [105][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5906(0.6097) Grad: 2494.3340  LR: 0.00030903  \n",
      "Epoch: [105][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6550(0.6550) Grad: 0.0000  \n",
      "Epoch: [105][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6638(0.6591) Grad: 0.0000  \n",
      "Epoch 105 - avg_train_loss: 0.6097  avg_val_loss: 0.6591  time: 0s\n",
      "Epoch 105 - Score: 0.8917\n",
      "Epoch: [106][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6121(0.6121) Grad: 555.2292  LR: 0.00030903  \n",
      "Epoch: [106][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6519(0.6163) Grad: 3449.8325  LR: 0.00030903  \n",
      "Epoch: [106][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6526(0.6526) Grad: 0.0000  \n",
      "Epoch: [106][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6638(0.6578) Grad: 0.0000  \n",
      "Epoch 106 - avg_train_loss: 0.6163  avg_val_loss: 0.6578  time: 0s\n",
      "Epoch 106 - Score: 0.9000\n",
      "Epoch: [107][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6572(0.6572) Grad: 3106.6204  LR: 0.00030903  \n",
      "Epoch: [107][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6427(0.6130) Grad: 1071.3049  LR: 0.00027813  \n",
      "Epoch: [107][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6508) Grad: 0.0000  \n",
      "Epoch: [107][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6641(0.6570) Grad: 0.0000  \n",
      "Epoch 107 - avg_train_loss: 0.6130  avg_val_loss: 0.6570  time: 0s\n",
      "Epoch 107 - Score: 0.9000\n",
      "Epoch: [108][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6143(0.6143) Grad: 4313.8989  LR: 0.00027813  \n",
      "Epoch: [108][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6026(0.6133) Grad: 2355.5872  LR: 0.00027813  \n",
      "Epoch: [108][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6510(0.6510) Grad: 0.0000  \n",
      "Epoch: [108][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6619(0.6561) Grad: 0.0000  \n",
      "Epoch 108 - avg_train_loss: 0.6133  avg_val_loss: 0.6561  time: 0s\n",
      "Epoch 108 - Score: 0.9000\n",
      "Epoch: [109][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6104(0.6104) Grad: 2757.3342  LR: 0.00027813  \n",
      "Epoch: [109][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6141(0.6120) Grad: 4821.1357  LR: 0.00027813  \n",
      "Epoch: [109][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6508) Grad: 0.0000  \n",
      "Epoch: [109][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6629(0.6564) Grad: 0.0000  \n",
      "Epoch 109 - avg_train_loss: 0.6120  avg_val_loss: 0.6564  time: 0s\n",
      "Epoch 109 - Score: 0.9000\n",
      "Epoch: [110][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5967(0.5967) Grad: 1492.0656  LR: 0.00027813  \n",
      "Epoch: [110][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5875(0.6091) Grad: 1179.6445  LR: 0.00022528  \n",
      "Epoch: [110][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6511(0.6511) Grad: 0.0000  \n",
      "Epoch: [110][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6619(0.6561) Grad: 0.0000  \n",
      "Epoch 110 - avg_train_loss: 0.6091  avg_val_loss: 0.6561  time: 0s\n",
      "Epoch 110 - Score: 0.9000\n",
      "Epoch: [111][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6021(0.6021) Grad: 2568.3181  LR: 0.00025032  \n",
      "Epoch: [111][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6190(0.6143) Grad: 674.3742  LR: 0.00025032  \n",
      "Epoch: [111][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6510(0.6510) Grad: 0.0000  \n",
      "Epoch: [111][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6626(0.6564) Grad: 0.0000  \n",
      "Epoch 111 - avg_train_loss: 0.6143  avg_val_loss: 0.6564  time: 0s\n",
      "Epoch 111 - Score: 0.9000\n",
      "Epoch: [112][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6189(0.6189) Grad: 4790.5229  LR: 0.00025032  \n",
      "Epoch: [112][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6238(0.6148) Grad: 1360.4382  LR: 0.00025032  \n",
      "Epoch: [112][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6500) Grad: 0.0000  \n",
      "Epoch: [112][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6654(0.6572) Grad: 0.0000  \n",
      "Epoch 112 - avg_train_loss: 0.6148  avg_val_loss: 0.6572  time: 0s\n",
      "Epoch 112 - Score: 0.9000\n",
      "Epoch: [113][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6149(0.6149) Grad: 4411.2759  LR: 0.00025032  \n",
      "Epoch: [113][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6196(0.6125) Grad: 7653.3550  LR: 0.00025032  \n",
      "Epoch: [113][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6505) Grad: 0.0000  \n",
      "Epoch: [113][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6644(0.6570) Grad: 0.0000  \n",
      "Epoch 113 - avg_train_loss: 0.6125  avg_val_loss: 0.6570  time: 0s\n",
      "Epoch 113 - Score: 0.8958\n",
      "Epoch: [114][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6320(0.6320) Grad: 7534.8833  LR: 0.00020276  \n",
      "Epoch: [114][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6256(0.6148) Grad: 4427.7568  LR: 0.00022528  \n",
      "Epoch: [114][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6513) Grad: 0.0000  \n",
      "Epoch: [114][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6624(0.6565) Grad: 0.0000  \n",
      "Epoch 114 - avg_train_loss: 0.6148  avg_val_loss: 0.6565  time: 0s\n",
      "Epoch 114 - Score: 0.9000\n",
      "Epoch: [115][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5997(0.5997) Grad: 2564.3867  LR: 0.00022528  \n",
      "Epoch: [115][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6088) Grad: 604.6599  LR: 0.00022528  \n",
      "Epoch: [115][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6514(0.6514) Grad: 0.0000  \n",
      "Epoch: [115][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6659(0.6582) Grad: 0.0000  \n",
      "Epoch 115 - avg_train_loss: 0.6088  avg_val_loss: 0.6582  time: 0s\n",
      "Epoch 115 - Score: 0.9000\n",
      "Epoch: [116][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6001(0.6001) Grad: 5912.7178  LR: 0.00022528  \n",
      "Epoch: [116][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6058(0.6110) Grad: 4111.2695  LR: 0.00022528  \n",
      "Epoch: [116][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6508) Grad: 0.0000  \n",
      "Epoch: [116][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6699(0.6597) Grad: 0.0000  \n",
      "Epoch 116 - avg_train_loss: 0.6110  avg_val_loss: 0.6597  time: 0s\n",
      "Epoch 116 - Score: 0.8917\n",
      "Epoch: [117][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6352(0.6352) Grad: 949.0400  LR: 0.00022528  \n",
      "Epoch: [117][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6157(0.6141) Grad: 4092.9146  LR: 0.00020276  \n",
      "Epoch: [117][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6509(0.6509) Grad: 0.0000  \n",
      "Epoch: [117][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6711(0.6604) Grad: 0.0000  \n",
      "Epoch 117 - avg_train_loss: 0.6141  avg_val_loss: 0.6604  time: 0s\n",
      "Epoch 117 - Score: 0.8917\n",
      "Epoch: [118][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6117(0.6117) Grad: 4866.2324  LR: 0.00020276  \n",
      "Epoch: [118][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6117(0.6138) Grad: 2288.6538  LR: 0.00020276  \n",
      "Epoch: [118][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6500) Grad: 0.0000  \n",
      "Epoch: [118][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6722(0.6604) Grad: 0.0000  \n",
      "Epoch 118 - avg_train_loss: 0.6138  avg_val_loss: 0.6604  time: 0s\n",
      "Epoch 118 - Score: 0.8917\n",
      "Epoch: [119][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5947(0.5947) Grad: 5695.7227  LR: 0.00020276  \n",
      "Epoch: [119][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6118(0.6126) Grad: 694.5789  LR: 0.00020276  \n",
      "Epoch: [119][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [119][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6715(0.6601) Grad: 0.0000  \n",
      "Epoch 119 - avg_train_loss: 0.6126  avg_val_loss: 0.6601  time: 0s\n",
      "Epoch 119 - Score: 0.8917\n",
      "Epoch: [120][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5992(0.5992) Grad: 1246.4661  LR: 0.00020276  \n",
      "Epoch: [120][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6037(0.6110) Grad: 1453.0205  LR: 0.00018248  \n",
      "Epoch: [120][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6503) Grad: 0.0000  \n",
      "Epoch: [120][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6725(0.6606) Grad: 0.0000  \n",
      "Epoch 120 - avg_train_loss: 0.6110  avg_val_loss: 0.6606  time: 0s\n",
      "Epoch 120 - Score: 0.8917\n",
      "Epoch: [121][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6477(0.6477) Grad: 3279.4973  LR: 0.00018248  \n",
      "Epoch: [121][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6059(0.6145) Grad: 6226.1064  LR: 0.00018248  \n",
      "Epoch: [121][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6500) Grad: 0.0000  \n",
      "Epoch: [121][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6708(0.6597) Grad: 0.0000  \n",
      "Epoch 121 - avg_train_loss: 0.6145  avg_val_loss: 0.6597  time: 0s\n",
      "Epoch 121 - Score: 0.8917\n",
      "Epoch: [122][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6254(0.6254) Grad: 4182.7217  LR: 0.00018248  \n",
      "Epoch: [122][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6167(0.6099) Grad: 2160.3831  LR: 0.00018248  \n",
      "Epoch: [122][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [122][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6670(0.6580) Grad: 0.0000  \n",
      "Epoch 122 - avg_train_loss: 0.6099  avg_val_loss: 0.6580  time: 0s\n",
      "Epoch 122 - Score: 0.8917\n",
      "Epoch: [123][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5883(0.5883) Grad: 1552.9951  LR: 0.00018248  \n",
      "Epoch: [123][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6032(0.6129) Grad: 1016.4529  LR: 0.00016423  \n",
      "Epoch: [123][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [123][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6659(0.6576) Grad: 0.0000  \n",
      "Epoch 123 - avg_train_loss: 0.6129  avg_val_loss: 0.6576  time: 0s\n",
      "Epoch 123 - Score: 0.9000\n",
      "Epoch: [124][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6025(0.6025) Grad: 741.9283  LR: 0.00016423  \n",
      "Epoch: [124][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6092) Grad: 6661.5400  LR: 0.00016423  \n",
      "Epoch: [124][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6503) Grad: 0.0000  \n",
      "Epoch: [124][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6675(0.6583) Grad: 0.0000  \n",
      "Epoch 124 - avg_train_loss: 0.6092  avg_val_loss: 0.6583  time: 0s\n",
      "Epoch 124 - Score: 0.8958\n",
      "Epoch: [125][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6141(0.6141) Grad: 5122.4409  LR: 0.00016423  \n",
      "Epoch: [125][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5758(0.6102) Grad: 1925.3657  LR: 0.00016423  \n",
      "Epoch: [125][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [125][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6685(0.6589) Grad: 0.0000  \n",
      "Epoch 125 - avg_train_loss: 0.6102  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 125 - Score: 0.8958\n",
      "Epoch: [126][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6433(0.6433) Grad: 3561.4475  LR: 0.00016423  \n",
      "Epoch: [126][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6073(0.6136) Grad: 5571.7163  LR: 0.00014781  \n",
      "Epoch: [126][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6502) Grad: 0.0000  \n",
      "Epoch: [126][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6689(0.6589) Grad: 0.0000  \n",
      "Epoch 126 - avg_train_loss: 0.6136  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 126 - Score: 0.8958\n",
      "Epoch: [127][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6148(0.6148) Grad: 4072.7695  LR: 0.00014781  \n",
      "Epoch: [127][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6155(0.6129) Grad: 5514.0063  LR: 0.00014781  \n",
      "Epoch: [127][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [127][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6689(0.6590) Grad: 0.0000  \n",
      "Epoch 127 - avg_train_loss: 0.6129  avg_val_loss: 0.6590  time: 0s\n",
      "Epoch 127 - Score: 0.8958\n",
      "Epoch: [128][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6143(0.6143) Grad: 2257.4871  LR: 0.00014781  \n",
      "Epoch: [128][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6130) Grad: 1799.5370  LR: 0.00014781  \n",
      "Epoch: [128][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6509(0.6509) Grad: 0.0000  \n",
      "Epoch: [128][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6685(0.6591) Grad: 0.0000  \n",
      "Epoch 128 - avg_train_loss: 0.6130  avg_val_loss: 0.6591  time: 0s\n",
      "Epoch 128 - Score: 0.8958\n",
      "Epoch: [129][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6036(0.6036) Grad: 2183.8662  LR: 0.00014781  \n",
      "Epoch: [129][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6107(0.6108) Grad: 3538.1912  LR: 0.00013303  \n",
      "Epoch: [129][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6509(0.6509) Grad: 0.0000  \n",
      "Epoch: [129][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6680(0.6589) Grad: 0.0000  \n",
      "Epoch 129 - avg_train_loss: 0.6108  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 129 - Score: 0.8958\n",
      "Epoch: [130][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5902(0.5902) Grad: 2582.4548  LR: 0.00013303  \n",
      "Epoch: [130][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5876(0.6103) Grad: 996.3871  LR: 0.00013303  \n",
      "Epoch: [130][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6507(0.6507) Grad: 0.0000  \n",
      "Epoch: [130][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6684(0.6590) Grad: 0.0000  \n",
      "Epoch 130 - avg_train_loss: 0.6103  avg_val_loss: 0.6590  time: 0s\n",
      "Epoch 130 - Score: 0.8958\n",
      "Epoch: [131][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6125(0.6125) Grad: 2188.9746  LR: 0.00013303  \n",
      "Epoch: [131][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6389(0.6122) Grad: 4151.0479  LR: 0.00013303  \n",
      "Epoch: [131][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6503) Grad: 0.0000  \n",
      "Epoch: [131][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6688(0.6589) Grad: 0.0000  \n",
      "Epoch 131 - avg_train_loss: 0.6122  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 131 - Score: 0.8958\n",
      "Epoch: [132][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6082(0.6082) Grad: 3491.9985  LR: 0.00013303  \n",
      "Epoch: [132][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6265(0.6150) Grad: 887.1851  LR: 0.00010775  \n",
      "Epoch: [132][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6500) Grad: 0.0000  \n",
      "Epoch: [132][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6713(0.6599) Grad: 0.0000  \n",
      "Epoch 132 - avg_train_loss: 0.6150  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 132 - Score: 0.8958\n",
      "Epoch: [133][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6414(0.6414) Grad: 2342.7048  LR: 0.00011973  \n",
      "Epoch: [133][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6081) Grad: 5679.6548  LR: 0.00011973  \n",
      "Epoch: [133][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6500) Grad: 0.0000  \n",
      "Epoch: [133][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6713(0.6599) Grad: 0.0000  \n",
      "Epoch 133 - avg_train_loss: 0.6081  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 133 - Score: 0.8958\n",
      "Epoch: [134][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6427(0.6427) Grad: 3818.9543  LR: 0.00011973  \n",
      "Epoch: [134][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5918(0.6102) Grad: 3484.2795  LR: 0.00011973  \n",
      "Epoch: [134][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6499) Grad: 0.0000  \n",
      "Epoch: [134][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6688(0.6587) Grad: 0.0000  \n",
      "Epoch 134 - avg_train_loss: 0.6102  avg_val_loss: 0.6587  time: 0s\n",
      "Epoch 134 - Score: 0.8958\n",
      "Epoch: [135][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6050) Grad: 2184.0762  LR: 0.00011973  \n",
      "Epoch: [135][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6566(0.6091) Grad: 5053.4268  LR: 0.00011973  \n",
      "Epoch: [135][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [135][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6677(0.6583) Grad: 0.0000  \n",
      "Epoch 135 - avg_train_loss: 0.6091  avg_val_loss: 0.6583  time: 0s\n",
      "Epoch 135 - Score: 0.8958\n",
      "Epoch: [136][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6049(0.6049) Grad: 3067.7085  LR: 0.00009698  \n",
      "Epoch: [136][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6106) Grad: 1649.8357  LR: 0.00010775  \n",
      "Epoch: [136][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [136][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6689(0.6589) Grad: 0.0000  \n",
      "Epoch 136 - avg_train_loss: 0.6106  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 136 - Score: 0.8958\n",
      "Epoch: [137][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5970(0.5970) Grad: 1686.7976  LR: 0.00010775  \n",
      "Epoch: [137][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6120(0.6117) Grad: 5561.4946  LR: 0.00010775  \n",
      "Epoch: [137][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [137][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6675(0.6582) Grad: 0.0000  \n",
      "Epoch 137 - avg_train_loss: 0.6117  avg_val_loss: 0.6582  time: 0s\n",
      "Epoch 137 - Score: 0.8958\n",
      "Epoch: [138][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6281(0.6281) Grad: 2412.0259  LR: 0.00010775  \n",
      "Epoch: [138][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5835(0.6117) Grad: 4142.9502  LR: 0.00010775  \n",
      "Epoch: [138][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6499) Grad: 0.0000  \n",
      "Epoch: [138][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6699(0.6592) Grad: 0.0000  \n",
      "Epoch 138 - avg_train_loss: 0.6117  avg_val_loss: 0.6592  time: 0s\n",
      "Epoch 138 - Score: 0.8958\n",
      "Epoch: [139][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6280(0.6280) Grad: 2835.0718  LR: 0.00010775  \n",
      "Epoch: [139][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6238(0.6151) Grad: 7206.3560  LR: 0.00009698  \n",
      "Epoch: [139][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [139][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6711(0.6598) Grad: 0.0000  \n",
      "Epoch 139 - avg_train_loss: 0.6151  avg_val_loss: 0.6598  time: 0s\n",
      "Epoch 139 - Score: 0.8958\n",
      "Epoch: [140][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6209(0.6209) Grad: 2247.2964  LR: 0.00009698  \n",
      "Epoch: [140][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5954(0.6073) Grad: 1764.0612  LR: 0.00009698  \n",
      "Epoch: [140][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [140][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6710(0.6597) Grad: 0.0000  \n",
      "Epoch 140 - avg_train_loss: 0.6073  avg_val_loss: 0.6597  time: 0s\n",
      "Epoch 140 - Score: 0.8958\n",
      "Epoch: [141][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5853(0.5853) Grad: 2590.2830  LR: 0.00009698  \n",
      "Epoch: [141][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6597(0.6117) Grad: 4197.9380  LR: 0.00009698  \n",
      "Epoch: [141][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [141][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6711(0.6597) Grad: 0.0000  \n",
      "Epoch 141 - avg_train_loss: 0.6117  avg_val_loss: 0.6597  time: 0s\n",
      "Epoch 141 - Score: 0.8958\n",
      "Epoch: [142][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6232(0.6232) Grad: 3529.6504  LR: 0.00009698  \n",
      "Epoch: [142][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5899(0.6089) Grad: 2657.2434  LR: 0.00008728  \n",
      "Epoch: [142][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [142][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6723(0.6603) Grad: 0.0000  \n",
      "Epoch 142 - avg_train_loss: 0.6089  avg_val_loss: 0.6603  time: 0s\n",
      "Epoch 142 - Score: 0.8917\n",
      "Epoch: [143][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6331(0.6331) Grad: 4907.1978  LR: 0.00008728  \n",
      "Epoch: [143][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5930(0.6104) Grad: 5120.9746  LR: 0.00008728  \n",
      "Epoch: [143][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [143][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6709(0.6596) Grad: 0.0000  \n",
      "Epoch 143 - avg_train_loss: 0.6104  avg_val_loss: 0.6596  time: 0s\n",
      "Epoch 143 - Score: 0.8917\n",
      "Epoch: [144][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5928(0.5928) Grad: 2347.0142  LR: 0.00008728  \n",
      "Epoch: [144][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6028(0.6099) Grad: 2796.1707  LR: 0.00008728  \n",
      "Epoch: [144][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [144][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6700(0.6592) Grad: 0.0000  \n",
      "Epoch 144 - avg_train_loss: 0.6099  avg_val_loss: 0.6592  time: 0s\n",
      "Epoch 144 - Score: 0.8917\n",
      "Epoch: [145][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6103(0.6103) Grad: 1675.1376  LR: 0.00008728  \n",
      "Epoch: [145][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6092) Grad: 1731.6614  LR: 0.00007855  \n",
      "Epoch: [145][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [145][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6702(0.6592) Grad: 0.0000  \n",
      "Epoch 145 - avg_train_loss: 0.6092  avg_val_loss: 0.6592  time: 0s\n",
      "Epoch 145 - Score: 0.8917\n",
      "Epoch: [146][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6108(0.6108) Grad: 1882.3798  LR: 0.00007855  \n",
      "Epoch: [146][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6016(0.6150) Grad: 2743.9907  LR: 0.00007855  \n",
      "Epoch: [146][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [146][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6695(0.6589) Grad: 0.0000  \n",
      "Epoch 146 - avg_train_loss: 0.6150  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 146 - Score: 0.8958\n",
      "Epoch: [147][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6056(0.6056) Grad: 7018.8120  LR: 0.00007855  \n",
      "Epoch: [147][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6263(0.6139) Grad: 4579.5874  LR: 0.00007855  \n",
      "Epoch: [147][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [147][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6698(0.6590) Grad: 0.0000  \n",
      "Epoch 147 - avg_train_loss: 0.6139  avg_val_loss: 0.6590  time: 0s\n",
      "Epoch 147 - Score: 0.8958\n",
      "Epoch: [148][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6055(0.6055) Grad: 3388.0193  LR: 0.00007855  \n",
      "Epoch: [148][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6390(0.6109) Grad: 3075.3328  LR: 0.00007070  \n",
      "Epoch: [148][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [148][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6724(0.6602) Grad: 0.0000  \n",
      "Epoch 148 - avg_train_loss: 0.6109  avg_val_loss: 0.6602  time: 0s\n",
      "Epoch 148 - Score: 0.8917\n",
      "Epoch: [149][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6120(0.6120) Grad: 2133.3076  LR: 0.00007070  \n",
      "Epoch: [149][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6102) Grad: 532.0535  LR: 0.00007070  \n",
      "Epoch: [149][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [149][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6716(0.6599) Grad: 0.0000  \n",
      "Epoch 149 - avg_train_loss: 0.6102  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 149 - Score: 0.8917\n",
      "Epoch: [150][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6232(0.6232) Grad: 3950.8723  LR: 0.00007070  \n",
      "Epoch: [150][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6109(0.6115) Grad: 1964.2118  LR: 0.00007070  \n",
      "Epoch: [150][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [150][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6720(0.6600) Grad: 0.0000  \n",
      "Epoch 150 - avg_train_loss: 0.6115  avg_val_loss: 0.6600  time: 0s\n",
      "Epoch 150 - Score: 0.8917\n",
      "Epoch: [151][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6089(0.6089) Grad: 2199.7124  LR: 0.00007070  \n",
      "Epoch: [151][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6192(0.6115) Grad: 1285.8445  LR: 0.00006363  \n",
      "Epoch: [151][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [151][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6707(0.6596) Grad: 0.0000  \n",
      "Epoch 151 - avg_train_loss: 0.6115  avg_val_loss: 0.6596  time: 0s\n",
      "Epoch 151 - Score: 0.8917\n",
      "Epoch: [152][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5948(0.5948) Grad: 1041.0834  LR: 0.00006363  \n",
      "Epoch: [152][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6234(0.6115) Grad: 1294.8604  LR: 0.00006363  \n",
      "Epoch: [152][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6499) Grad: 0.0000  \n",
      "Epoch: [152][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6706(0.6595) Grad: 0.0000  \n",
      "Epoch 152 - avg_train_loss: 0.6115  avg_val_loss: 0.6595  time: 0s\n",
      "Epoch 152 - Score: 0.8917\n",
      "Epoch: [153][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5956(0.5956) Grad: 3251.3770  LR: 0.00006363  \n",
      "Epoch: [153][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5979(0.6134) Grad: 3385.5256  LR: 0.00006363  \n",
      "Epoch: [153][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [153][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6714(0.6600) Grad: 0.0000  \n",
      "Epoch 153 - avg_train_loss: 0.6134  avg_val_loss: 0.6600  time: 0s\n",
      "Epoch 153 - Score: 0.8917\n",
      "Epoch: [154][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6136(0.6136) Grad: 1447.5553  LR: 0.00006363  \n",
      "Epoch: [154][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6056(0.6131) Grad: 3880.9802  LR: 0.00005154  \n",
      "Epoch: [154][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6499) Grad: 0.0000  \n",
      "Epoch: [154][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6703(0.6594) Grad: 0.0000  \n",
      "Epoch 154 - avg_train_loss: 0.6131  avg_val_loss: 0.6594  time: 0s\n",
      "Epoch 154 - Score: 0.8917\n",
      "Epoch: [155][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5999(0.5999) Grad: 4433.3535  LR: 0.00005726  \n",
      "Epoch: [155][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6048(0.6058) Grad: 2755.3550  LR: 0.00005726  \n",
      "Epoch: [155][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [155][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6710(0.6596) Grad: 0.0000  \n",
      "Epoch 155 - avg_train_loss: 0.6058  avg_val_loss: 0.6596  time: 0s\n",
      "Epoch 155 - Score: 0.8917\n",
      "Epoch: [156][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6106(0.6106) Grad: 3631.3469  LR: 0.00005726  \n",
      "Epoch: [156][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5877(0.6092) Grad: 1099.3547  LR: 0.00005726  \n",
      "Epoch: [156][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [156][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6723(0.6602) Grad: 0.0000  \n",
      "Epoch 156 - avg_train_loss: 0.6092  avg_val_loss: 0.6602  time: 0s\n",
      "Epoch 156 - Score: 0.8917\n",
      "Epoch: [157][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6155(0.6155) Grad: 6035.5215  LR: 0.00005726  \n",
      "Epoch: [157][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5966(0.6083) Grad: 1267.9116  LR: 0.00005726  \n",
      "Epoch: [157][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [157][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6719(0.6600) Grad: 0.0000  \n",
      "Epoch 157 - avg_train_loss: 0.6083  avg_val_loss: 0.6600  time: 0s\n",
      "Epoch 157 - Score: 0.8917\n",
      "Epoch: [158][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6109(0.6109) Grad: 1578.2728  LR: 0.00004638  \n",
      "Epoch: [158][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6101(0.6123) Grad: 1006.0870  LR: 0.00005154  \n",
      "Epoch: [158][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [158][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6725(0.6604) Grad: 0.0000  \n",
      "Epoch 158 - avg_train_loss: 0.6123  avg_val_loss: 0.6604  time: 0s\n",
      "Epoch 158 - Score: 0.8917\n",
      "Epoch: [159][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 2496.0698  LR: 0.00005154  \n",
      "Epoch: [159][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6329(0.6135) Grad: 4862.8721  LR: 0.00005154  \n",
      "Epoch: [159][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [159][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6729(0.6606) Grad: 0.0000  \n",
      "Epoch 159 - avg_train_loss: 0.6135  avg_val_loss: 0.6606  time: 0s\n",
      "Epoch 159 - Score: 0.8917\n",
      "Epoch: [160][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5807(0.5807) Grad: 1847.4384  LR: 0.00005154  \n",
      "Epoch: [160][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6220(0.6084) Grad: 5901.4023  LR: 0.00005154  \n",
      "Epoch: [160][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [160][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6717(0.6600) Grad: 0.0000  \n",
      "Epoch 160 - avg_train_loss: 0.6084  avg_val_loss: 0.6600  time: 0s\n",
      "Epoch 160 - Score: 0.8917\n",
      "Epoch: [161][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5912(0.5912) Grad: 7198.4771  LR: 0.00005154  \n",
      "Epoch: [161][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6111(0.6108) Grad: 816.6512  LR: 0.00004638  \n",
      "Epoch: [161][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [161][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6717(0.6599) Grad: 0.0000  \n",
      "Epoch 161 - avg_train_loss: 0.6108  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 161 - Score: 0.8917\n",
      "Epoch: [162][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6139(0.6139) Grad: 4272.4824  LR: 0.00004638  \n",
      "Epoch: [162][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6319(0.6136) Grad: 1312.7390  LR: 0.00004638  \n",
      "Epoch: [162][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [162][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6716(0.6599) Grad: 0.0000  \n",
      "Epoch 162 - avg_train_loss: 0.6136  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 162 - Score: 0.8917\n",
      "Epoch: [163][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6073(0.6073) Grad: 7267.1602  LR: 0.00004638  \n",
      "Epoch: [163][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6203(0.6149) Grad: 4953.8906  LR: 0.00004638  \n",
      "Epoch: [163][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [163][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6699(0.6591) Grad: 0.0000  \n",
      "Epoch 163 - avg_train_loss: 0.6149  avg_val_loss: 0.6591  time: 0s\n",
      "Epoch 163 - Score: 0.8958\n",
      "Epoch: [164][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6259(0.6259) Grad: 2306.5754  LR: 0.00004638  \n",
      "Epoch: [164][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5848(0.6102) Grad: 3730.6226  LR: 0.00004175  \n",
      "Epoch: [164][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [164][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6706(0.6594) Grad: 0.0000  \n",
      "Epoch 164 - avg_train_loss: 0.6102  avg_val_loss: 0.6594  time: 0s\n",
      "Epoch 164 - Score: 0.8958\n",
      "Epoch: [165][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6118(0.6118) Grad: 2900.3438  LR: 0.00004175  \n",
      "Epoch: [165][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6119(0.6069) Grad: 524.4379  LR: 0.00004175  \n",
      "Epoch: [165][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [165][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6693(0.6589) Grad: 0.0000  \n",
      "Epoch 165 - avg_train_loss: 0.6069  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 165 - Score: 0.8958\n",
      "Epoch: [166][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6153(0.6153) Grad: 3054.8662  LR: 0.00004175  \n",
      "Epoch: [166][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5867(0.6083) Grad: 6989.1343  LR: 0.00004175  \n",
      "Epoch: [166][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [166][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6716(0.6599) Grad: 0.0000  \n",
      "Epoch 166 - avg_train_loss: 0.6083  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 166 - Score: 0.8958\n",
      "Epoch: [167][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6063(0.6063) Grad: 1632.5057  LR: 0.00004175  \n",
      "Epoch: [167][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6172(0.6112) Grad: 4558.4150  LR: 0.00003757  \n",
      "Epoch: [167][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [167][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6727(0.6603) Grad: 0.0000  \n",
      "Epoch 167 - avg_train_loss: 0.6112  avg_val_loss: 0.6603  time: 0s\n",
      "Epoch 167 - Score: 0.8917\n",
      "Epoch: [168][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6080(0.6080) Grad: 2257.9573  LR: 0.00003757  \n",
      "Epoch: [168][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6043(0.6128) Grad: 2213.3831  LR: 0.00003757  \n",
      "Epoch: [168][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [168][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6736(0.6608) Grad: 0.0000  \n",
      "Epoch 168 - avg_train_loss: 0.6128  avg_val_loss: 0.6608  time: 0s\n",
      "Epoch 168 - Score: 0.8917\n",
      "Epoch: [169][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6325(0.6325) Grad: 6258.6128  LR: 0.00003757  \n",
      "Epoch: [169][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6102(0.6103) Grad: 594.9068  LR: 0.00003757  \n",
      "Epoch: [169][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [169][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6734(0.6607) Grad: 0.0000  \n",
      "Epoch 169 - avg_train_loss: 0.6103  avg_val_loss: 0.6607  time: 0s\n",
      "Epoch 169 - Score: 0.8917\n",
      "Epoch: [170][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6164(0.6164) Grad: 5331.0181  LR: 0.00003757  \n",
      "Epoch: [170][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6198(0.6076) Grad: 2711.0012  LR: 0.00003381  \n",
      "Epoch: [170][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [170][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6722(0.6602) Grad: 0.0000  \n",
      "Epoch 170 - avg_train_loss: 0.6076  avg_val_loss: 0.6602  time: 0s\n",
      "Epoch 170 - Score: 0.8958\n",
      "Epoch: [171][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5832(0.5832) Grad: 3461.8208  LR: 0.00003381  \n",
      "Epoch: [171][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6281(0.6084) Grad: 2624.7922  LR: 0.00003381  \n",
      "Epoch: [171][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [171][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6721(0.6601) Grad: 0.0000  \n",
      "Epoch 171 - avg_train_loss: 0.6084  avg_val_loss: 0.6601  time: 0s\n",
      "Epoch 171 - Score: 0.8958\n",
      "Epoch: [172][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6057(0.6057) Grad: 3794.4763  LR: 0.00003381  \n",
      "Epoch: [172][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5826(0.6115) Grad: 4892.9453  LR: 0.00003381  \n",
      "Epoch: [172][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [172][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6730(0.6605) Grad: 0.0000  \n",
      "Epoch 172 - avg_train_loss: 0.6115  avg_val_loss: 0.6605  time: 0s\n",
      "Epoch 172 - Score: 0.8917\n",
      "Epoch: [173][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5898(0.5898) Grad: 2505.4849  LR: 0.00003381  \n",
      "Epoch: [173][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6191(0.6082) Grad: 1054.0985  LR: 0.00003043  \n",
      "Epoch: [173][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [173][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6726(0.6603) Grad: 0.0000  \n",
      "Epoch 173 - avg_train_loss: 0.6082  avg_val_loss: 0.6603  time: 0s\n",
      "Epoch 173 - Score: 0.8917\n",
      "Epoch: [174][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6491) Grad: 2640.3088  LR: 0.00003043  \n",
      "Epoch: [174][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6119) Grad: 2710.9758  LR: 0.00003043  \n",
      "Epoch: [174][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [174][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6714(0.6599) Grad: 0.0000  \n",
      "Epoch 174 - avg_train_loss: 0.6119  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 174 - Score: 0.8917\n",
      "Epoch: [175][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6329(0.6329) Grad: 6953.4712  LR: 0.00003043  \n",
      "Epoch: [175][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5811(0.6085) Grad: 985.3228  LR: 0.00003043  \n",
      "Epoch: [175][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [175][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6711(0.6597) Grad: 0.0000  \n",
      "Epoch 175 - avg_train_loss: 0.6085  avg_val_loss: 0.6597  time: 0s\n",
      "Epoch 175 - Score: 0.8917\n",
      "Epoch: [176][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6143(0.6143) Grad: 4642.6489  LR: 0.00003043  \n",
      "Epoch: [176][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6053) Grad: 3709.6316  LR: 0.00002465  \n",
      "Epoch: [176][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [176][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6716(0.6599) Grad: 0.0000  \n",
      "Epoch 176 - avg_train_loss: 0.6053  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 176 - Score: 0.8958\n",
      "Epoch: [177][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6034(0.6034) Grad: 1974.4072  LR: 0.00002739  \n",
      "Epoch: [177][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6262(0.6099) Grad: 4963.9629  LR: 0.00002739  \n",
      "Epoch: [177][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [177][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6712(0.6597) Grad: 0.0000  \n",
      "Epoch 177 - avg_train_loss: 0.6099  avg_val_loss: 0.6597  time: 0s\n",
      "Epoch 177 - Score: 0.8958\n",
      "Epoch: [178][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6340(0.6340) Grad: 3393.1472  LR: 0.00002739  \n",
      "Epoch: [178][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6081) Grad: 5462.0166  LR: 0.00002739  \n",
      "Epoch: [178][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [178][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6705(0.6594) Grad: 0.0000  \n",
      "Epoch 178 - avg_train_loss: 0.6081  avg_val_loss: 0.6594  time: 0s\n",
      "Epoch 178 - Score: 0.8958\n",
      "Epoch: [179][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5920(0.5920) Grad: 4047.1160  LR: 0.00002739  \n",
      "Epoch: [179][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6021(0.6067) Grad: 1611.3160  LR: 0.00002739  \n",
      "Epoch: [179][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [179][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6721(0.6601) Grad: 0.0000  \n",
      "Epoch 179 - avg_train_loss: 0.6067  avg_val_loss: 0.6601  time: 0s\n",
      "Epoch 179 - Score: 0.8958\n",
      "Epoch: [180][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5801(0.5801) Grad: 520.3406  LR: 0.00002219  \n",
      "Epoch: [180][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5782(0.6088) Grad: 3803.5076  LR: 0.00002465  \n",
      "Epoch: [180][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [180][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6726(0.6603) Grad: 0.0000  \n",
      "Epoch 180 - avg_train_loss: 0.6088  avg_val_loss: 0.6603  time: 0s\n",
      "Epoch 180 - Score: 0.8958\n",
      "Epoch: [181][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6260(0.6260) Grad: 882.5540  LR: 0.00002465  \n",
      "Epoch: [181][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6010(0.6111) Grad: 3352.0156  LR: 0.00002465  \n",
      "Epoch: [181][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [181][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6725(0.6603) Grad: 0.0000  \n",
      "Epoch 181 - avg_train_loss: 0.6111  avg_val_loss: 0.6603  time: 0s\n",
      "Epoch 181 - Score: 0.8958\n",
      "Epoch: [182][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5759(0.5759) Grad: 2858.6033  LR: 0.00002465  \n",
      "Epoch: [182][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6269(0.6098) Grad: 5880.7959  LR: 0.00002465  \n",
      "Epoch: [182][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [182][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6728(0.6605) Grad: 0.0000  \n",
      "Epoch 182 - avg_train_loss: 0.6098  avg_val_loss: 0.6605  time: 0s\n",
      "Epoch 182 - Score: 0.8917\n",
      "Epoch: [183][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6134(0.6134) Grad: 5249.1475  LR: 0.00002465  \n",
      "Epoch: [183][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5808(0.6094) Grad: 1790.7137  LR: 0.00002219  \n",
      "Epoch: [183][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [183][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6730(0.6606) Grad: 0.0000  \n",
      "Epoch 183 - avg_train_loss: 0.6094  avg_val_loss: 0.6606  time: 0s\n",
      "Epoch 183 - Score: 0.8917\n",
      "Epoch: [184][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5941(0.5941) Grad: 1683.7013  LR: 0.00002219  \n",
      "Epoch: [184][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6269(0.6112) Grad: 7535.4575  LR: 0.00002219  \n",
      "Epoch: [184][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [184][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6721(0.6601) Grad: 0.0000  \n",
      "Epoch 184 - avg_train_loss: 0.6112  avg_val_loss: 0.6601  time: 0s\n",
      "Epoch 184 - Score: 0.8958\n",
      "Epoch: [185][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6117(0.6117) Grad: 558.2719  LR: 0.00002219  \n",
      "Epoch: [185][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6320(0.6136) Grad: 5416.0625  LR: 0.00002219  \n",
      "Epoch: [185][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [185][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6720(0.6600) Grad: 0.0000  \n",
      "Epoch 185 - avg_train_loss: 0.6136  avg_val_loss: 0.6600  time: 0s\n",
      "Epoch 185 - Score: 0.8958\n",
      "Epoch: [186][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5933(0.5933) Grad: 4019.5923  LR: 0.00002219  \n",
      "Epoch: [186][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6283(0.6082) Grad: 3447.3735  LR: 0.00001997  \n",
      "Epoch: [186][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [186][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6710(0.6596) Grad: 0.0000  \n",
      "Epoch 186 - avg_train_loss: 0.6082  avg_val_loss: 0.6596  time: 0s\n",
      "Epoch 186 - Score: 0.8958\n",
      "Epoch: [187][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5908(0.5908) Grad: 2507.4697  LR: 0.00001997  \n",
      "Epoch: [187][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5907(0.6099) Grad: 3699.8306  LR: 0.00001997  \n",
      "Epoch: [187][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [187][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6717(0.6600) Grad: 0.0000  \n",
      "Epoch 187 - avg_train_loss: 0.6099  avg_val_loss: 0.6600  time: 0s\n",
      "Epoch 187 - Score: 0.8958\n",
      "Epoch: [188][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5897(0.5897) Grad: 858.1530  LR: 0.00001997  \n",
      "Epoch: [188][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6069(0.6136) Grad: 3717.2087  LR: 0.00001997  \n",
      "Epoch: [188][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [188][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6717(0.6600) Grad: 0.0000  \n",
      "Epoch 188 - avg_train_loss: 0.6136  avg_val_loss: 0.6600  time: 0s\n",
      "Epoch 188 - Score: 0.8958\n",
      "Epoch: [189][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6046) Grad: 2631.9800  LR: 0.00001997  \n",
      "Epoch: [189][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6258(0.6084) Grad: 2916.0500  LR: 0.00001797  \n",
      "Epoch: [189][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [189][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6713(0.6598) Grad: 0.0000  \n",
      "Epoch 189 - avg_train_loss: 0.6084  avg_val_loss: 0.6598  time: 0s\n",
      "Epoch 189 - Score: 0.8958\n",
      "Epoch: [190][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6047) Grad: 628.6311  LR: 0.00001797  \n",
      "Epoch: [190][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6030(0.6082) Grad: 1536.3230  LR: 0.00001797  \n",
      "Epoch: [190][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [190][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6716(0.6599) Grad: 0.0000  \n",
      "Epoch 190 - avg_train_loss: 0.6082  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 190 - Score: 0.8958\n",
      "Epoch: [191][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5954(0.5954) Grad: 2530.3853  LR: 0.00001797  \n",
      "Epoch: [191][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6099(0.6120) Grad: 2408.3677  LR: 0.00001797  \n",
      "Epoch: [191][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [191][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6702(0.6592) Grad: 0.0000  \n",
      "Epoch 191 - avg_train_loss: 0.6120  avg_val_loss: 0.6592  time: 0s\n",
      "Epoch 191 - Score: 0.8958\n",
      "Epoch: [192][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6059(0.6059) Grad: 3003.1360  LR: 0.00001797  \n",
      "Epoch: [192][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6072(0.6114) Grad: 3929.2178  LR: 0.00001617  \n",
      "Epoch: [192][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [192][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6698(0.6589) Grad: 0.0000  \n",
      "Epoch 192 - avg_train_loss: 0.6114  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 192 - Score: 0.8958\n",
      "Epoch: [193][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6294(0.6294) Grad: 4250.1235  LR: 0.00001617  \n",
      "Epoch: [193][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6023(0.6135) Grad: 413.5923  LR: 0.00001617  \n",
      "Epoch: [193][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [193][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6694(0.6588) Grad: 0.0000  \n",
      "Epoch 193 - avg_train_loss: 0.6135  avg_val_loss: 0.6588  time: 0s\n",
      "Epoch 193 - Score: 0.8958\n",
      "Epoch: [194][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6324(0.6324) Grad: 3282.7241  LR: 0.00001617  \n",
      "Epoch: [194][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6298(0.6086) Grad: 6014.4946  LR: 0.00001617  \n",
      "Epoch: [194][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [194][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6706(0.6593) Grad: 0.0000  \n",
      "Epoch 194 - avg_train_loss: 0.6086  avg_val_loss: 0.6593  time: 0s\n",
      "Epoch 194 - Score: 0.8958\n",
      "Epoch: [195][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6328(0.6328) Grad: 1606.1066  LR: 0.00001617  \n",
      "Epoch: [195][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5892(0.6077) Grad: 997.0560  LR: 0.00001456  \n",
      "Epoch: [195][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [195][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6699(0.6590) Grad: 0.0000  \n",
      "Epoch 195 - avg_train_loss: 0.6077  avg_val_loss: 0.6590  time: 0s\n",
      "Epoch 195 - Score: 0.8958\n",
      "Epoch: [196][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6106(0.6106) Grad: 469.0669  LR: 0.00001456  \n",
      "Epoch: [196][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6016(0.6082) Grad: 640.5665  LR: 0.00001456  \n",
      "Epoch: [196][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [196][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6697(0.6590) Grad: 0.0000  \n",
      "Epoch 196 - avg_train_loss: 0.6082  avg_val_loss: 0.6590  time: 0s\n",
      "Epoch 196 - Score: 0.8958\n",
      "Epoch: [197][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6253(0.6253) Grad: 1424.3422  LR: 0.00001456  \n",
      "Epoch: [197][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6074(0.6059) Grad: 3289.9028  LR: 0.00001456  \n",
      "Epoch: [197][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [197][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6716(0.6599) Grad: 0.0000  \n",
      "Epoch 197 - avg_train_loss: 0.6059  avg_val_loss: 0.6599  time: 0s\n",
      "Epoch 197 - Score: 0.8958\n",
      "Epoch: [198][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5875(0.5875) Grad: 1568.1533  LR: 0.00001456  \n",
      "Epoch: [198][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6257(0.6086) Grad: 4360.0317  LR: 0.00001179  \n",
      "Epoch: [198][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [198][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6719(0.6600) Grad: 0.0000  \n",
      "Epoch 198 - avg_train_loss: 0.6086  avg_val_loss: 0.6600  time: 0s\n",
      "Epoch 198 - Score: 0.8958\n",
      "Epoch: [199][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6299(0.6299) Grad: 3904.1909  LR: 0.00001310  \n",
      "Epoch: [199][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6410(0.6104) Grad: 1506.8478  LR: 0.00001310  \n",
      "Epoch: [199][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [199][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6710(0.6596) Grad: 0.0000  \n",
      "Epoch 199 - avg_train_loss: 0.6104  avg_val_loss: 0.6596  time: 0s\n",
      "Epoch 199 - Score: 0.8958\n",
      "Epoch: [200][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5958(0.5958) Grad: 1663.6334  LR: 0.00001310  \n",
      "Epoch: [200][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6441(0.6053) Grad: 3360.3247  LR: 0.00001310  \n",
      "Epoch: [200][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch: [200][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6715(0.6598) Grad: 0.0000  \n",
      "Epoch 200 - avg_train_loss: 0.6053  avg_val_loss: 0.6598  time: 0s\n",
      "Epoch 200 - Score: 0.8958\n",
      "========== fold: 0 result ==========\n",
      "Score: 0.9083\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 1.1536(1.1536) Grad: 199458.5312  LR: 0.01000000  \n",
      "Epoch: [1][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9083(0.9504) Grad: 14598.5488  LR: 0.01000000  \n",
      "Epoch: [1][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4655(1.4655) Grad: 0.0000  \n",
      "Epoch: [1][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4800(1.4723) Grad: 0.0000  \n",
      "Epoch 1 - avg_train_loss: 0.9504  avg_val_loss: 1.4723  time: 0s\n",
      "Epoch 1 - Score: 0.0792\n",
      "Epoch 1 - Save Best Score: 0.0792 Model\n",
      "Epoch: [2][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8592(0.8592) Grad: 15208.9688  LR: 0.01000000  \n",
      "Epoch: [2][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8456(0.8694) Grad: 12540.3164  LR: 0.01000000  \n",
      "Epoch: [2][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4655(1.4655) Grad: 0.0000  \n",
      "Epoch: [2][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4800(1.4723) Grad: 0.0000  \n",
      "Epoch 2 - avg_train_loss: 0.8694  avg_val_loss: 1.4723  time: 0s\n",
      "Epoch 2 - Score: 0.0792\n",
      "Epoch: [3][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8313(0.8313) Grad: 12947.2998  LR: 0.01000000  \n",
      "Epoch: [3][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8226(0.8320) Grad: 13552.8164  LR: 0.01000000  \n",
      "Epoch: [3][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4645(1.4645) Grad: 0.0000  \n",
      "Epoch: [3][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4793(1.4714) Grad: 0.0000  \n",
      "Epoch 3 - avg_train_loss: 0.8320  avg_val_loss: 1.4714  time: 0s\n",
      "Epoch 3 - Score: 0.0792\n",
      "Epoch: [4][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8436(0.8436) Grad: 13704.5117  LR: 0.00810000  \n",
      "Epoch: [4][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7733(0.8057) Grad: 10789.8926  LR: 0.00900000  \n",
      "Epoch: [4][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9632(0.9632) Grad: 0.0000  \n",
      "Epoch: [4][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9862(0.9739) Grad: 0.0000  \n",
      "Epoch 4 - avg_train_loss: 0.8057  avg_val_loss: 0.9739  time: 0s\n",
      "Epoch 4 - Score: 0.6458\n",
      "Epoch 4 - Save Best Score: 0.6458 Model\n",
      "Epoch: [5][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8132(0.8132) Grad: 13078.5869  LR: 0.00900000  \n",
      "Epoch: [5][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7609(0.7755) Grad: 9935.4014  LR: 0.00900000  \n",
      "Epoch: [5][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9058(0.9058) Grad: 0.0000  \n",
      "Epoch: [5][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9278(0.9161) Grad: 0.0000  \n",
      "Epoch 5 - avg_train_loss: 0.7755  avg_val_loss: 0.9161  time: 0s\n",
      "Epoch 5 - Score: 0.7750\n",
      "Epoch 5 - Save Best Score: 0.7750 Model\n",
      "Epoch: [6][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7755(0.7755) Grad: 10308.9346  LR: 0.00900000  \n",
      "Epoch: [6][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7327(0.7554) Grad: 8724.6885  LR: 0.00900000  \n",
      "Epoch: [6][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8283(0.8283) Grad: 0.0000  \n",
      "Epoch: [6][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8464(0.8368) Grad: 0.0000  \n",
      "Epoch 6 - avg_train_loss: 0.7554  avg_val_loss: 0.8368  time: 0s\n",
      "Epoch 6 - Score: 0.8750\n",
      "Epoch 6 - Save Best Score: 0.8750 Model\n",
      "Epoch: [7][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7860(0.7860) Grad: 9781.3848  LR: 0.00900000  \n",
      "Epoch: [7][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7416(0.7383) Grad: 8135.5122  LR: 0.00810000  \n",
      "Epoch: [7][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7856(0.7856) Grad: 0.0000  \n",
      "Epoch: [7][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7990(0.7919) Grad: 0.0000  \n",
      "Epoch 7 - avg_train_loss: 0.7383  avg_val_loss: 0.7919  time: 0s\n",
      "Epoch 7 - Score: 0.8792\n",
      "Epoch 7 - Save Best Score: 0.8792 Model\n",
      "Epoch: [8][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7199(0.7199) Grad: 6709.1602  LR: 0.00810000  \n",
      "Epoch: [8][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7378(0.7217) Grad: 9290.7773  LR: 0.00810000  \n",
      "Epoch: [8][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7732(0.7732) Grad: 0.0000  \n",
      "Epoch: [8][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7803(0.7765) Grad: 0.0000  \n",
      "Epoch 8 - avg_train_loss: 0.7217  avg_val_loss: 0.7765  time: 0s\n",
      "Epoch 8 - Score: 0.8875\n",
      "Epoch 8 - Save Best Score: 0.8875 Model\n",
      "Epoch: [9][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7223(0.7223) Grad: 7784.7422  LR: 0.00810000  \n",
      "Epoch: [9][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7500(0.7096) Grad: 6942.4028  LR: 0.00810000  \n",
      "Epoch: [9][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9245(0.9245) Grad: 0.0000  \n",
      "Epoch: [9][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9373(0.9304) Grad: 0.0000  \n",
      "Epoch 9 - avg_train_loss: 0.7096  avg_val_loss: 0.9304  time: 0s\n",
      "Epoch 9 - Score: 0.6792\n",
      "Epoch: [10][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7073(0.7073) Grad: 6452.7695  LR: 0.00810000  \n",
      "Epoch: [10][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7250(0.7017) Grad: 6130.9912  LR: 0.00729000  \n",
      "Epoch: [10][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7418(0.7418) Grad: 0.0000  \n",
      "Epoch: [10][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7383(0.7402) Grad: 0.0000  \n",
      "Epoch 10 - avg_train_loss: 0.7017  avg_val_loss: 0.7402  time: 0s\n",
      "Epoch 10 - Score: 0.8417\n",
      "Epoch: [11][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6968(0.6968) Grad: 6541.9941  LR: 0.00729000  \n",
      "Epoch: [11][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6705(0.6901) Grad: 4822.1567  LR: 0.00729000  \n",
      "Epoch: [11][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7517(0.7517) Grad: 0.0000  \n",
      "Epoch: [11][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7501(0.7509) Grad: 0.0000  \n",
      "Epoch 11 - avg_train_loss: 0.6901  avg_val_loss: 0.7509  time: 0s\n",
      "Epoch 11 - Score: 0.8208\n",
      "Epoch: [12][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6840(0.6840) Grad: 5620.1157  LR: 0.00729000  \n",
      "Epoch: [12][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6686(0.6837) Grad: 4401.3105  LR: 0.00729000  \n",
      "Epoch: [12][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7743(0.7743) Grad: 0.0000  \n",
      "Epoch: [12][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8059(0.7890) Grad: 0.0000  \n",
      "Epoch 12 - avg_train_loss: 0.6837  avg_val_loss: 0.7890  time: 0s\n",
      "Epoch 12 - Score: 0.8667\n",
      "Epoch: [13][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6839(0.6839) Grad: 4085.1074  LR: 0.00729000  \n",
      "Epoch: [13][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6408(0.6753) Grad: 4223.1084  LR: 0.00656100  \n",
      "Epoch: [13][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7055(0.7055) Grad: 0.0000  \n",
      "Epoch: [13][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7339(0.7187) Grad: 0.0000  \n",
      "Epoch 13 - avg_train_loss: 0.6753  avg_val_loss: 0.7187  time: 0s\n",
      "Epoch 13 - Score: 0.8833\n",
      "Epoch: [14][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6767(0.6767) Grad: 3289.5181  LR: 0.00656100  \n",
      "Epoch: [14][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6893(0.6698) Grad: 3506.5134  LR: 0.00656100  \n",
      "Epoch: [14][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6952(0.6952) Grad: 0.0000  \n",
      "Epoch: [14][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7382(0.7153) Grad: 0.0000  \n",
      "Epoch 14 - avg_train_loss: 0.6698  avg_val_loss: 0.7153  time: 0s\n",
      "Epoch 14 - Score: 0.8750\n",
      "Epoch: [15][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6792(0.6792) Grad: 2745.9585  LR: 0.00656100  \n",
      "Epoch: [15][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6534(0.6606) Grad: 3812.3818  LR: 0.00656100  \n",
      "Epoch: [15][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7271(0.7271) Grad: 0.0000  \n",
      "Epoch: [15][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7430(0.7345) Grad: 0.0000  \n",
      "Epoch 15 - avg_train_loss: 0.6606  avg_val_loss: 0.7345  time: 0s\n",
      "Epoch 15 - Score: 0.8375\n",
      "Epoch: [16][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6599(0.6599) Grad: 3097.5635  LR: 0.00656100  \n",
      "Epoch: [16][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6394(0.6652) Grad: 2985.5396  LR: 0.00590490  \n",
      "Epoch: [16][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6816(0.6816) Grad: 0.0000  \n",
      "Epoch: [16][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7255(0.7021) Grad: 0.0000  \n",
      "Epoch 16 - avg_train_loss: 0.6652  avg_val_loss: 0.7021  time: 0s\n",
      "Epoch 16 - Score: 0.8750\n",
      "Epoch: [17][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6714(0.6714) Grad: 2821.3745  LR: 0.00590490  \n",
      "Epoch: [17][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6645(0.6569) Grad: 3717.7390  LR: 0.00590490  \n",
      "Epoch: [17][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6813(0.6813) Grad: 0.0000  \n",
      "Epoch: [17][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7179(0.6984) Grad: 0.0000  \n",
      "Epoch 17 - avg_train_loss: 0.6569  avg_val_loss: 0.6984  time: 0s\n",
      "Epoch 17 - Score: 0.8792\n",
      "Epoch: [18][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6974(0.6974) Grad: 3002.4441  LR: 0.00590490  \n",
      "Epoch: [18][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6570(0.6591) Grad: 5722.1431  LR: 0.00590490  \n",
      "Epoch: [18][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6913(0.6913) Grad: 0.0000  \n",
      "Epoch: [18][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7130(0.7014) Grad: 0.0000  \n",
      "Epoch 18 - avg_train_loss: 0.6591  avg_val_loss: 0.7014  time: 0s\n",
      "Epoch 18 - Score: 0.8667\n",
      "Epoch: [19][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6170(0.6170) Grad: 3114.9946  LR: 0.00590490  \n",
      "Epoch: [19][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6274(0.6480) Grad: 2444.3835  LR: 0.00531441  \n",
      "Epoch: [19][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6652(0.6652) Grad: 0.0000  \n",
      "Epoch: [19][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7048(0.6837) Grad: 0.0000  \n",
      "Epoch 19 - avg_train_loss: 0.6480  avg_val_loss: 0.6837  time: 0s\n",
      "Epoch 19 - Score: 0.8875\n",
      "Epoch: [20][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6217(0.6217) Grad: 2729.5078  LR: 0.00531441  \n",
      "Epoch: [20][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6538(0.6476) Grad: 1653.1243  LR: 0.00531441  \n",
      "Epoch: [20][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6969(0.6969) Grad: 0.0000  \n",
      "Epoch: [20][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7182(0.7068) Grad: 0.0000  \n",
      "Epoch 20 - avg_train_loss: 0.6476  avg_val_loss: 0.7068  time: 0s\n",
      "Epoch 20 - Score: 0.8667\n",
      "Epoch: [21][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6927(0.6927) Grad: 2198.3186  LR: 0.00531441  \n",
      "Epoch: [21][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6620(0.6523) Grad: 2370.1047  LR: 0.00531441  \n",
      "Epoch: [21][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6726(0.6726) Grad: 0.0000  \n",
      "Epoch: [21][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7078(0.6890) Grad: 0.0000  \n",
      "Epoch 21 - avg_train_loss: 0.6523  avg_val_loss: 0.6890  time: 0s\n",
      "Epoch 21 - Score: 0.8833\n",
      "Epoch: [22][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6079(0.6079) Grad: 3142.8762  LR: 0.00531441  \n",
      "Epoch: [22][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6328(0.6438) Grad: 2603.5017  LR: 0.00430467  \n",
      "Epoch: [22][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6726(0.6726) Grad: 0.0000  \n",
      "Epoch: [22][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7070(0.6887) Grad: 0.0000  \n",
      "Epoch 22 - avg_train_loss: 0.6438  avg_val_loss: 0.6887  time: 0s\n",
      "Epoch 22 - Score: 0.8833\n",
      "Epoch: [23][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6771(0.6771) Grad: 1613.2491  LR: 0.00478297  \n",
      "Epoch: [23][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6317(0.6420) Grad: 3302.0481  LR: 0.00478297  \n",
      "Epoch: [23][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7180(0.7180) Grad: 0.0000  \n",
      "Epoch: [23][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7436(0.7299) Grad: 0.0000  \n",
      "Epoch 23 - avg_train_loss: 0.6420  avg_val_loss: 0.7299  time: 0s\n",
      "Epoch 23 - Score: 0.8292\n",
      "Epoch: [24][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6438(0.6438) Grad: 1304.7302  LR: 0.00478297  \n",
      "Epoch: [24][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6245(0.6414) Grad: 1943.4290  LR: 0.00478297  \n",
      "Epoch: [24][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6511(0.6511) Grad: 0.0000  \n",
      "Epoch: [24][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6998(0.6738) Grad: 0.0000  \n",
      "Epoch 24 - avg_train_loss: 0.6414  avg_val_loss: 0.6738  time: 0s\n",
      "Epoch 24 - Score: 0.8875\n",
      "Epoch: [25][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6227(0.6227) Grad: 2000.7084  LR: 0.00478297  \n",
      "Epoch: [25][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6406) Grad: 1347.5381  LR: 0.00478297  \n",
      "Epoch: [25][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6522(0.6522) Grad: 0.0000  \n",
      "Epoch: [25][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6958(0.6726) Grad: 0.0000  \n",
      "Epoch 25 - avg_train_loss: 0.6406  avg_val_loss: 0.6726  time: 0s\n",
      "Epoch 25 - Score: 0.8917\n",
      "Epoch 25 - Save Best Score: 0.8917 Model\n",
      "Epoch: [26][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6193(0.6193) Grad: 1919.9501  LR: 0.00387420  \n",
      "Epoch: [26][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6544(0.6409) Grad: 2402.4534  LR: 0.00430467  \n",
      "Epoch: [26][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6513) Grad: 0.0000  \n",
      "Epoch: [26][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6902(0.6695) Grad: 0.0000  \n",
      "Epoch 26 - avg_train_loss: 0.6409  avg_val_loss: 0.6695  time: 0s\n",
      "Epoch 26 - Score: 0.8917\n",
      "Epoch: [27][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6247(0.6247) Grad: 1632.0256  LR: 0.00430467  \n",
      "Epoch: [27][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6223(0.6397) Grad: 1531.4294  LR: 0.00430467  \n",
      "Epoch: [27][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6509(0.6509) Grad: 0.0000  \n",
      "Epoch: [27][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6840(0.6664) Grad: 0.0000  \n",
      "Epoch 27 - avg_train_loss: 0.6397  avg_val_loss: 0.6664  time: 0s\n",
      "Epoch 27 - Score: 0.9000\n",
      "Epoch 27 - Save Best Score: 0.9000 Model\n",
      "Epoch: [28][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6206(0.6206) Grad: 1231.9172  LR: 0.00430467  \n",
      "Epoch: [28][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6378) Grad: 2053.8091  LR: 0.00430467  \n",
      "Epoch: [28][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6521(0.6521) Grad: 0.0000  \n",
      "Epoch: [28][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6830(0.6665) Grad: 0.0000  \n",
      "Epoch 28 - avg_train_loss: 0.6378  avg_val_loss: 0.6665  time: 0s\n",
      "Epoch 28 - Score: 0.9000\n",
      "Epoch: [29][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6583(0.6583) Grad: 1451.1327  LR: 0.00430467  \n",
      "Epoch: [29][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6375(0.6360) Grad: 1662.3682  LR: 0.00387420  \n",
      "Epoch: [29][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6450) Grad: 0.0000  \n",
      "Epoch: [29][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6714(0.6573) Grad: 0.0000  \n",
      "Epoch 29 - avg_train_loss: 0.6360  avg_val_loss: 0.6573  time: 0s\n",
      "Epoch 29 - Score: 0.9042\n",
      "Epoch 29 - Save Best Score: 0.9042 Model\n",
      "Epoch: [30][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6322(0.6322) Grad: 969.9100  LR: 0.00387420  \n",
      "Epoch: [30][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6001(0.6319) Grad: 1537.6328  LR: 0.00387420  \n",
      "Epoch: [30][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6413(0.6413) Grad: 0.0000  \n",
      "Epoch: [30][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6692(0.6543) Grad: 0.0000  \n",
      "Epoch 30 - avg_train_loss: 0.6319  avg_val_loss: 0.6543  time: 0s\n",
      "Epoch 30 - Score: 0.9042\n",
      "Epoch: [31][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6325(0.6325) Grad: 1620.2418  LR: 0.00387420  \n",
      "Epoch: [31][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6318(0.6357) Grad: 888.9174  LR: 0.00387420  \n",
      "Epoch: [31][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6392(0.6392) Grad: 0.0000  \n",
      "Epoch: [31][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6691(0.6531) Grad: 0.0000  \n",
      "Epoch 31 - avg_train_loss: 0.6357  avg_val_loss: 0.6531  time: 0s\n",
      "Epoch 31 - Score: 0.9083\n",
      "Epoch 31 - Save Best Score: 0.9083 Model\n",
      "Epoch: [32][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6353(0.6353) Grad: 1142.0818  LR: 0.00387420  \n",
      "Epoch: [32][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6327(0.6309) Grad: 1843.2538  LR: 0.00348678  \n",
      "Epoch: [32][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6362(0.6362) Grad: 0.0000  \n",
      "Epoch: [32][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6681(0.6511) Grad: 0.0000  \n",
      "Epoch 32 - avg_train_loss: 0.6309  avg_val_loss: 0.6511  time: 0s\n",
      "Epoch 32 - Score: 0.9083\n",
      "Epoch: [33][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6034(0.6034) Grad: 1664.9800  LR: 0.00348678  \n",
      "Epoch: [33][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6081(0.6274) Grad: 876.1682  LR: 0.00348678  \n",
      "Epoch: [33][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6374(0.6374) Grad: 0.0000  \n",
      "Epoch: [33][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6671(0.6513) Grad: 0.0000  \n",
      "Epoch 33 - avg_train_loss: 0.6274  avg_val_loss: 0.6513  time: 0s\n",
      "Epoch 33 - Score: 0.9083\n",
      "Epoch: [34][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6255(0.6255) Grad: 967.8231  LR: 0.00348678  \n",
      "Epoch: [34][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6147(0.6327) Grad: 725.5192  LR: 0.00348678  \n",
      "Epoch: [34][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6386(0.6386) Grad: 0.0000  \n",
      "Epoch: [34][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6663(0.6515) Grad: 0.0000  \n",
      "Epoch 34 - avg_train_loss: 0.6327  avg_val_loss: 0.6515  time: 0s\n",
      "Epoch 34 - Score: 0.9083\n",
      "Epoch: [35][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 757.0910  LR: 0.00348678  \n",
      "Epoch: [35][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6913(0.6280) Grad: 976.2803  LR: 0.00313811  \n",
      "Epoch: [35][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6274(0.6274) Grad: 0.0000  \n",
      "Epoch: [35][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6560(0.6408) Grad: 0.0000  \n",
      "Epoch 35 - avg_train_loss: 0.6280  avg_val_loss: 0.6408  time: 0s\n",
      "Epoch 35 - Score: 0.9125\n",
      "Epoch 35 - Save Best Score: 0.9125 Model\n",
      "Epoch: [36][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6372(0.6372) Grad: 3663.0071  LR: 0.00313811  \n",
      "Epoch: [36][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6629(0.6356) Grad: 1518.5907  LR: 0.00313811  \n",
      "Epoch: [36][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6244(0.6244) Grad: 0.0000  \n",
      "Epoch: [36][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6564(0.6393) Grad: 0.0000  \n",
      "Epoch 36 - avg_train_loss: 0.6356  avg_val_loss: 0.6393  time: 0s\n",
      "Epoch 36 - Score: 0.9208\n",
      "Epoch 36 - Save Best Score: 0.9208 Model\n",
      "Epoch: [37][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6009(0.6009) Grad: 1931.6951  LR: 0.00313811  \n",
      "Epoch: [37][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6652(0.6298) Grad: 508.0416  LR: 0.00313811  \n",
      "Epoch: [37][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6285(0.6285) Grad: 0.0000  \n",
      "Epoch: [37][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6631(0.6447) Grad: 0.0000  \n",
      "Epoch 37 - avg_train_loss: 0.6298  avg_val_loss: 0.6447  time: 0s\n",
      "Epoch 37 - Score: 0.9125\n",
      "Epoch: [38][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5932(0.5932) Grad: 1666.0490  LR: 0.00313811  \n",
      "Epoch: [38][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6338) Grad: 3804.4053  LR: 0.00282430  \n",
      "Epoch: [38][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 0.0000  \n",
      "Epoch: [38][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6335) Grad: 0.0000  \n",
      "Epoch 38 - avg_train_loss: 0.6338  avg_val_loss: 0.6335  time: 0s\n",
      "Epoch 38 - Score: 0.9250\n",
      "Epoch 38 - Save Best Score: 0.9250 Model\n",
      "Epoch: [39][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6286(0.6286) Grad: 935.0992  LR: 0.00282430  \n",
      "Epoch: [39][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6025(0.6329) Grad: 1023.4749  LR: 0.00282430  \n",
      "Epoch: [39][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6190(0.6190) Grad: 0.0000  \n",
      "Epoch: [39][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6543(0.6354) Grad: 0.0000  \n",
      "Epoch 39 - avg_train_loss: 0.6329  avg_val_loss: 0.6354  time: 0s\n",
      "Epoch 39 - Score: 0.9208\n",
      "Epoch: [40][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6446) Grad: 982.1668  LR: 0.00282430  \n",
      "Epoch: [40][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6667(0.6262) Grad: 678.8680  LR: 0.00282430  \n",
      "Epoch: [40][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6213(0.6213) Grad: 0.0000  \n",
      "Epoch: [40][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6552(0.6371) Grad: 0.0000  \n",
      "Epoch 40 - avg_train_loss: 0.6262  avg_val_loss: 0.6371  time: 0s\n",
      "Epoch 40 - Score: 0.9208\n",
      "Epoch: [41][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5937(0.5937) Grad: 2300.0928  LR: 0.00282430  \n",
      "Epoch: [41][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6124(0.6302) Grad: 1024.4120  LR: 0.00254187  \n",
      "Epoch: [41][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6201(0.6201) Grad: 0.0000  \n",
      "Epoch: [41][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6528(0.6353) Grad: 0.0000  \n",
      "Epoch 41 - avg_train_loss: 0.6302  avg_val_loss: 0.6353  time: 0s\n",
      "Epoch 41 - Score: 0.9208\n",
      "Epoch: [42][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6284(0.6284) Grad: 1018.3066  LR: 0.00254187  \n",
      "Epoch: [42][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5838(0.6280) Grad: 922.6230  LR: 0.00254187  \n",
      "Epoch: [42][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6237(0.6237) Grad: 0.0000  \n",
      "Epoch: [42][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6560(0.6388) Grad: 0.0000  \n",
      "Epoch 42 - avg_train_loss: 0.6280  avg_val_loss: 0.6388  time: 0s\n",
      "Epoch 42 - Score: 0.9167\n",
      "Epoch: [43][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6245(0.6245) Grad: 1953.8549  LR: 0.00254187  \n",
      "Epoch: [43][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6365(0.6347) Grad: 844.2900  LR: 0.00254187  \n",
      "Epoch: [43][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6232(0.6232) Grad: 0.0000  \n",
      "Epoch: [43][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6544(0.6378) Grad: 0.0000  \n",
      "Epoch 43 - avg_train_loss: 0.6347  avg_val_loss: 0.6378  time: 0s\n",
      "Epoch 43 - Score: 0.9208\n",
      "Epoch: [44][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6345(0.6345) Grad: 433.7942  LR: 0.00254187  \n",
      "Epoch: [44][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6452(0.6322) Grad: 2321.1296  LR: 0.00205891  \n",
      "Epoch: [44][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6173(0.6173) Grad: 0.0000  \n",
      "Epoch: [44][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6314) Grad: 0.0000  \n",
      "Epoch 44 - avg_train_loss: 0.6322  avg_val_loss: 0.6314  time: 0s\n",
      "Epoch 44 - Score: 0.9250\n",
      "Epoch: [45][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6589(0.6589) Grad: 830.6147  LR: 0.00228768  \n",
      "Epoch: [45][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6195(0.6272) Grad: 1281.4705  LR: 0.00228768  \n",
      "Epoch: [45][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6094) Grad: 0.0000  \n",
      "Epoch: [45][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6467(0.6268) Grad: 0.0000  \n",
      "Epoch 45 - avg_train_loss: 0.6272  avg_val_loss: 0.6268  time: 0s\n",
      "Epoch 45 - Score: 0.9292\n",
      "Epoch 45 - Save Best Score: 0.9292 Model\n",
      "Epoch: [46][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5915(0.5915) Grad: 749.4242  LR: 0.00228768  \n",
      "Epoch: [46][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6649(0.6275) Grad: 874.5846  LR: 0.00228768  \n",
      "Epoch: [46][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6097) Grad: 0.0000  \n",
      "Epoch: [46][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6470(0.6272) Grad: 0.0000  \n",
      "Epoch 46 - avg_train_loss: 0.6275  avg_val_loss: 0.6272  time: 0s\n",
      "Epoch 46 - Score: 0.9333\n",
      "Epoch 46 - Save Best Score: 0.9333 Model\n",
      "Epoch: [47][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6132(0.6132) Grad: 815.4060  LR: 0.00228768  \n",
      "Epoch: [47][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5922(0.6294) Grad: 1637.7496  LR: 0.00228768  \n",
      "Epoch: [47][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6093) Grad: 0.0000  \n",
      "Epoch: [47][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6264) Grad: 0.0000  \n",
      "Epoch 47 - avg_train_loss: 0.6294  avg_val_loss: 0.6264  time: 0s\n",
      "Epoch 47 - Score: 0.9333\n",
      "Epoch: [48][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6379(0.6379) Grad: 2444.9048  LR: 0.00185302  \n",
      "Epoch: [48][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6522(0.6199) Grad: 788.2783  LR: 0.00205891  \n",
      "Epoch: [48][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6101(0.6101) Grad: 0.0000  \n",
      "Epoch: [48][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6459(0.6268) Grad: 0.0000  \n",
      "Epoch 48 - avg_train_loss: 0.6199  avg_val_loss: 0.6268  time: 0s\n",
      "Epoch 48 - Score: 0.9292\n",
      "Epoch: [49][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6493(0.6493) Grad: 403.9522  LR: 0.00205891  \n",
      "Epoch: [49][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6135(0.6262) Grad: 2462.9451  LR: 0.00205891  \n",
      "Epoch: [49][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6050) Grad: 0.0000  \n",
      "Epoch: [49][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6455(0.6239) Grad: 0.0000  \n",
      "Epoch 49 - avg_train_loss: 0.6262  avg_val_loss: 0.6239  time: 0s\n",
      "Epoch 49 - Score: 0.9333\n",
      "Epoch: [50][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6660(0.6660) Grad: 506.4374  LR: 0.00205891  \n",
      "Epoch: [50][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6339(0.6241) Grad: 4807.5063  LR: 0.00205891  \n",
      "Epoch: [50][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6035) Grad: 0.0000  \n",
      "Epoch: [50][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6442(0.6225) Grad: 0.0000  \n",
      "Epoch 50 - avg_train_loss: 0.6241  avg_val_loss: 0.6225  time: 0s\n",
      "Epoch 50 - Score: 0.9333\n",
      "Epoch: [51][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6277(0.6277) Grad: 585.4482  LR: 0.00205891  \n",
      "Epoch: [51][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6215(0.6322) Grad: 763.9760  LR: 0.00185302  \n",
      "Epoch: [51][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6031(0.6031) Grad: 0.0000  \n",
      "Epoch: [51][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6225) Grad: 0.0000  \n",
      "Epoch 51 - avg_train_loss: 0.6322  avg_val_loss: 0.6225  time: 0s\n",
      "Epoch 51 - Score: 0.9333\n",
      "Epoch: [52][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6307(0.6307) Grad: 3928.4199  LR: 0.00185302  \n",
      "Epoch: [52][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6219(0.6276) Grad: 679.2178  LR: 0.00185302  \n",
      "Epoch: [52][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6040(0.6040) Grad: 0.0000  \n",
      "Epoch: [52][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6458(0.6235) Grad: 0.0000  \n",
      "Epoch 52 - avg_train_loss: 0.6276  avg_val_loss: 0.6235  time: 0s\n",
      "Epoch 52 - Score: 0.9333\n",
      "Epoch: [53][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6291) Grad: 1887.7882  LR: 0.00185302  \n",
      "Epoch: [53][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6121(0.6241) Grad: 882.0628  LR: 0.00185302  \n",
      "Epoch: [53][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6065(0.6065) Grad: 0.0000  \n",
      "Epoch: [53][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6465(0.6252) Grad: 0.0000  \n",
      "Epoch 53 - avg_train_loss: 0.6241  avg_val_loss: 0.6252  time: 0s\n",
      "Epoch 53 - Score: 0.9333\n",
      "Epoch: [54][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6377(0.6377) Grad: 1694.3418  LR: 0.00185302  \n",
      "Epoch: [54][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6458(0.6261) Grad: 1646.7991  LR: 0.00166772  \n",
      "Epoch: [54][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6072(0.6072) Grad: 0.0000  \n",
      "Epoch: [54][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6466(0.6256) Grad: 0.0000  \n",
      "Epoch 54 - avg_train_loss: 0.6261  avg_val_loss: 0.6256  time: 0s\n",
      "Epoch 54 - Score: 0.9333\n",
      "Epoch: [55][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6133(0.6133) Grad: 831.1996  LR: 0.00166772  \n",
      "Epoch: [55][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6699(0.6266) Grad: 2140.4534  LR: 0.00166772  \n",
      "Epoch: [55][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6047) Grad: 0.0000  \n",
      "Epoch: [55][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6458(0.6239) Grad: 0.0000  \n",
      "Epoch 55 - avg_train_loss: 0.6266  avg_val_loss: 0.6239  time: 0s\n",
      "Epoch 55 - Score: 0.9333\n",
      "Epoch: [56][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6356(0.6356) Grad: 589.8141  LR: 0.00166772  \n",
      "Epoch: [56][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6228) Grad: 2594.5688  LR: 0.00166772  \n",
      "Epoch: [56][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6052(0.6052) Grad: 0.0000  \n",
      "Epoch: [56][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6456(0.6241) Grad: 0.0000  \n",
      "Epoch 56 - avg_train_loss: 0.6228  avg_val_loss: 0.6241  time: 0s\n",
      "Epoch 56 - Score: 0.9333\n",
      "Epoch: [57][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6196(0.6196) Grad: 743.6412  LR: 0.00166772  \n",
      "Epoch: [57][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6356(0.6234) Grad: 959.9613  LR: 0.00150095  \n",
      "Epoch: [57][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6116(0.6116) Grad: 0.0000  \n",
      "Epoch: [57][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6455(0.6274) Grad: 0.0000  \n",
      "Epoch 57 - avg_train_loss: 0.6234  avg_val_loss: 0.6274  time: 0s\n",
      "Epoch 57 - Score: 0.9292\n",
      "Epoch: [58][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6260(0.6260) Grad: 542.3819  LR: 0.00150095  \n",
      "Epoch: [58][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6038(0.6217) Grad: 552.9194  LR: 0.00150095  \n",
      "Epoch: [58][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6090(0.6090) Grad: 0.0000  \n",
      "Epoch: [58][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6257) Grad: 0.0000  \n",
      "Epoch 58 - avg_train_loss: 0.6217  avg_val_loss: 0.6257  time: 0s\n",
      "Epoch 58 - Score: 0.9292\n",
      "Epoch: [59][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6275(0.6275) Grad: 490.5958  LR: 0.00150095  \n",
      "Epoch: [59][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5943(0.6237) Grad: 485.1194  LR: 0.00150095  \n",
      "Epoch: [59][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6051(0.6051) Grad: 0.0000  \n",
      "Epoch: [59][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6235) Grad: 0.0000  \n",
      "Epoch 59 - avg_train_loss: 0.6237  avg_val_loss: 0.6235  time: 0s\n",
      "Epoch 59 - Score: 0.9333\n",
      "Epoch: [60][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5960(0.5960) Grad: 988.6964  LR: 0.00150095  \n",
      "Epoch: [60][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6207(0.6248) Grad: 2257.0017  LR: 0.00135085  \n",
      "Epoch: [60][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6041(0.6041) Grad: 0.0000  \n",
      "Epoch: [60][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6451(0.6232) Grad: 0.0000  \n",
      "Epoch 60 - avg_train_loss: 0.6248  avg_val_loss: 0.6232  time: 0s\n",
      "Epoch 60 - Score: 0.9333\n",
      "Epoch: [61][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6349(0.6349) Grad: 618.5052  LR: 0.00135085  \n",
      "Epoch: [61][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6077(0.6278) Grad: 889.9688  LR: 0.00135085  \n",
      "Epoch: [61][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6049(0.6049) Grad: 0.0000  \n",
      "Epoch: [61][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6453(0.6237) Grad: 0.0000  \n",
      "Epoch 61 - avg_train_loss: 0.6278  avg_val_loss: 0.6237  time: 0s\n",
      "Epoch 61 - Score: 0.9333\n",
      "Epoch: [62][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6250(0.6250) Grad: 368.0526  LR: 0.00135085  \n",
      "Epoch: [62][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6428(0.6286) Grad: 592.7675  LR: 0.00135085  \n",
      "Epoch: [62][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6041(0.6041) Grad: 0.0000  \n",
      "Epoch: [62][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6452(0.6233) Grad: 0.0000  \n",
      "Epoch 62 - avg_train_loss: 0.6286  avg_val_loss: 0.6233  time: 0s\n",
      "Epoch 62 - Score: 0.9333\n",
      "Epoch: [63][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6491) Grad: 337.6625  LR: 0.00135085  \n",
      "Epoch: [63][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6254(0.6297) Grad: 335.5703  LR: 0.00121577  \n",
      "Epoch: [63][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6042(0.6042) Grad: 0.0000  \n",
      "Epoch: [63][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6452(0.6233) Grad: 0.0000  \n",
      "Epoch 63 - avg_train_loss: 0.6297  avg_val_loss: 0.6233  time: 0s\n",
      "Epoch 63 - Score: 0.9333\n",
      "Epoch: [64][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 694.6027  LR: 0.00121577  \n",
      "Epoch: [64][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6351(0.6266) Grad: 773.9646  LR: 0.00121577  \n",
      "Epoch: [64][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6037(0.6037) Grad: 0.0000  \n",
      "Epoch: [64][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6453(0.6231) Grad: 0.0000  \n",
      "Epoch 64 - avg_train_loss: 0.6266  avg_val_loss: 0.6231  time: 0s\n",
      "Epoch 64 - Score: 0.9333\n",
      "Epoch: [65][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6097) Grad: 1025.6798  LR: 0.00121577  \n",
      "Epoch: [65][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6263) Grad: 392.9693  LR: 0.00121577  \n",
      "Epoch: [65][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6038(0.6038) Grad: 0.0000  \n",
      "Epoch: [65][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6451(0.6231) Grad: 0.0000  \n",
      "Epoch 65 - avg_train_loss: 0.6263  avg_val_loss: 0.6231  time: 0s\n",
      "Epoch 65 - Score: 0.9333\n",
      "Epoch: [66][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6194) Grad: 536.1355  LR: 0.00121577  \n",
      "Epoch: [66][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6627(0.6267) Grad: 585.3387  LR: 0.00098477  \n",
      "Epoch: [66][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6042(0.6042) Grad: 0.0000  \n",
      "Epoch: [66][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6232) Grad: 0.0000  \n",
      "Epoch 66 - avg_train_loss: 0.6267  avg_val_loss: 0.6232  time: 0s\n",
      "Epoch 66 - Score: 0.9333\n",
      "Epoch: [67][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5961(0.5961) Grad: 480.4432  LR: 0.00109419  \n",
      "Epoch: [67][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6014(0.6228) Grad: 1653.7542  LR: 0.00109419  \n",
      "Epoch: [67][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6041(0.6041) Grad: 0.0000  \n",
      "Epoch: [67][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6231) Grad: 0.0000  \n",
      "Epoch 67 - avg_train_loss: 0.6228  avg_val_loss: 0.6231  time: 0s\n",
      "Epoch 67 - Score: 0.9333\n",
      "Epoch: [68][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5959(0.5959) Grad: 483.8416  LR: 0.00109419  \n",
      "Epoch: [68][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6325(0.6216) Grad: 326.6514  LR: 0.00109419  \n",
      "Epoch: [68][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6035) Grad: 0.0000  \n",
      "Epoch: [68][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6228) Grad: 0.0000  \n",
      "Epoch 68 - avg_train_loss: 0.6216  avg_val_loss: 0.6228  time: 0s\n",
      "Epoch 68 - Score: 0.9333\n",
      "Epoch: [69][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6566(0.6566) Grad: 325.6345  LR: 0.00109419  \n",
      "Epoch: [69][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6020(0.6212) Grad: 441.9266  LR: 0.00109419  \n",
      "Epoch: [69][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6027(0.6027) Grad: 0.0000  \n",
      "Epoch: [69][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6223) Grad: 0.0000  \n",
      "Epoch 69 - avg_train_loss: 0.6212  avg_val_loss: 0.6223  time: 0s\n",
      "Epoch 69 - Score: 0.9333\n",
      "Epoch: [70][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6261(0.6261) Grad: 562.1160  LR: 0.00088629  \n",
      "Epoch: [70][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6371(0.6244) Grad: 1917.3080  LR: 0.00098477  \n",
      "Epoch: [70][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6023(0.6023) Grad: 0.0000  \n",
      "Epoch: [70][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6443(0.6219) Grad: 0.0000  \n",
      "Epoch 70 - avg_train_loss: 0.6244  avg_val_loss: 0.6219  time: 0s\n",
      "Epoch 70 - Score: 0.9333\n",
      "Epoch: [71][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6168(0.6168) Grad: 703.0488  LR: 0.00098477  \n",
      "Epoch: [71][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6241(0.6245) Grad: 1454.1937  LR: 0.00098477  \n",
      "Epoch: [71][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6026(0.6026) Grad: 0.0000  \n",
      "Epoch: [71][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6222) Grad: 0.0000  \n",
      "Epoch 71 - avg_train_loss: 0.6245  avg_val_loss: 0.6222  time: 0s\n",
      "Epoch 71 - Score: 0.9333\n",
      "Epoch: [72][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6261(0.6261) Grad: 401.3974  LR: 0.00098477  \n",
      "Epoch: [72][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5958(0.6242) Grad: 1241.1334  LR: 0.00098477  \n",
      "Epoch: [72][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6044) Grad: 0.0000  \n",
      "Epoch: [72][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6232) Grad: 0.0000  \n",
      "Epoch 72 - avg_train_loss: 0.6242  avg_val_loss: 0.6232  time: 0s\n",
      "Epoch 72 - Score: 0.9333\n",
      "Epoch: [73][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6177(0.6177) Grad: 339.9724  LR: 0.00098477  \n",
      "Epoch: [73][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6241) Grad: 359.7894  LR: 0.00088629  \n",
      "Epoch: [73][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6083(0.6083) Grad: 0.0000  \n",
      "Epoch: [73][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6253) Grad: 0.0000  \n",
      "Epoch 73 - avg_train_loss: 0.6241  avg_val_loss: 0.6253  time: 0s\n",
      "Epoch 73 - Score: 0.9292\n",
      "Epoch: [74][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6177(0.6177) Grad: 326.2083  LR: 0.00088629  \n",
      "Epoch: [74][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6102(0.6182) Grad: 513.7905  LR: 0.00088629  \n",
      "Epoch: [74][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6095) Grad: 0.0000  \n",
      "Epoch: [74][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6444(0.6258) Grad: 0.0000  \n",
      "Epoch 74 - avg_train_loss: 0.6182  avg_val_loss: 0.6258  time: 0s\n",
      "Epoch 74 - Score: 0.9292\n",
      "Epoch: [75][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6169(0.6169) Grad: 419.5860  LR: 0.00088629  \n",
      "Epoch: [75][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6102(0.6230) Grad: 364.6330  LR: 0.00088629  \n",
      "Epoch: [75][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6100(0.6100) Grad: 0.0000  \n",
      "Epoch: [75][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6444(0.6260) Grad: 0.0000  \n",
      "Epoch 75 - avg_train_loss: 0.6230  avg_val_loss: 0.6260  time: 0s\n",
      "Epoch 75 - Score: 0.9292\n",
      "Epoch: [76][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5949(0.5949) Grad: 539.4747  LR: 0.00088629  \n",
      "Epoch: [76][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6409(0.6221) Grad: 589.2000  LR: 0.00079766  \n",
      "Epoch: [76][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6066) Grad: 0.0000  \n",
      "Epoch: [76][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6443(0.6242) Grad: 0.0000  \n",
      "Epoch 76 - avg_train_loss: 0.6221  avg_val_loss: 0.6242  time: 0s\n",
      "Epoch 76 - Score: 0.9292\n",
      "Epoch: [77][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6201(0.6201) Grad: 567.0666  LR: 0.00079766  \n",
      "Epoch: [77][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6245) Grad: 468.0298  LR: 0.00079766  \n",
      "Epoch: [77][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6037(0.6037) Grad: 0.0000  \n",
      "Epoch: [77][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6440(0.6225) Grad: 0.0000  \n",
      "Epoch 77 - avg_train_loss: 0.6245  avg_val_loss: 0.6225  time: 0s\n",
      "Epoch 77 - Score: 0.9333\n",
      "Epoch: [78][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6106(0.6106) Grad: 387.2623  LR: 0.00079766  \n",
      "Epoch: [78][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6326(0.6208) Grad: 512.8768  LR: 0.00079766  \n",
      "Epoch: [78][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [78][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6438(0.6237) Grad: 0.0000  \n",
      "Epoch 78 - avg_train_loss: 0.6208  avg_val_loss: 0.6237  time: 0s\n",
      "Epoch 78 - Score: 0.9292\n",
      "Epoch: [79][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6376(0.6376) Grad: 2922.7375  LR: 0.00079766  \n",
      "Epoch: [79][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5873(0.6218) Grad: 918.8783  LR: 0.00071790  \n",
      "Epoch: [79][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6074(0.6074) Grad: 0.0000  \n",
      "Epoch: [79][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6439(0.6244) Grad: 0.0000  \n",
      "Epoch 79 - avg_train_loss: 0.6218  avg_val_loss: 0.6244  time: 0s\n",
      "Epoch 79 - Score: 0.9292\n",
      "Epoch: [80][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6177(0.6177) Grad: 375.0296  LR: 0.00071790  \n",
      "Epoch: [80][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6403(0.6254) Grad: 1834.2230  LR: 0.00071790  \n",
      "Epoch: [80][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6080(0.6080) Grad: 0.0000  \n",
      "Epoch: [80][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6250) Grad: 0.0000  \n",
      "Epoch 80 - avg_train_loss: 0.6254  avg_val_loss: 0.6250  time: 0s\n",
      "Epoch 80 - Score: 0.9292\n",
      "Epoch: [81][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6193(0.6193) Grad: 2500.2988  LR: 0.00071790  \n",
      "Epoch: [81][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6477(0.6264) Grad: 556.6700  LR: 0.00071790  \n",
      "Epoch: [81][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6045(0.6045) Grad: 0.0000  \n",
      "Epoch: [81][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6232) Grad: 0.0000  \n",
      "Epoch 81 - avg_train_loss: 0.6264  avg_val_loss: 0.6232  time: 0s\n",
      "Epoch 81 - Score: 0.9333\n",
      "Epoch: [82][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6263(0.6263) Grad: 394.3709  LR: 0.00071790  \n",
      "Epoch: [82][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6039(0.6244) Grad: 723.3575  LR: 0.00064611  \n",
      "Epoch: [82][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6025(0.6025) Grad: 0.0000  \n",
      "Epoch: [82][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6444(0.6221) Grad: 0.0000  \n",
      "Epoch 82 - avg_train_loss: 0.6244  avg_val_loss: 0.6221  time: 0s\n",
      "Epoch 82 - Score: 0.9333\n",
      "Epoch: [83][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6124(0.6124) Grad: 2484.2754  LR: 0.00064611  \n",
      "Epoch: [83][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6193(0.6223) Grad: 466.8289  LR: 0.00064611  \n",
      "Epoch: [83][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6021(0.6021) Grad: 0.0000  \n",
      "Epoch: [83][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6444(0.6219) Grad: 0.0000  \n",
      "Epoch 83 - avg_train_loss: 0.6223  avg_val_loss: 0.6219  time: 0s\n",
      "Epoch 83 - Score: 0.9333\n",
      "Epoch: [84][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6178) Grad: 525.6649  LR: 0.00064611  \n",
      "Epoch: [84][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6303(0.6263) Grad: 3455.4185  LR: 0.00064611  \n",
      "Epoch: [84][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6013(0.6013) Grad: 0.0000  \n",
      "Epoch: [84][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6438(0.6211) Grad: 0.0000  \n",
      "Epoch 84 - avg_train_loss: 0.6263  avg_val_loss: 0.6211  time: 0s\n",
      "Epoch 84 - Score: 0.9333\n",
      "Epoch: [85][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5937(0.5937) Grad: 763.9976  LR: 0.00064611  \n",
      "Epoch: [85][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6188) Grad: 1117.5208  LR: 0.00058150  \n",
      "Epoch: [85][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6030(0.6030) Grad: 0.0000  \n",
      "Epoch: [85][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6441(0.6222) Grad: 0.0000  \n",
      "Epoch 85 - avg_train_loss: 0.6188  avg_val_loss: 0.6222  time: 0s\n",
      "Epoch 85 - Score: 0.9333\n",
      "Epoch: [86][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6049(0.6049) Grad: 2676.1958  LR: 0.00058150  \n",
      "Epoch: [86][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6133(0.6218) Grad: 7160.5166  LR: 0.00058150  \n",
      "Epoch: [86][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6054(0.6054) Grad: 0.0000  \n",
      "Epoch: [86][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6250) Grad: 0.0000  \n",
      "Epoch 86 - avg_train_loss: 0.6218  avg_val_loss: 0.6250  time: 0s\n",
      "Epoch 86 - Score: 0.9333\n",
      "Epoch: [87][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6084(0.6084) Grad: 3311.0977  LR: 0.00058150  \n",
      "Epoch: [87][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6004(0.6197) Grad: 3247.8411  LR: 0.00058150  \n",
      "Epoch: [87][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6050) Grad: 0.0000  \n",
      "Epoch: [87][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6235) Grad: 0.0000  \n",
      "Epoch 87 - avg_train_loss: 0.6197  avg_val_loss: 0.6235  time: 0s\n",
      "Epoch 87 - Score: 0.9333\n",
      "Epoch: [88][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6547(0.6547) Grad: 1032.8606  LR: 0.00058150  \n",
      "Epoch: [88][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6260(0.6229) Grad: 2159.5566  LR: 0.00047101  \n",
      "Epoch: [88][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6039(0.6039) Grad: 0.0000  \n",
      "Epoch: [88][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6437(0.6225) Grad: 0.0000  \n",
      "Epoch 88 - avg_train_loss: 0.6229  avg_val_loss: 0.6225  time: 0s\n",
      "Epoch 88 - Score: 0.9333\n",
      "Epoch: [89][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6314(0.6314) Grad: 1674.1968  LR: 0.00052335  \n",
      "Epoch: [89][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6029(0.6251) Grad: 470.4150  LR: 0.00052335  \n",
      "Epoch: [89][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6033(0.6033) Grad: 0.0000  \n",
      "Epoch: [89][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6436(0.6221) Grad: 0.0000  \n",
      "Epoch 89 - avg_train_loss: 0.6251  avg_val_loss: 0.6221  time: 0s\n",
      "Epoch 89 - Score: 0.9333\n",
      "Epoch: [90][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6091) Grad: 379.4205  LR: 0.00052335  \n",
      "Epoch: [90][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6017(0.6264) Grad: 647.0046  LR: 0.00052335  \n",
      "Epoch: [90][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6024(0.6024) Grad: 0.0000  \n",
      "Epoch: [90][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6433(0.6215) Grad: 0.0000  \n",
      "Epoch 90 - avg_train_loss: 0.6264  avg_val_loss: 0.6215  time: 0s\n",
      "Epoch 90 - Score: 0.9333\n",
      "Epoch: [91][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6531(0.6531) Grad: 2913.5334  LR: 0.00052335  \n",
      "Epoch: [91][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5862(0.6204) Grad: 566.1943  LR: 0.00052335  \n",
      "Epoch: [91][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6027(0.6027) Grad: 0.0000  \n",
      "Epoch: [91][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6223) Grad: 0.0000  \n",
      "Epoch 91 - avg_train_loss: 0.6204  avg_val_loss: 0.6223  time: 0s\n",
      "Epoch 91 - Score: 0.9333\n",
      "Epoch: [92][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6275(0.6275) Grad: 1245.1046  LR: 0.00042391  \n",
      "Epoch: [92][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6136(0.6204) Grad: 3995.3325  LR: 0.00047101  \n",
      "Epoch: [92][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6032(0.6032) Grad: 0.0000  \n",
      "Epoch: [92][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6227) Grad: 0.0000  \n",
      "Epoch 92 - avg_train_loss: 0.6204  avg_val_loss: 0.6227  time: 0s\n",
      "Epoch 92 - Score: 0.9333\n",
      "Epoch: [93][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6299(0.6299) Grad: 2244.0732  LR: 0.00047101  \n",
      "Epoch: [93][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6197(0.6217) Grad: 2009.0428  LR: 0.00047101  \n",
      "Epoch: [93][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6050) Grad: 0.0000  \n",
      "Epoch: [93][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6452(0.6238) Grad: 0.0000  \n",
      "Epoch 93 - avg_train_loss: 0.6217  avg_val_loss: 0.6238  time: 0s\n",
      "Epoch 93 - Score: 0.9333\n",
      "Epoch: [94][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6329(0.6329) Grad: 928.6801  LR: 0.00047101  \n",
      "Epoch: [94][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5905(0.6248) Grad: 1396.3179  LR: 0.00047101  \n",
      "Epoch: [94][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6031(0.6031) Grad: 0.0000  \n",
      "Epoch: [94][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6226) Grad: 0.0000  \n",
      "Epoch 94 - avg_train_loss: 0.6248  avg_val_loss: 0.6226  time: 0s\n",
      "Epoch 94 - Score: 0.9333\n",
      "Epoch: [95][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6129(0.6129) Grad: 1546.2539  LR: 0.00047101  \n",
      "Epoch: [95][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6256(0.6207) Grad: 404.7933  LR: 0.00042391  \n",
      "Epoch: [95][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6030(0.6030) Grad: 0.0000  \n",
      "Epoch: [95][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6225) Grad: 0.0000  \n",
      "Epoch 95 - avg_train_loss: 0.6207  avg_val_loss: 0.6225  time: 0s\n",
      "Epoch 95 - Score: 0.9333\n",
      "Epoch: [96][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6246(0.6246) Grad: 533.4997  LR: 0.00042391  \n",
      "Epoch: [96][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6405(0.6201) Grad: 314.5485  LR: 0.00042391  \n",
      "Epoch: [96][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6044) Grad: 0.0000  \n",
      "Epoch: [96][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6232) Grad: 0.0000  \n",
      "Epoch 96 - avg_train_loss: 0.6201  avg_val_loss: 0.6232  time: 0s\n",
      "Epoch 96 - Score: 0.9333\n",
      "Epoch: [97][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6013(0.6013) Grad: 637.5203  LR: 0.00042391  \n",
      "Epoch: [97][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6160(0.6166) Grad: 825.2282  LR: 0.00042391  \n",
      "Epoch: [97][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6067(0.6067) Grad: 0.0000  \n",
      "Epoch: [97][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6458(0.6249) Grad: 0.0000  \n",
      "Epoch 97 - avg_train_loss: 0.6166  avg_val_loss: 0.6249  time: 0s\n",
      "Epoch 97 - Score: 0.9292\n",
      "Epoch: [98][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6269(0.6269) Grad: 1388.5358  LR: 0.00042391  \n",
      "Epoch: [98][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5958(0.6206) Grad: 2736.3574  LR: 0.00038152  \n",
      "Epoch: [98][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6062(0.6062) Grad: 0.0000  \n",
      "Epoch: [98][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6466(0.6251) Grad: 0.0000  \n",
      "Epoch 98 - avg_train_loss: 0.6206  avg_val_loss: 0.6251  time: 0s\n",
      "Epoch 98 - Score: 0.9292\n",
      "Epoch: [99][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6314(0.6314) Grad: 3333.8914  LR: 0.00038152  \n",
      "Epoch: [99][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6082(0.6160) Grad: 3420.1670  LR: 0.00038152  \n",
      "Epoch: [99][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6049(0.6049) Grad: 0.0000  \n",
      "Epoch: [99][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6241) Grad: 0.0000  \n",
      "Epoch 99 - avg_train_loss: 0.6160  avg_val_loss: 0.6241  time: 0s\n",
      "Epoch 99 - Score: 0.9333\n",
      "Epoch: [100][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6168(0.6168) Grad: 1262.8152  LR: 0.00038152  \n",
      "Epoch: [100][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6303(0.6188) Grad: 2324.9724  LR: 0.00038152  \n",
      "Epoch: [100][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6040(0.6040) Grad: 0.0000  \n",
      "Epoch: [100][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6469(0.6240) Grad: 0.0000  \n",
      "Epoch 100 - avg_train_loss: 0.6188  avg_val_loss: 0.6240  time: 0s\n",
      "Epoch 100 - Score: 0.9333\n",
      "Epoch: [101][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6343(0.6343) Grad: 2409.5854  LR: 0.00038152  \n",
      "Epoch: [101][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6336(0.6235) Grad: 5525.6167  LR: 0.00034337  \n",
      "Epoch: [101][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6037(0.6037) Grad: 0.0000  \n",
      "Epoch: [101][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6477(0.6242) Grad: 0.0000  \n",
      "Epoch 101 - avg_train_loss: 0.6235  avg_val_loss: 0.6242  time: 0s\n",
      "Epoch 101 - Score: 0.9333\n",
      "Epoch: [102][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6344(0.6344) Grad: 2159.3208  LR: 0.00034337  \n",
      "Epoch: [102][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5960(0.6214) Grad: 1895.2837  LR: 0.00034337  \n",
      "Epoch: [102][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6027(0.6027) Grad: 0.0000  \n",
      "Epoch: [102][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6239) Grad: 0.0000  \n",
      "Epoch 102 - avg_train_loss: 0.6214  avg_val_loss: 0.6239  time: 0s\n",
      "Epoch 102 - Score: 0.9333\n",
      "Epoch: [103][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6041(0.6041) Grad: 5447.9282  LR: 0.00034337  \n",
      "Epoch: [103][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6282(0.6249) Grad: 3618.8186  LR: 0.00034337  \n",
      "Epoch: [103][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6021(0.6021) Grad: 0.0000  \n",
      "Epoch: [103][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6233) Grad: 0.0000  \n",
      "Epoch 103 - avg_train_loss: 0.6249  avg_val_loss: 0.6233  time: 0s\n",
      "Epoch 103 - Score: 0.9333\n",
      "Epoch: [104][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6363(0.6363) Grad: 3393.0513  LR: 0.00034337  \n",
      "Epoch: [104][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6141(0.6219) Grad: 1626.9408  LR: 0.00030903  \n",
      "Epoch: [104][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6015(0.6015) Grad: 0.0000  \n",
      "Epoch: [104][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6469(0.6227) Grad: 0.0000  \n",
      "Epoch 104 - avg_train_loss: 0.6219  avg_val_loss: 0.6227  time: 0s\n",
      "Epoch 104 - Score: 0.9333\n",
      "Epoch: [105][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6286(0.6286) Grad: 3868.0251  LR: 0.00030903  \n",
      "Epoch: [105][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6511(0.6236) Grad: 4244.4614  LR: 0.00030903  \n",
      "Epoch: [105][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6017(0.6017) Grad: 0.0000  \n",
      "Epoch: [105][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6478(0.6232) Grad: 0.0000  \n",
      "Epoch 105 - avg_train_loss: 0.6236  avg_val_loss: 0.6232  time: 0s\n",
      "Epoch 105 - Score: 0.9333\n",
      "Epoch: [106][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6110(0.6110) Grad: 2946.3699  LR: 0.00030903  \n",
      "Epoch: [106][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6155(0.6189) Grad: 2284.0627  LR: 0.00030903  \n",
      "Epoch: [106][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6021(0.6021) Grad: 0.0000  \n",
      "Epoch: [106][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6484(0.6237) Grad: 0.0000  \n",
      "Epoch 106 - avg_train_loss: 0.6189  avg_val_loss: 0.6237  time: 0s\n",
      "Epoch 106 - Score: 0.9333\n",
      "Epoch: [107][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6092) Grad: 3848.2429  LR: 0.00030903  \n",
      "Epoch: [107][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6318(0.6226) Grad: 6561.0342  LR: 0.00027813  \n",
      "Epoch: [107][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6023(0.6023) Grad: 0.0000  \n",
      "Epoch: [107][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6485(0.6238) Grad: 0.0000  \n",
      "Epoch 107 - avg_train_loss: 0.6226  avg_val_loss: 0.6238  time: 0s\n",
      "Epoch 107 - Score: 0.9333\n",
      "Epoch: [108][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6450) Grad: 4501.8345  LR: 0.00027813  \n",
      "Epoch: [108][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6282(0.6208) Grad: 3384.2529  LR: 0.00027813  \n",
      "Epoch: [108][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6027(0.6027) Grad: 0.0000  \n",
      "Epoch: [108][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6248) Grad: 0.0000  \n",
      "Epoch 108 - avg_train_loss: 0.6208  avg_val_loss: 0.6248  time: 0s\n",
      "Epoch 108 - Score: 0.9292\n",
      "Epoch: [109][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6175(0.6175) Grad: 678.9653  LR: 0.00027813  \n",
      "Epoch: [109][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6191(0.6249) Grad: 4261.7593  LR: 0.00027813  \n",
      "Epoch: [109][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6030(0.6030) Grad: 0.0000  \n",
      "Epoch: [109][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6248) Grad: 0.0000  \n",
      "Epoch 109 - avg_train_loss: 0.6249  avg_val_loss: 0.6248  time: 0s\n",
      "Epoch 109 - Score: 0.9333\n",
      "Epoch: [110][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5796(0.5796) Grad: 1870.8239  LR: 0.00027813  \n",
      "Epoch: [110][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6312(0.6187) Grad: 4412.6992  LR: 0.00022528  \n",
      "Epoch: [110][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6030(0.6030) Grad: 0.0000  \n",
      "Epoch: [110][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6482(0.6241) Grad: 0.0000  \n",
      "Epoch 110 - avg_train_loss: 0.6187  avg_val_loss: 0.6241  time: 0s\n",
      "Epoch 110 - Score: 0.9333\n",
      "Epoch: [111][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6326(0.6326) Grad: 847.5806  LR: 0.00025032  \n",
      "Epoch: [111][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6319(0.6193) Grad: 522.2003  LR: 0.00025032  \n",
      "Epoch: [111][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6028(0.6028) Grad: 0.0000  \n",
      "Epoch: [111][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6478(0.6238) Grad: 0.0000  \n",
      "Epoch 111 - avg_train_loss: 0.6193  avg_val_loss: 0.6238  time: 0s\n",
      "Epoch 111 - Score: 0.9333\n",
      "Epoch: [112][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6318(0.6318) Grad: 983.3923  LR: 0.00025032  \n",
      "Epoch: [112][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5819(0.6221) Grad: 2585.1287  LR: 0.00025032  \n",
      "Epoch: [112][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6036(0.6036) Grad: 0.0000  \n",
      "Epoch: [112][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6488(0.6247) Grad: 0.0000  \n",
      "Epoch 112 - avg_train_loss: 0.6221  avg_val_loss: 0.6247  time: 0s\n",
      "Epoch 112 - Score: 0.9292\n",
      "Epoch: [113][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6330(0.6330) Grad: 1607.8154  LR: 0.00025032  \n",
      "Epoch: [113][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6304(0.6226) Grad: 5700.2803  LR: 0.00025032  \n",
      "Epoch: [113][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6044) Grad: 0.0000  \n",
      "Epoch: [113][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6471(0.6243) Grad: 0.0000  \n",
      "Epoch 113 - avg_train_loss: 0.6226  avg_val_loss: 0.6243  time: 0s\n",
      "Epoch 113 - Score: 0.9333\n",
      "Epoch: [114][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6244(0.6244) Grad: 2056.9158  LR: 0.00020276  \n",
      "Epoch: [114][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6086(0.6198) Grad: 2536.2288  LR: 0.00022528  \n",
      "Epoch: [114][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6047) Grad: 0.0000  \n",
      "Epoch: [114][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6466(0.6243) Grad: 0.0000  \n",
      "Epoch 114 - avg_train_loss: 0.6198  avg_val_loss: 0.6243  time: 0s\n",
      "Epoch 114 - Score: 0.9333\n",
      "Epoch: [115][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5989(0.5989) Grad: 3128.8586  LR: 0.00022528  \n",
      "Epoch: [115][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6160) Grad: 1194.0848  LR: 0.00022528  \n",
      "Epoch: [115][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6043(0.6043) Grad: 0.0000  \n",
      "Epoch: [115][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6467(0.6241) Grad: 0.0000  \n",
      "Epoch 115 - avg_train_loss: 0.6160  avg_val_loss: 0.6241  time: 0s\n",
      "Epoch 115 - Score: 0.9333\n",
      "Epoch: [116][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6068(0.6068) Grad: 3462.9753  LR: 0.00022528  \n",
      "Epoch: [116][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6177) Grad: 4430.1372  LR: 0.00022528  \n",
      "Epoch: [116][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6044) Grad: 0.0000  \n",
      "Epoch: [116][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6259) Grad: 0.0000  \n",
      "Epoch 116 - avg_train_loss: 0.6177  avg_val_loss: 0.6259  time: 0s\n",
      "Epoch 116 - Score: 0.9292\n",
      "Epoch: [117][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6348(0.6348) Grad: 7468.9302  LR: 0.00022528  \n",
      "Epoch: [117][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6184) Grad: 1985.9174  LR: 0.00020276  \n",
      "Epoch: [117][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6042(0.6042) Grad: 0.0000  \n",
      "Epoch: [117][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6520(0.6265) Grad: 0.0000  \n",
      "Epoch 117 - avg_train_loss: 0.6184  avg_val_loss: 0.6265  time: 0s\n",
      "Epoch 117 - Score: 0.9292\n",
      "Epoch: [118][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6169(0.6169) Grad: 5664.6323  LR: 0.00020276  \n",
      "Epoch: [118][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6202) Grad: 844.7795  LR: 0.00020276  \n",
      "Epoch: [118][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6046) Grad: 0.0000  \n",
      "Epoch: [118][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6540(0.6277) Grad: 0.0000  \n",
      "Epoch 118 - avg_train_loss: 0.6202  avg_val_loss: 0.6277  time: 0s\n",
      "Epoch 118 - Score: 0.9292\n",
      "Epoch: [119][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5816(0.5816) Grad: 989.3590  LR: 0.00020276  \n",
      "Epoch: [119][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6583(0.6231) Grad: 6908.2109  LR: 0.00020276  \n",
      "Epoch: [119][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6044) Grad: 0.0000  \n",
      "Epoch: [119][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6550(0.6280) Grad: 0.0000  \n",
      "Epoch 119 - avg_train_loss: 0.6231  avg_val_loss: 0.6280  time: 0s\n",
      "Epoch 119 - Score: 0.9292\n",
      "Epoch: [120][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6391(0.6391) Grad: 6000.9561  LR: 0.00020276  \n",
      "Epoch: [120][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6238(0.6221) Grad: 5323.3936  LR: 0.00018248  \n",
      "Epoch: [120][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6055(0.6055) Grad: 0.0000  \n",
      "Epoch: [120][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6549(0.6285) Grad: 0.0000  \n",
      "Epoch 120 - avg_train_loss: 0.6221  avg_val_loss: 0.6285  time: 0s\n",
      "Epoch 120 - Score: 0.9208\n",
      "Epoch: [121][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5778(0.5778) Grad: 5134.1343  LR: 0.00018248  \n",
      "Epoch: [121][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6337(0.6179) Grad: 796.3357  LR: 0.00018248  \n",
      "Epoch: [121][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [121][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6549(0.6289) Grad: 0.0000  \n",
      "Epoch 121 - avg_train_loss: 0.6179  avg_val_loss: 0.6289  time: 0s\n",
      "Epoch 121 - Score: 0.9208\n",
      "Epoch: [122][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6431(0.6431) Grad: 5117.0278  LR: 0.00018248  \n",
      "Epoch: [122][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6145(0.6169) Grad: 6594.5508  LR: 0.00018248  \n",
      "Epoch: [122][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6058(0.6058) Grad: 0.0000  \n",
      "Epoch: [122][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6281) Grad: 0.0000  \n",
      "Epoch 122 - avg_train_loss: 0.6169  avg_val_loss: 0.6281  time: 0s\n",
      "Epoch 122 - Score: 0.9250\n",
      "Epoch: [123][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5778(0.5778) Grad: 6959.0659  LR: 0.00018248  \n",
      "Epoch: [123][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6424(0.6171) Grad: 2365.5188  LR: 0.00016423  \n",
      "Epoch: [123][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6058(0.6058) Grad: 0.0000  \n",
      "Epoch: [123][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6531(0.6279) Grad: 0.0000  \n",
      "Epoch 123 - avg_train_loss: 0.6171  avg_val_loss: 0.6279  time: 0s\n",
      "Epoch 123 - Score: 0.9250\n",
      "Epoch: [124][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5972(0.5972) Grad: 3768.4111  LR: 0.00016423  \n",
      "Epoch: [124][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5898(0.6179) Grad: 1252.8854  LR: 0.00016423  \n",
      "Epoch: [124][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6063(0.6063) Grad: 0.0000  \n",
      "Epoch: [124][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6537(0.6285) Grad: 0.0000  \n",
      "Epoch 124 - avg_train_loss: 0.6179  avg_val_loss: 0.6285  time: 0s\n",
      "Epoch 124 - Score: 0.9250\n",
      "Epoch: [125][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6138(0.6138) Grad: 6251.5425  LR: 0.00016423  \n",
      "Epoch: [125][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6170(0.6180) Grad: 5941.4175  LR: 0.00016423  \n",
      "Epoch: [125][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6058(0.6058) Grad: 0.0000  \n",
      "Epoch: [125][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6548(0.6287) Grad: 0.0000  \n",
      "Epoch 125 - avg_train_loss: 0.6180  avg_val_loss: 0.6287  time: 0s\n",
      "Epoch 125 - Score: 0.9250\n",
      "Epoch: [126][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6327(0.6327) Grad: 9104.6133  LR: 0.00016423  \n",
      "Epoch: [126][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6289(0.6182) Grad: 5504.7520  LR: 0.00014781  \n",
      "Epoch: [126][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [126][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6551(0.6290) Grad: 0.0000  \n",
      "Epoch 126 - avg_train_loss: 0.6182  avg_val_loss: 0.6290  time: 0s\n",
      "Epoch 126 - Score: 0.9250\n",
      "Epoch: [127][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5981(0.5981) Grad: 4584.6377  LR: 0.00014781  \n",
      "Epoch: [127][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5949(0.6223) Grad: 911.5557  LR: 0.00014781  \n",
      "Epoch: [127][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [127][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6558(0.6293) Grad: 0.0000  \n",
      "Epoch 127 - avg_train_loss: 0.6223  avg_val_loss: 0.6293  time: 0s\n",
      "Epoch 127 - Score: 0.9208\n",
      "Epoch: [128][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6445) Grad: 6745.8667  LR: 0.00014781  \n",
      "Epoch: [128][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6033(0.6215) Grad: 630.9836  LR: 0.00014781  \n",
      "Epoch: [128][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [128][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6571(0.6301) Grad: 0.0000  \n",
      "Epoch 128 - avg_train_loss: 0.6215  avg_val_loss: 0.6301  time: 0s\n",
      "Epoch 128 - Score: 0.9208\n",
      "Epoch: [129][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6050) Grad: 6490.8774  LR: 0.00014781  \n",
      "Epoch: [129][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6076(0.6217) Grad: 5083.1099  LR: 0.00013303  \n",
      "Epoch: [129][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [129][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6585(0.6305) Grad: 0.0000  \n",
      "Epoch 129 - avg_train_loss: 0.6217  avg_val_loss: 0.6305  time: 0s\n",
      "Epoch 129 - Score: 0.9208\n",
      "Epoch: [130][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5881(0.5881) Grad: 3113.2576  LR: 0.00013303  \n",
      "Epoch: [130][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6426(0.6145) Grad: 5224.9326  LR: 0.00013303  \n",
      "Epoch: [130][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [130][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6574(0.6300) Grad: 0.0000  \n",
      "Epoch 130 - avg_train_loss: 0.6145  avg_val_loss: 0.6300  time: 0s\n",
      "Epoch 130 - Score: 0.9208\n",
      "Epoch: [131][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6165(0.6165) Grad: 4982.8506  LR: 0.00013303  \n",
      "Epoch: [131][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6276(0.6178) Grad: 4440.4292  LR: 0.00013303  \n",
      "Epoch: [131][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6051(0.6051) Grad: 0.0000  \n",
      "Epoch: [131][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6555(0.6286) Grad: 0.0000  \n",
      "Epoch 131 - avg_train_loss: 0.6178  avg_val_loss: 0.6286  time: 0s\n",
      "Epoch 131 - Score: 0.9292\n",
      "Epoch: [132][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6257(0.6257) Grad: 6360.6030  LR: 0.00013303  \n",
      "Epoch: [132][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6617(0.6195) Grad: 3592.2588  LR: 0.00010775  \n",
      "Epoch: [132][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6044) Grad: 0.0000  \n",
      "Epoch: [132][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6553(0.6282) Grad: 0.0000  \n",
      "Epoch 132 - avg_train_loss: 0.6195  avg_val_loss: 0.6282  time: 0s\n",
      "Epoch 132 - Score: 0.9292\n",
      "Epoch: [133][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6305(0.6305) Grad: 4013.9731  LR: 0.00011973  \n",
      "Epoch: [133][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6155) Grad: 9072.3467  LR: 0.00011973  \n",
      "Epoch: [133][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6040(0.6040) Grad: 0.0000  \n",
      "Epoch: [133][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6553(0.6279) Grad: 0.0000  \n",
      "Epoch 133 - avg_train_loss: 0.6155  avg_val_loss: 0.6279  time: 0s\n",
      "Epoch 133 - Score: 0.9292\n",
      "Epoch: [134][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6313(0.6313) Grad: 3330.9558  LR: 0.00011973  \n",
      "Epoch: [134][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6325(0.6156) Grad: 6058.3369  LR: 0.00011973  \n",
      "Epoch: [134][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6048(0.6048) Grad: 0.0000  \n",
      "Epoch: [134][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6559(0.6286) Grad: 0.0000  \n",
      "Epoch 134 - avg_train_loss: 0.6156  avg_val_loss: 0.6286  time: 0s\n",
      "Epoch 134 - Score: 0.9292\n",
      "Epoch: [135][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6142(0.6142) Grad: 3992.8037  LR: 0.00011973  \n",
      "Epoch: [135][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6398(0.6188) Grad: 9613.2129  LR: 0.00011973  \n",
      "Epoch: [135][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6049(0.6049) Grad: 0.0000  \n",
      "Epoch: [135][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6564(0.6290) Grad: 0.0000  \n",
      "Epoch 135 - avg_train_loss: 0.6188  avg_val_loss: 0.6290  time: 0s\n",
      "Epoch 135 - Score: 0.9250\n",
      "Epoch: [136][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6425(0.6425) Grad: 1411.0806  LR: 0.00009698  \n",
      "Epoch: [136][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6014(0.6154) Grad: 5125.8584  LR: 0.00010775  \n",
      "Epoch: [136][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6050) Grad: 0.0000  \n",
      "Epoch: [136][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6551(0.6284) Grad: 0.0000  \n",
      "Epoch 136 - avg_train_loss: 0.6154  avg_val_loss: 0.6284  time: 0s\n",
      "Epoch 136 - Score: 0.9292\n",
      "Epoch: [137][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6116(0.6116) Grad: 8237.1230  LR: 0.00010775  \n",
      "Epoch: [137][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6373(0.6226) Grad: 12136.0068  LR: 0.00010775  \n",
      "Epoch: [137][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6053(0.6053) Grad: 0.0000  \n",
      "Epoch: [137][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6543(0.6282) Grad: 0.0000  \n",
      "Epoch 137 - avg_train_loss: 0.6226  avg_val_loss: 0.6282  time: 0s\n",
      "Epoch 137 - Score: 0.9292\n",
      "Epoch: [138][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6225(0.6225) Grad: 3523.2812  LR: 0.00010775  \n",
      "Epoch: [138][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5935(0.6144) Grad: 3634.7837  LR: 0.00010775  \n",
      "Epoch: [138][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6053(0.6053) Grad: 0.0000  \n",
      "Epoch: [138][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6278) Grad: 0.0000  \n",
      "Epoch 138 - avg_train_loss: 0.6144  avg_val_loss: 0.6278  time: 0s\n",
      "Epoch 138 - Score: 0.9292\n",
      "Epoch: [139][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5983(0.5983) Grad: 3310.2244  LR: 0.00010775  \n",
      "Epoch: [139][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6347(0.6150) Grad: 6590.4683  LR: 0.00009698  \n",
      "Epoch: [139][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [139][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6528(0.6280) Grad: 0.0000  \n",
      "Epoch 139 - avg_train_loss: 0.6150  avg_val_loss: 0.6280  time: 0s\n",
      "Epoch 139 - Score: 0.9250\n",
      "Epoch: [140][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6378(0.6378) Grad: 4761.9038  LR: 0.00009698  \n",
      "Epoch: [140][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6462(0.6187) Grad: 5355.9658  LR: 0.00009698  \n",
      "Epoch: [140][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [140][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6284) Grad: 0.0000  \n",
      "Epoch 140 - avg_train_loss: 0.6187  avg_val_loss: 0.6284  time: 0s\n",
      "Epoch 140 - Score: 0.9250\n",
      "Epoch: [141][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6355(0.6355) Grad: 3384.8074  LR: 0.00009698  \n",
      "Epoch: [141][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6129(0.6210) Grad: 4773.0435  LR: 0.00009698  \n",
      "Epoch: [141][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6069(0.6069) Grad: 0.0000  \n",
      "Epoch: [141][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6546(0.6292) Grad: 0.0000  \n",
      "Epoch 141 - avg_train_loss: 0.6210  avg_val_loss: 0.6292  time: 0s\n",
      "Epoch 141 - Score: 0.9250\n",
      "Epoch: [142][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6257(0.6257) Grad: 702.2841  LR: 0.00009698  \n",
      "Epoch: [142][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6206(0.6189) Grad: 3772.2212  LR: 0.00008728  \n",
      "Epoch: [142][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6070(0.6070) Grad: 0.0000  \n",
      "Epoch: [142][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6555(0.6297) Grad: 0.0000  \n",
      "Epoch 142 - avg_train_loss: 0.6189  avg_val_loss: 0.6297  time: 0s\n",
      "Epoch 142 - Score: 0.9208\n",
      "Epoch: [143][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6403(0.6403) Grad: 440.6059  LR: 0.00008728  \n",
      "Epoch: [143][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5766(0.6130) Grad: 3974.4966  LR: 0.00008728  \n",
      "Epoch: [143][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [143][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6564(0.6296) Grad: 0.0000  \n",
      "Epoch 143 - avg_train_loss: 0.6130  avg_val_loss: 0.6296  time: 0s\n",
      "Epoch 143 - Score: 0.9208\n",
      "Epoch: [144][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6095) Grad: 2282.5938  LR: 0.00008728  \n",
      "Epoch: [144][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5882(0.6121) Grad: 1751.4609  LR: 0.00008728  \n",
      "Epoch: [144][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6059(0.6059) Grad: 0.0000  \n",
      "Epoch: [144][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6566(0.6295) Grad: 0.0000  \n",
      "Epoch 144 - avg_train_loss: 0.6121  avg_val_loss: 0.6295  time: 0s\n",
      "Epoch 144 - Score: 0.9208\n",
      "Epoch: [145][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6214(0.6214) Grad: 8788.7783  LR: 0.00008728  \n",
      "Epoch: [145][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6136(0.6203) Grad: 8352.9658  LR: 0.00007855  \n",
      "Epoch: [145][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [145][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6569(0.6298) Grad: 0.0000  \n",
      "Epoch 145 - avg_train_loss: 0.6203  avg_val_loss: 0.6298  time: 0s\n",
      "Epoch 145 - Score: 0.9208\n",
      "Epoch: [146][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5892(0.5892) Grad: 4603.1250  LR: 0.00007855  \n",
      "Epoch: [146][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5784(0.6137) Grad: 7055.1777  LR: 0.00007855  \n",
      "Epoch: [146][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [146][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6571(0.6298) Grad: 0.0000  \n",
      "Epoch 146 - avg_train_loss: 0.6137  avg_val_loss: 0.6298  time: 0s\n",
      "Epoch 146 - Score: 0.9208\n",
      "Epoch: [147][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6161(0.6161) Grad: 10471.7373  LR: 0.00007855  \n",
      "Epoch: [147][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6163(0.6222) Grad: 6196.4624  LR: 0.00007855  \n",
      "Epoch: [147][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [147][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6565(0.6296) Grad: 0.0000  \n",
      "Epoch 147 - avg_train_loss: 0.6222  avg_val_loss: 0.6296  time: 0s\n",
      "Epoch 147 - Score: 0.9208\n",
      "Epoch: [148][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6200) Grad: 4051.4585  LR: 0.00007855  \n",
      "Epoch: [148][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6336(0.6137) Grad: 5373.5889  LR: 0.00007070  \n",
      "Epoch: [148][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6058(0.6058) Grad: 0.0000  \n",
      "Epoch: [148][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6570(0.6297) Grad: 0.0000  \n",
      "Epoch 148 - avg_train_loss: 0.6137  avg_val_loss: 0.6297  time: 0s\n",
      "Epoch 148 - Score: 0.9208\n",
      "Epoch: [149][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6081(0.6081) Grad: 4223.3564  LR: 0.00007070  \n",
      "Epoch: [149][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6198(0.6184) Grad: 4226.4888  LR: 0.00007070  \n",
      "Epoch: [149][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [149][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6575(0.6301) Grad: 0.0000  \n",
      "Epoch 149 - avg_train_loss: 0.6184  avg_val_loss: 0.6301  time: 0s\n",
      "Epoch 149 - Score: 0.9208\n",
      "Epoch: [150][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6310(0.6310) Grad: 2956.7319  LR: 0.00007070  \n",
      "Epoch: [150][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5811(0.6153) Grad: 1801.5240  LR: 0.00007070  \n",
      "Epoch: [150][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6058(0.6058) Grad: 0.0000  \n",
      "Epoch: [150][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6581(0.6302) Grad: 0.0000  \n",
      "Epoch 150 - avg_train_loss: 0.6153  avg_val_loss: 0.6302  time: 0s\n",
      "Epoch 150 - Score: 0.9250\n",
      "Epoch: [151][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6387(0.6387) Grad: 6320.8296  LR: 0.00007070  \n",
      "Epoch: [151][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6350(0.6155) Grad: 3131.2410  LR: 0.00006363  \n",
      "Epoch: [151][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [151][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6582(0.6303) Grad: 0.0000  \n",
      "Epoch 151 - avg_train_loss: 0.6155  avg_val_loss: 0.6303  time: 0s\n",
      "Epoch 151 - Score: 0.9208\n",
      "Epoch: [152][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6243(0.6243) Grad: 7679.0361  LR: 0.00006363  \n",
      "Epoch: [152][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6118(0.6142) Grad: 2450.1787  LR: 0.00006363  \n",
      "Epoch: [152][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6062(0.6062) Grad: 0.0000  \n",
      "Epoch: [152][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6579(0.6303) Grad: 0.0000  \n",
      "Epoch 152 - avg_train_loss: 0.6142  avg_val_loss: 0.6303  time: 0s\n",
      "Epoch 152 - Score: 0.9208\n",
      "Epoch: [153][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6517(0.6517) Grad: 3712.1257  LR: 0.00006363  \n",
      "Epoch: [153][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6115(0.6199) Grad: 4663.2754  LR: 0.00006363  \n",
      "Epoch: [153][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [153][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6573(0.6301) Grad: 0.0000  \n",
      "Epoch 153 - avg_train_loss: 0.6199  avg_val_loss: 0.6301  time: 0s\n",
      "Epoch 153 - Score: 0.9208\n",
      "Epoch: [154][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 4761.5181  LR: 0.00006363  \n",
      "Epoch: [154][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6065(0.6198) Grad: 4026.4302  LR: 0.00005154  \n",
      "Epoch: [154][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6065(0.6065) Grad: 0.0000  \n",
      "Epoch: [154][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6576(0.6304) Grad: 0.0000  \n",
      "Epoch 154 - avg_train_loss: 0.6198  avg_val_loss: 0.6304  time: 0s\n",
      "Epoch 154 - Score: 0.9208\n",
      "Epoch: [155][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6131(0.6131) Grad: 3849.3848  LR: 0.00005726  \n",
      "Epoch: [155][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6562(0.6191) Grad: 4625.2412  LR: 0.00005726  \n",
      "Epoch: [155][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [155][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6577(0.6304) Grad: 0.0000  \n",
      "Epoch 155 - avg_train_loss: 0.6191  avg_val_loss: 0.6304  time: 0s\n",
      "Epoch 155 - Score: 0.9208\n",
      "Epoch: [156][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6057(0.6057) Grad: 4534.4004  LR: 0.00005726  \n",
      "Epoch: [156][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6255(0.6191) Grad: 6941.8843  LR: 0.00005726  \n",
      "Epoch: [156][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [156][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6573(0.6302) Grad: 0.0000  \n",
      "Epoch 156 - avg_train_loss: 0.6191  avg_val_loss: 0.6302  time: 0s\n",
      "Epoch 156 - Score: 0.9208\n",
      "Epoch: [157][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 5046.5996  LR: 0.00005726  \n",
      "Epoch: [157][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5998(0.6123) Grad: 6262.8262  LR: 0.00005726  \n",
      "Epoch: [157][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6067(0.6067) Grad: 0.0000  \n",
      "Epoch: [157][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6580(0.6306) Grad: 0.0000  \n",
      "Epoch 157 - avg_train_loss: 0.6123  avg_val_loss: 0.6306  time: 0s\n",
      "Epoch 157 - Score: 0.9208\n",
      "Epoch: [158][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6226(0.6226) Grad: 2970.3523  LR: 0.00004638  \n",
      "Epoch: [158][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6155(0.6148) Grad: 8799.4443  LR: 0.00005154  \n",
      "Epoch: [158][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6069(0.6069) Grad: 0.0000  \n",
      "Epoch: [158][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6583(0.6308) Grad: 0.0000  \n",
      "Epoch 158 - avg_train_loss: 0.6148  avg_val_loss: 0.6308  time: 0s\n",
      "Epoch 158 - Score: 0.9208\n",
      "Epoch: [159][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6033(0.6033) Grad: 2283.3179  LR: 0.00005154  \n",
      "Epoch: [159][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6257(0.6191) Grad: 971.0233  LR: 0.00005154  \n",
      "Epoch: [159][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6073(0.6073) Grad: 0.0000  \n",
      "Epoch: [159][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6581(0.6310) Grad: 0.0000  \n",
      "Epoch 159 - avg_train_loss: 0.6191  avg_val_loss: 0.6310  time: 0s\n",
      "Epoch 159 - Score: 0.9208\n",
      "Epoch: [160][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6008(0.6008) Grad: 6562.2949  LR: 0.00005154  \n",
      "Epoch: [160][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6563(0.6165) Grad: 4872.3550  LR: 0.00005154  \n",
      "Epoch: [160][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6075(0.6075) Grad: 0.0000  \n",
      "Epoch: [160][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6579(0.6310) Grad: 0.0000  \n",
      "Epoch 160 - avg_train_loss: 0.6165  avg_val_loss: 0.6310  time: 0s\n",
      "Epoch 160 - Score: 0.9208\n",
      "Epoch: [161][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6266(0.6266) Grad: 3981.5601  LR: 0.00005154  \n",
      "Epoch: [161][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6516(0.6215) Grad: 6558.2324  LR: 0.00004638  \n",
      "Epoch: [161][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6074(0.6074) Grad: 0.0000  \n",
      "Epoch: [161][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6582(0.6311) Grad: 0.0000  \n",
      "Epoch 161 - avg_train_loss: 0.6215  avg_val_loss: 0.6311  time: 0s\n",
      "Epoch 161 - Score: 0.9208\n",
      "Epoch: [162][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6030(0.6030) Grad: 574.8633  LR: 0.00004638  \n",
      "Epoch: [162][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6331(0.6182) Grad: 7797.1289  LR: 0.00004638  \n",
      "Epoch: [162][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6075(0.6075) Grad: 0.0000  \n",
      "Epoch: [162][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6578(0.6310) Grad: 0.0000  \n",
      "Epoch 162 - avg_train_loss: 0.6182  avg_val_loss: 0.6310  time: 0s\n",
      "Epoch 162 - Score: 0.9208\n",
      "Epoch: [163][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6265(0.6265) Grad: 11271.0664  LR: 0.00004638  \n",
      "Epoch: [163][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5821(0.6182) Grad: 3260.2839  LR: 0.00004638  \n",
      "Epoch: [163][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6067(0.6067) Grad: 0.0000  \n",
      "Epoch: [163][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6581(0.6307) Grad: 0.0000  \n",
      "Epoch 163 - avg_train_loss: 0.6182  avg_val_loss: 0.6307  time: 0s\n",
      "Epoch 163 - Score: 0.9208\n",
      "Epoch: [164][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6364(0.6364) Grad: 3788.1323  LR: 0.00004638  \n",
      "Epoch: [164][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6202(0.6164) Grad: 7335.6191  LR: 0.00004175  \n",
      "Epoch: [164][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6069(0.6069) Grad: 0.0000  \n",
      "Epoch: [164][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6577(0.6306) Grad: 0.0000  \n",
      "Epoch 164 - avg_train_loss: 0.6164  avg_val_loss: 0.6306  time: 0s\n",
      "Epoch 164 - Score: 0.9208\n",
      "Epoch: [165][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6073(0.6073) Grad: 2866.4561  LR: 0.00004175  \n",
      "Epoch: [165][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6173) Grad: 1817.0763  LR: 0.00004175  \n",
      "Epoch: [165][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6062(0.6062) Grad: 0.0000  \n",
      "Epoch: [165][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6578(0.6303) Grad: 0.0000  \n",
      "Epoch 165 - avg_train_loss: 0.6173  avg_val_loss: 0.6303  time: 0s\n",
      "Epoch 165 - Score: 0.9208\n",
      "Epoch: [166][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6081(0.6081) Grad: 6729.8691  LR: 0.00004175  \n",
      "Epoch: [166][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6071(0.6162) Grad: 4197.2983  LR: 0.00004175  \n",
      "Epoch: [166][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [166][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6579(0.6303) Grad: 0.0000  \n",
      "Epoch 166 - avg_train_loss: 0.6162  avg_val_loss: 0.6303  time: 0s\n",
      "Epoch 166 - Score: 0.9208\n",
      "Epoch: [167][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6123(0.6123) Grad: 3243.5256  LR: 0.00004175  \n",
      "Epoch: [167][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6152) Grad: 7113.5005  LR: 0.00003757  \n",
      "Epoch: [167][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6066) Grad: 0.0000  \n",
      "Epoch: [167][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6586(0.6309) Grad: 0.0000  \n",
      "Epoch 167 - avg_train_loss: 0.6152  avg_val_loss: 0.6309  time: 0s\n",
      "Epoch 167 - Score: 0.9208\n",
      "Epoch: [168][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6050) Grad: 5610.0933  LR: 0.00003757  \n",
      "Epoch: [168][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6147) Grad: 536.4374  LR: 0.00003757  \n",
      "Epoch: [168][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [168][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6587(0.6308) Grad: 0.0000  \n",
      "Epoch 168 - avg_train_loss: 0.6147  avg_val_loss: 0.6308  time: 0s\n",
      "Epoch 168 - Score: 0.9208\n",
      "Epoch: [169][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5827(0.5827) Grad: 3998.0981  LR: 0.00003757  \n",
      "Epoch: [169][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6396(0.6170) Grad: 6313.5513  LR: 0.00003757  \n",
      "Epoch: [169][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6066) Grad: 0.0000  \n",
      "Epoch: [169][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6590(0.6311) Grad: 0.0000  \n",
      "Epoch 169 - avg_train_loss: 0.6170  avg_val_loss: 0.6311  time: 0s\n",
      "Epoch 169 - Score: 0.9208\n",
      "Epoch: [170][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6217(0.6217) Grad: 4350.4351  LR: 0.00003757  \n",
      "Epoch: [170][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6041(0.6140) Grad: 3085.7344  LR: 0.00003381  \n",
      "Epoch: [170][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6070(0.6070) Grad: 0.0000  \n",
      "Epoch: [170][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6598(0.6316) Grad: 0.0000  \n",
      "Epoch 170 - avg_train_loss: 0.6140  avg_val_loss: 0.6316  time: 0s\n",
      "Epoch 170 - Score: 0.9208\n",
      "Epoch: [171][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6140(0.6140) Grad: 4035.7100  LR: 0.00003381  \n",
      "Epoch: [171][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6137) Grad: 1048.3864  LR: 0.00003381  \n",
      "Epoch: [171][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6072(0.6072) Grad: 0.0000  \n",
      "Epoch: [171][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6601(0.6319) Grad: 0.0000  \n",
      "Epoch 171 - avg_train_loss: 0.6137  avg_val_loss: 0.6319  time: 0s\n",
      "Epoch 171 - Score: 0.9208\n",
      "Epoch: [172][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6033(0.6033) Grad: 3685.6348  LR: 0.00003381  \n",
      "Epoch: [172][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6219(0.6138) Grad: 7543.0410  LR: 0.00003381  \n",
      "Epoch: [172][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6072(0.6072) Grad: 0.0000  \n",
      "Epoch: [172][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6599(0.6318) Grad: 0.0000  \n",
      "Epoch 172 - avg_train_loss: 0.6138  avg_val_loss: 0.6318  time: 0s\n",
      "Epoch 172 - Score: 0.9208\n",
      "Epoch: [173][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6070(0.6070) Grad: 3740.4744  LR: 0.00003381  \n",
      "Epoch: [173][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5819(0.6180) Grad: 2463.8091  LR: 0.00003043  \n",
      "Epoch: [173][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6071(0.6071) Grad: 0.0000  \n",
      "Epoch: [173][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6601(0.6318) Grad: 0.0000  \n",
      "Epoch 173 - avg_train_loss: 0.6180  avg_val_loss: 0.6318  time: 0s\n",
      "Epoch 173 - Score: 0.9208\n",
      "Epoch: [174][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6187) Grad: 1280.8403  LR: 0.00003043  \n",
      "Epoch: [174][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6159) Grad: 4746.7739  LR: 0.00003043  \n",
      "Epoch: [174][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6069(0.6069) Grad: 0.0000  \n",
      "Epoch: [174][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6600(0.6316) Grad: 0.0000  \n",
      "Epoch 174 - avg_train_loss: 0.6159  avg_val_loss: 0.6316  time: 0s\n",
      "Epoch 174 - Score: 0.9208\n",
      "Epoch: [175][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6079(0.6079) Grad: 6810.1802  LR: 0.00003043  \n",
      "Epoch: [175][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6020(0.6177) Grad: 13862.2197  LR: 0.00003043  \n",
      "Epoch: [175][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6070(0.6070) Grad: 0.0000  \n",
      "Epoch: [175][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6594(0.6315) Grad: 0.0000  \n",
      "Epoch 175 - avg_train_loss: 0.6177  avg_val_loss: 0.6315  time: 0s\n",
      "Epoch 175 - Score: 0.9208\n",
      "Epoch: [176][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6125(0.6125) Grad: 1555.4397  LR: 0.00003043  \n",
      "Epoch: [176][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6143(0.6186) Grad: 6806.0635  LR: 0.00002465  \n",
      "Epoch: [176][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6072(0.6072) Grad: 0.0000  \n",
      "Epoch: [176][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6590(0.6314) Grad: 0.0000  \n",
      "Epoch 176 - avg_train_loss: 0.6186  avg_val_loss: 0.6314  time: 0s\n",
      "Epoch 176 - Score: 0.9208\n",
      "Epoch: [177][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6462(0.6462) Grad: 13890.8389  LR: 0.00002739  \n",
      "Epoch: [177][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6078(0.6146) Grad: 2123.8823  LR: 0.00002739  \n",
      "Epoch: [177][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6073(0.6073) Grad: 0.0000  \n",
      "Epoch: [177][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6586(0.6312) Grad: 0.0000  \n",
      "Epoch 177 - avg_train_loss: 0.6146  avg_val_loss: 0.6312  time: 0s\n",
      "Epoch 177 - Score: 0.9208\n",
      "Epoch: [178][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6166(0.6166) Grad: 3574.9314  LR: 0.00002739  \n",
      "Epoch: [178][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6115(0.6151) Grad: 3012.8845  LR: 0.00002739  \n",
      "Epoch: [178][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6071(0.6071) Grad: 0.0000  \n",
      "Epoch: [178][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6586(0.6311) Grad: 0.0000  \n",
      "Epoch 178 - avg_train_loss: 0.6151  avg_val_loss: 0.6311  time: 0s\n",
      "Epoch 178 - Score: 0.9208\n",
      "Epoch: [179][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6291) Grad: 5742.2070  LR: 0.00002739  \n",
      "Epoch: [179][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6366(0.6202) Grad: 4565.6040  LR: 0.00002739  \n",
      "Epoch: [179][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6079(0.6079) Grad: 0.0000  \n",
      "Epoch: [179][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6594(0.6320) Grad: 0.0000  \n",
      "Epoch 179 - avg_train_loss: 0.6202  avg_val_loss: 0.6320  time: 0s\n",
      "Epoch 179 - Score: 0.9208\n",
      "Epoch: [180][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6326(0.6326) Grad: 942.5331  LR: 0.00002219  \n",
      "Epoch: [180][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6166) Grad: 5477.2168  LR: 0.00002465  \n",
      "Epoch: [180][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6081(0.6081) Grad: 0.0000  \n",
      "Epoch: [180][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6596(0.6321) Grad: 0.0000  \n",
      "Epoch 180 - avg_train_loss: 0.6166  avg_val_loss: 0.6321  time: 0s\n",
      "Epoch 180 - Score: 0.9208\n",
      "Epoch: [181][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6308(0.6308) Grad: 4775.2764  LR: 0.00002465  \n",
      "Epoch: [181][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6036(0.6187) Grad: 4389.2080  LR: 0.00002465  \n",
      "Epoch: [181][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6077(0.6077) Grad: 0.0000  \n",
      "Epoch: [181][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6598(0.6320) Grad: 0.0000  \n",
      "Epoch 181 - avg_train_loss: 0.6187  avg_val_loss: 0.6320  time: 0s\n",
      "Epoch 181 - Score: 0.9208\n",
      "Epoch: [182][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5984(0.5984) Grad: 5326.2207  LR: 0.00002465  \n",
      "Epoch: [182][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6278(0.6144) Grad: 3906.0840  LR: 0.00002465  \n",
      "Epoch: [182][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6071(0.6071) Grad: 0.0000  \n",
      "Epoch: [182][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6597(0.6317) Grad: 0.0000  \n",
      "Epoch 182 - avg_train_loss: 0.6144  avg_val_loss: 0.6317  time: 0s\n",
      "Epoch 182 - Score: 0.9208\n",
      "Epoch: [183][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5896(0.5896) Grad: 1310.0925  LR: 0.00002465  \n",
      "Epoch: [183][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5956(0.6147) Grad: 3745.5139  LR: 0.00002219  \n",
      "Epoch: [183][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6067(0.6067) Grad: 0.0000  \n",
      "Epoch: [183][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6598(0.6315) Grad: 0.0000  \n",
      "Epoch 183 - avg_train_loss: 0.6147  avg_val_loss: 0.6315  time: 0s\n",
      "Epoch 183 - Score: 0.9208\n",
      "Epoch: [184][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5810(0.5810) Grad: 1936.6970  LR: 0.00002219  \n",
      "Epoch: [184][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6137(0.6175) Grad: 4672.9937  LR: 0.00002219  \n",
      "Epoch: [184][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6063(0.6063) Grad: 0.0000  \n",
      "Epoch: [184][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6311) Grad: 0.0000  \n",
      "Epoch 184 - avg_train_loss: 0.6175  avg_val_loss: 0.6311  time: 0s\n",
      "Epoch 184 - Score: 0.9208\n",
      "Epoch: [185][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6212(0.6212) Grad: 1826.5002  LR: 0.00002219  \n",
      "Epoch: [185][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6271(0.6186) Grad: 5909.4604  LR: 0.00002219  \n",
      "Epoch: [185][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [185][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6312) Grad: 0.0000  \n",
      "Epoch 185 - avg_train_loss: 0.6186  avg_val_loss: 0.6312  time: 0s\n",
      "Epoch 185 - Score: 0.9250\n",
      "Epoch: [186][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6048(0.6048) Grad: 8032.4258  LR: 0.00002219  \n",
      "Epoch: [186][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6055(0.6200) Grad: 6492.0356  LR: 0.00001997  \n",
      "Epoch: [186][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [186][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6591(0.6310) Grad: 0.0000  \n",
      "Epoch 186 - avg_train_loss: 0.6200  avg_val_loss: 0.6310  time: 0s\n",
      "Epoch 186 - Score: 0.9208\n",
      "Epoch: [187][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6196(0.6196) Grad: 5815.0181  LR: 0.00001997  \n",
      "Epoch: [187][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6278(0.6162) Grad: 3508.0703  LR: 0.00001997  \n",
      "Epoch: [187][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6065(0.6065) Grad: 0.0000  \n",
      "Epoch: [187][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6591(0.6310) Grad: 0.0000  \n",
      "Epoch 187 - avg_train_loss: 0.6162  avg_val_loss: 0.6310  time: 0s\n",
      "Epoch 187 - Score: 0.9208\n",
      "Epoch: [188][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6292(0.6292) Grad: 3731.2747  LR: 0.00001997  \n",
      "Epoch: [188][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6119) Grad: 3221.7810  LR: 0.00001997  \n",
      "Epoch: [188][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6063(0.6063) Grad: 0.0000  \n",
      "Epoch: [188][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6589(0.6309) Grad: 0.0000  \n",
      "Epoch 188 - avg_train_loss: 0.6119  avg_val_loss: 0.6309  time: 0s\n",
      "Epoch 188 - Score: 0.9208\n",
      "Epoch: [189][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6148(0.6148) Grad: 4603.6948  LR: 0.00001997  \n",
      "Epoch: [189][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5930(0.6181) Grad: 3835.8164  LR: 0.00001797  \n",
      "Epoch: [189][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [189][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6589(0.6307) Grad: 0.0000  \n",
      "Epoch 189 - avg_train_loss: 0.6181  avg_val_loss: 0.6307  time: 0s\n",
      "Epoch 189 - Score: 0.9250\n",
      "Epoch: [190][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5864(0.5864) Grad: 862.3342  LR: 0.00001797  \n",
      "Epoch: [190][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6542(0.6163) Grad: 4875.1743  LR: 0.00001797  \n",
      "Epoch: [190][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [190][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6590(0.6308) Grad: 0.0000  \n",
      "Epoch 190 - avg_train_loss: 0.6163  avg_val_loss: 0.6308  time: 0s\n",
      "Epoch 190 - Score: 0.9250\n",
      "Epoch: [191][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6284(0.6284) Grad: 5972.4624  LR: 0.00001797  \n",
      "Epoch: [191][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6317(0.6143) Grad: 7010.6553  LR: 0.00001797  \n",
      "Epoch: [191][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [191][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6592(0.6310) Grad: 0.0000  \n",
      "Epoch 191 - avg_train_loss: 0.6143  avg_val_loss: 0.6310  time: 0s\n",
      "Epoch 191 - Score: 0.9208\n",
      "Epoch: [192][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5879(0.5879) Grad: 2049.2551  LR: 0.00001797  \n",
      "Epoch: [192][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6241(0.6166) Grad: 4913.5518  LR: 0.00001617  \n",
      "Epoch: [192][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [192][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6312) Grad: 0.0000  \n",
      "Epoch 192 - avg_train_loss: 0.6166  avg_val_loss: 0.6312  time: 0s\n",
      "Epoch 192 - Score: 0.9208\n",
      "Epoch: [193][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6345(0.6345) Grad: 6061.9067  LR: 0.00001617  \n",
      "Epoch: [193][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5920(0.6144) Grad: 6193.9722  LR: 0.00001617  \n",
      "Epoch: [193][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6066) Grad: 0.0000  \n",
      "Epoch: [193][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6598(0.6314) Grad: 0.0000  \n",
      "Epoch 193 - avg_train_loss: 0.6144  avg_val_loss: 0.6314  time: 0s\n",
      "Epoch 193 - Score: 0.9208\n",
      "Epoch: [194][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6078(0.6078) Grad: 6291.0220  LR: 0.00001617  \n",
      "Epoch: [194][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6109(0.6169) Grad: 2970.6335  LR: 0.00001617  \n",
      "Epoch: [194][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6066) Grad: 0.0000  \n",
      "Epoch: [194][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6597(0.6314) Grad: 0.0000  \n",
      "Epoch 194 - avg_train_loss: 0.6169  avg_val_loss: 0.6314  time: 0s\n",
      "Epoch 194 - Score: 0.9208\n",
      "Epoch: [195][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5826(0.5826) Grad: 2679.2261  LR: 0.00001617  \n",
      "Epoch: [195][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6199(0.6168) Grad: 2593.8857  LR: 0.00001456  \n",
      "Epoch: [195][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6063(0.6063) Grad: 0.0000  \n",
      "Epoch: [195][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6311) Grad: 0.0000  \n",
      "Epoch 195 - avg_train_loss: 0.6168  avg_val_loss: 0.6311  time: 0s\n",
      "Epoch 195 - Score: 0.9208\n",
      "Epoch: [196][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6590(0.6590) Grad: 7115.7729  LR: 0.00001456  \n",
      "Epoch: [196][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5993(0.6177) Grad: 10524.8359  LR: 0.00001456  \n",
      "Epoch: [196][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6059(0.6059) Grad: 0.0000  \n",
      "Epoch: [196][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6593(0.6308) Grad: 0.0000  \n",
      "Epoch 196 - avg_train_loss: 0.6177  avg_val_loss: 0.6308  time: 0s\n",
      "Epoch 196 - Score: 0.9250\n",
      "Epoch: [197][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5798(0.5798) Grad: 6582.6875  LR: 0.00001456  \n",
      "Epoch: [197][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6043(0.6172) Grad: 5719.6572  LR: 0.00001456  \n",
      "Epoch: [197][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [197][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6310) Grad: 0.0000  \n",
      "Epoch 197 - avg_train_loss: 0.6172  avg_val_loss: 0.6310  time: 0s\n",
      "Epoch 197 - Score: 0.9250\n",
      "Epoch: [198][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6173(0.6173) Grad: 1643.2679  LR: 0.00001456  \n",
      "Epoch: [198][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5845(0.6116) Grad: 5780.4106  LR: 0.00001179  \n",
      "Epoch: [198][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6063(0.6063) Grad: 0.0000  \n",
      "Epoch: [198][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6311) Grad: 0.0000  \n",
      "Epoch 198 - avg_train_loss: 0.6116  avg_val_loss: 0.6311  time: 0s\n",
      "Epoch 198 - Score: 0.9208\n",
      "Epoch: [199][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6276(0.6276) Grad: 8417.8643  LR: 0.00001310  \n",
      "Epoch: [199][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6198(0.6155) Grad: 1059.4242  LR: 0.00001310  \n",
      "Epoch: [199][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6066) Grad: 0.0000  \n",
      "Epoch: [199][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6592(0.6312) Grad: 0.0000  \n",
      "Epoch 199 - avg_train_loss: 0.6155  avg_val_loss: 0.6312  time: 0s\n",
      "Epoch 199 - Score: 0.9208\n",
      "Epoch: [200][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6295(0.6295) Grad: 7790.8975  LR: 0.00001310  \n",
      "Epoch: [200][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6161) Grad: 2463.5986  LR: 0.00001310  \n",
      "Epoch: [200][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6066) Grad: 0.0000  \n",
      "Epoch: [200][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6593(0.6312) Grad: 0.0000  \n",
      "Epoch 200 - avg_train_loss: 0.6161  avg_val_loss: 0.6312  time: 0s\n",
      "Epoch 200 - Score: 0.9208\n",
      "========== fold: 1 result ==========\n",
      "Score: 0.9333\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 1.0903(1.0903) Grad: 175920.7969  LR: 0.01000000  \n",
      "Epoch: [1][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9042(0.9390) Grad: 15812.9600  LR: 0.01000000  \n",
      "Epoch: [1][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4653(1.4653) Grad: 0.0000  \n",
      "Epoch: [1][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4888(1.4763) Grad: 0.0000  \n",
      "Epoch 1 - avg_train_loss: 0.9390  avg_val_loss: 1.4763  time: 0s\n",
      "Epoch 1 - Score: 0.0750\n",
      "Epoch 1 - Save Best Score: 0.0750 Model\n",
      "Epoch: [2][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8786(0.8786) Grad: 15538.4219  LR: 0.01000000  \n",
      "Epoch: [2][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8193(0.8622) Grad: 12960.6562  LR: 0.01000000  \n",
      "Epoch: [2][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4379(1.4379) Grad: 0.0000  \n",
      "Epoch: [2][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4599(1.4481) Grad: 0.0000  \n",
      "Epoch 2 - avg_train_loss: 0.8622  avg_val_loss: 1.4481  time: 0s\n",
      "Epoch 2 - Score: 0.0792\n",
      "Epoch 2 - Save Best Score: 0.0792 Model\n",
      "Epoch: [3][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8844(0.8844) Grad: 14817.7744  LR: 0.01000000  \n",
      "Epoch: [3][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8436(0.8317) Grad: 14872.4238  LR: 0.01000000  \n",
      "Epoch: [3][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.1466(1.1466) Grad: 0.0000  \n",
      "Epoch: [3][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.1251(1.1366) Grad: 0.0000  \n",
      "Epoch 3 - avg_train_loss: 0.8317  avg_val_loss: 1.1366  time: 0s\n",
      "Epoch 3 - Score: 0.3583\n",
      "Epoch 3 - Save Best Score: 0.3583 Model\n",
      "Epoch: [4][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7842(0.7842) Grad: 12237.9707  LR: 0.00810000  \n",
      "Epoch: [4][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7698(0.7946) Grad: 11151.8799  LR: 0.00900000  \n",
      "Epoch: [4][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8556(0.8556) Grad: 0.0000  \n",
      "Epoch: [4][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8213(0.8396) Grad: 0.0000  \n",
      "Epoch 4 - avg_train_loss: 0.7946  avg_val_loss: 0.8396  time: 0s\n",
      "Epoch 4 - Score: 0.8208\n",
      "Epoch 4 - Save Best Score: 0.8208 Model\n",
      "Epoch: [5][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7977(0.7977) Grad: 10388.0674  LR: 0.00900000  \n",
      "Epoch: [5][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7330(0.7763) Grad: 10111.1416  LR: 0.00900000  \n",
      "Epoch: [5][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8624(0.8624) Grad: 0.0000  \n",
      "Epoch: [5][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8320(0.8482) Grad: 0.0000  \n",
      "Epoch 5 - avg_train_loss: 0.7763  avg_val_loss: 0.8482  time: 0s\n",
      "Epoch 5 - Score: 0.8708\n",
      "Epoch 5 - Save Best Score: 0.8708 Model\n",
      "Epoch: [6][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7618(0.7618) Grad: 8583.7529  LR: 0.00900000  \n",
      "Epoch: [6][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7398(0.7552) Grad: 9123.9248  LR: 0.00900000  \n",
      "Epoch: [6][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9040(0.9040) Grad: 0.0000  \n",
      "Epoch: [6][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8703(0.8882) Grad: 0.0000  \n",
      "Epoch 6 - avg_train_loss: 0.7552  avg_val_loss: 0.8882  time: 0s\n",
      "Epoch 6 - Score: 0.8708\n",
      "Epoch: [7][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7258(0.7258) Grad: 11354.2588  LR: 0.00900000  \n",
      "Epoch: [7][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7032(0.7340) Grad: 8684.1582  LR: 0.00810000  \n",
      "Epoch: [7][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8144(0.8144) Grad: 0.0000  \n",
      "Epoch: [7][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7744(0.7958) Grad: 0.0000  \n",
      "Epoch 7 - avg_train_loss: 0.7340  avg_val_loss: 0.7958  time: 0s\n",
      "Epoch 7 - Score: 0.8292\n",
      "Epoch: [8][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7041(0.7041) Grad: 8330.1631  LR: 0.00810000  \n",
      "Epoch: [8][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7357(0.7253) Grad: 9796.0479  LR: 0.00810000  \n",
      "Epoch: [8][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8981(0.8981) Grad: 0.0000  \n",
      "Epoch: [8][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8554(0.8782) Grad: 0.0000  \n",
      "Epoch 8 - avg_train_loss: 0.7253  avg_val_loss: 0.8782  time: 0s\n",
      "Epoch 8 - Score: 0.8583\n",
      "Epoch: [9][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7396(0.7396) Grad: 6843.1743  LR: 0.00810000  \n",
      "Epoch: [9][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7083(0.7096) Grad: 6617.3438  LR: 0.00810000  \n",
      "Epoch: [9][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8146(0.8146) Grad: 0.0000  \n",
      "Epoch: [9][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7699(0.7938) Grad: 0.0000  \n",
      "Epoch 9 - avg_train_loss: 0.7096  avg_val_loss: 0.7938  time: 0s\n",
      "Epoch 9 - Score: 0.8333\n",
      "Epoch: [10][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7055(0.7055) Grad: 5963.8018  LR: 0.00810000  \n",
      "Epoch: [10][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6776(0.6993) Grad: 5589.9316  LR: 0.00729000  \n",
      "Epoch: [10][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8049(0.8049) Grad: 0.0000  \n",
      "Epoch: [10][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7588(0.7834) Grad: 0.0000  \n",
      "Epoch 10 - avg_train_loss: 0.6993  avg_val_loss: 0.7834  time: 0s\n",
      "Epoch 10 - Score: 0.8167\n",
      "Epoch: [11][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6912(0.6912) Grad: 5343.4878  LR: 0.00729000  \n",
      "Epoch: [11][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6736(0.6842) Grad: 5513.8633  LR: 0.00729000  \n",
      "Epoch: [11][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7961(0.7961) Grad: 0.0000  \n",
      "Epoch: [11][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7461(0.7728) Grad: 0.0000  \n",
      "Epoch 11 - avg_train_loss: 0.6842  avg_val_loss: 0.7728  time: 0s\n",
      "Epoch 11 - Score: 0.8250\n",
      "Epoch: [12][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6690(0.6690) Grad: 6142.0610  LR: 0.00729000  \n",
      "Epoch: [12][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6733(0.6832) Grad: 4255.3672  LR: 0.00729000  \n",
      "Epoch: [12][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7720(0.7720) Grad: 0.0000  \n",
      "Epoch: [12][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7405(0.7573) Grad: 0.0000  \n",
      "Epoch 12 - avg_train_loss: 0.6832  avg_val_loss: 0.7573  time: 0s\n",
      "Epoch 12 - Score: 0.8708\n",
      "Epoch: [13][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6984(0.6984) Grad: 5574.0103  LR: 0.00729000  \n",
      "Epoch: [13][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6584(0.6751) Grad: 3084.8113  LR: 0.00656100  \n",
      "Epoch: [13][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7725(0.7725) Grad: 0.0000  \n",
      "Epoch: [13][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7367(0.7558) Grad: 0.0000  \n",
      "Epoch 13 - avg_train_loss: 0.6751  avg_val_loss: 0.7558  time: 0s\n",
      "Epoch 13 - Score: 0.8708\n",
      "Epoch: [14][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6535(0.6535) Grad: 3486.4590  LR: 0.00656100  \n",
      "Epoch: [14][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6689(0.6645) Grad: 4283.0015  LR: 0.00656100  \n",
      "Epoch: [14][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7529(0.7529) Grad: 0.0000  \n",
      "Epoch: [14][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7271(0.7409) Grad: 0.0000  \n",
      "Epoch 14 - avg_train_loss: 0.6645  avg_val_loss: 0.7409  time: 0s\n",
      "Epoch 14 - Score: 0.8625\n",
      "Epoch: [15][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6370(0.6370) Grad: 4626.9746  LR: 0.00656100  \n",
      "Epoch: [15][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6481(0.6571) Grad: 3369.6482  LR: 0.00656100  \n",
      "Epoch: [15][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7536(0.7536) Grad: 0.0000  \n",
      "Epoch: [15][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7146(0.7354) Grad: 0.0000  \n",
      "Epoch 15 - avg_train_loss: 0.6571  avg_val_loss: 0.7354  time: 0s\n",
      "Epoch 15 - Score: 0.8292\n",
      "Epoch: [16][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6320(0.6320) Grad: 3930.0295  LR: 0.00656100  \n",
      "Epoch: [16][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6665(0.6521) Grad: 2491.0974  LR: 0.00590490  \n",
      "Epoch: [16][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7608(0.7608) Grad: 0.0000  \n",
      "Epoch: [16][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7164(0.7401) Grad: 0.0000  \n",
      "Epoch 16 - avg_train_loss: 0.6521  avg_val_loss: 0.7401  time: 0s\n",
      "Epoch 16 - Score: 0.8208\n",
      "Epoch: [17][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6294(0.6294) Grad: 4174.2573  LR: 0.00590490  \n",
      "Epoch: [17][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6591(0.6504) Grad: 3845.0835  LR: 0.00590490  \n",
      "Epoch: [17][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7440(0.7440) Grad: 0.0000  \n",
      "Epoch: [17][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7191(0.7324) Grad: 0.0000  \n",
      "Epoch 17 - avg_train_loss: 0.6504  avg_val_loss: 0.7324  time: 0s\n",
      "Epoch 17 - Score: 0.8583\n",
      "Epoch: [18][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6780(0.6780) Grad: 2148.8062  LR: 0.00590490  \n",
      "Epoch: [18][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6410(0.6489) Grad: 2660.7014  LR: 0.00590490  \n",
      "Epoch: [18][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7429(0.7429) Grad: 0.0000  \n",
      "Epoch: [18][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7159(0.7303) Grad: 0.0000  \n",
      "Epoch 18 - avg_train_loss: 0.6489  avg_val_loss: 0.7303  time: 0s\n",
      "Epoch 18 - Score: 0.8708\n",
      "Epoch: [19][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6150(0.6150) Grad: 3259.1157  LR: 0.00590490  \n",
      "Epoch: [19][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6340(0.6480) Grad: 2467.8647  LR: 0.00531441  \n",
      "Epoch: [19][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7386(0.7386) Grad: 0.0000  \n",
      "Epoch: [19][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7156(0.7279) Grad: 0.0000  \n",
      "Epoch 19 - avg_train_loss: 0.6480  avg_val_loss: 0.7279  time: 0s\n",
      "Epoch 19 - Score: 0.8750\n",
      "Epoch 19 - Save Best Score: 0.8750 Model\n",
      "Epoch: [20][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6650(0.6650) Grad: 2660.4263  LR: 0.00531441  \n",
      "Epoch: [20][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6199(0.6457) Grad: 1850.0768  LR: 0.00531441  \n",
      "Epoch: [20][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6957(0.6957) Grad: 0.0000  \n",
      "Epoch: [20][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6863(0.6913) Grad: 0.0000  \n",
      "Epoch 20 - avg_train_loss: 0.6457  avg_val_loss: 0.6913  time: 0s\n",
      "Epoch 20 - Score: 0.8792\n",
      "Epoch 20 - Save Best Score: 0.8792 Model\n",
      "Epoch: [21][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6321(0.6321) Grad: 2671.1304  LR: 0.00531441  \n",
      "Epoch: [21][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6432(0.6413) Grad: 1818.9805  LR: 0.00531441  \n",
      "Epoch: [21][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6919(0.6919) Grad: 0.0000  \n",
      "Epoch: [21][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6852(0.6888) Grad: 0.0000  \n",
      "Epoch 21 - avg_train_loss: 0.6413  avg_val_loss: 0.6888  time: 0s\n",
      "Epoch 21 - Score: 0.8833\n",
      "Epoch 21 - Save Best Score: 0.8833 Model\n",
      "Epoch: [22][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 2392.6704  LR: 0.00531441  \n",
      "Epoch: [22][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6642(0.6411) Grad: 2655.0918  LR: 0.00430467  \n",
      "Epoch: [22][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6978(0.6978) Grad: 0.0000  \n",
      "Epoch: [22][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6803(0.6896) Grad: 0.0000  \n",
      "Epoch 22 - avg_train_loss: 0.6411  avg_val_loss: 0.6896  time: 0s\n",
      "Epoch 22 - Score: 0.8833\n",
      "Epoch: [23][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6408(0.6408) Grad: 2331.3862  LR: 0.00478297  \n",
      "Epoch: [23][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6588(0.6381) Grad: 1538.9086  LR: 0.00478297  \n",
      "Epoch: [23][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6931(0.6931) Grad: 0.0000  \n",
      "Epoch: [23][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6812(0.6876) Grad: 0.0000  \n",
      "Epoch 23 - avg_train_loss: 0.6381  avg_val_loss: 0.6876  time: 0s\n",
      "Epoch 23 - Score: 0.8833\n",
      "Epoch: [24][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6633(0.6633) Grad: 1677.4510  LR: 0.00478297  \n",
      "Epoch: [24][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6406(0.6342) Grad: 5025.0679  LR: 0.00478297  \n",
      "Epoch: [24][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6971(0.6971) Grad: 0.0000  \n",
      "Epoch: [24][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6902(0.6939) Grad: 0.0000  \n",
      "Epoch 24 - avg_train_loss: 0.6342  avg_val_loss: 0.6939  time: 0s\n",
      "Epoch 24 - Score: 0.8708\n",
      "Epoch: [25][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6434(0.6434) Grad: 1766.0121  LR: 0.00478297  \n",
      "Epoch: [25][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6885(0.6422) Grad: 1948.8944  LR: 0.00478297  \n",
      "Epoch: [25][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6844(0.6844) Grad: 0.0000  \n",
      "Epoch: [25][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6857(0.6850) Grad: 0.0000  \n",
      "Epoch 25 - avg_train_loss: 0.6422  avg_val_loss: 0.6850  time: 0s\n",
      "Epoch 25 - Score: 0.8833\n",
      "Epoch: [26][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6379(0.6379) Grad: 2117.6589  LR: 0.00387420  \n",
      "Epoch: [26][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6209(0.6386) Grad: 5162.1377  LR: 0.00430467  \n",
      "Epoch: [26][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6727(0.6727) Grad: 0.0000  \n",
      "Epoch: [26][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6743(0.6735) Grad: 0.0000  \n",
      "Epoch 26 - avg_train_loss: 0.6386  avg_val_loss: 0.6735  time: 0s\n",
      "Epoch 26 - Score: 0.8917\n",
      "Epoch 26 - Save Best Score: 0.8917 Model\n",
      "Epoch: [27][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6312(0.6312) Grad: 2406.8083  LR: 0.00430467  \n",
      "Epoch: [27][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6375(0.6368) Grad: 1725.1505  LR: 0.00430467  \n",
      "Epoch: [27][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6862(0.6862) Grad: 0.0000  \n",
      "Epoch: [27][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6882(0.6871) Grad: 0.0000  \n",
      "Epoch 27 - avg_train_loss: 0.6368  avg_val_loss: 0.6871  time: 0s\n",
      "Epoch 27 - Score: 0.8750\n",
      "Epoch: [28][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6245(0.6245) Grad: 3180.1248  LR: 0.00430467  \n",
      "Epoch: [28][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6351) Grad: 1484.6233  LR: 0.00430467  \n",
      "Epoch: [28][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6651(0.6651) Grad: 0.0000  \n",
      "Epoch: [28][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6646(0.6649) Grad: 0.0000  \n",
      "Epoch 28 - avg_train_loss: 0.6351  avg_val_loss: 0.6649  time: 0s\n",
      "Epoch 28 - Score: 0.9000\n",
      "Epoch 28 - Save Best Score: 0.9000 Model\n",
      "Epoch: [29][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6556(0.6556) Grad: 1312.7573  LR: 0.00430467  \n",
      "Epoch: [29][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6283(0.6342) Grad: 1358.2957  LR: 0.00387420  \n",
      "Epoch: [29][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6545(0.6545) Grad: 0.0000  \n",
      "Epoch: [29][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6490(0.6519) Grad: 0.0000  \n",
      "Epoch 29 - avg_train_loss: 0.6342  avg_val_loss: 0.6519  time: 0s\n",
      "Epoch 29 - Score: 0.9208\n",
      "Epoch 29 - Save Best Score: 0.9208 Model\n",
      "Epoch: [30][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6071(0.6071) Grad: 1354.0312  LR: 0.00387420  \n",
      "Epoch: [30][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6437(0.6337) Grad: 1471.4736  LR: 0.00387420  \n",
      "Epoch: [30][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6606(0.6606) Grad: 0.0000  \n",
      "Epoch: [30][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6528(0.6570) Grad: 0.0000  \n",
      "Epoch 30 - avg_train_loss: 0.6337  avg_val_loss: 0.6570  time: 0s\n",
      "Epoch 30 - Score: 0.9042\n",
      "Epoch: [31][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6350(0.6350) Grad: 1787.0496  LR: 0.00387420  \n",
      "Epoch: [31][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6111(0.6292) Grad: 1611.6406  LR: 0.00387420  \n",
      "Epoch: [31][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6662(0.6662) Grad: 0.0000  \n",
      "Epoch: [31][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6610(0.6638) Grad: 0.0000  \n",
      "Epoch 31 - avg_train_loss: 0.6292  avg_val_loss: 0.6638  time: 0s\n",
      "Epoch 31 - Score: 0.8958\n",
      "Epoch: [32][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6043(0.6043) Grad: 1144.8241  LR: 0.00387420  \n",
      "Epoch: [32][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6237(0.6312) Grad: 1865.7101  LR: 0.00348678  \n",
      "Epoch: [32][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6636(0.6636) Grad: 0.0000  \n",
      "Epoch: [32][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6608(0.6623) Grad: 0.0000  \n",
      "Epoch 32 - avg_train_loss: 0.6312  avg_val_loss: 0.6623  time: 0s\n",
      "Epoch 32 - Score: 0.8958\n",
      "Epoch: [33][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6317(0.6317) Grad: 916.0624  LR: 0.00348678  \n",
      "Epoch: [33][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6433(0.6296) Grad: 767.6869  LR: 0.00348678  \n",
      "Epoch: [33][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6447) Grad: 0.0000  \n",
      "Epoch: [33][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6440(0.6443) Grad: 0.0000  \n",
      "Epoch 33 - avg_train_loss: 0.6296  avg_val_loss: 0.6443  time: 0s\n",
      "Epoch 33 - Score: 0.9125\n",
      "Epoch: [34][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6103(0.6103) Grad: 1315.2769  LR: 0.00348678  \n",
      "Epoch: [34][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6172(0.6259) Grad: 2753.2119  LR: 0.00348678  \n",
      "Epoch: [34][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6466(0.6466) Grad: 0.0000  \n",
      "Epoch: [34][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6453(0.6460) Grad: 0.0000  \n",
      "Epoch 34 - avg_train_loss: 0.6259  avg_val_loss: 0.6460  time: 0s\n",
      "Epoch 34 - Score: 0.9125\n",
      "Epoch: [35][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6371(0.6371) Grad: 3142.5496  LR: 0.00348678  \n",
      "Epoch: [35][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6175(0.6271) Grad: 1786.8444  LR: 0.00313811  \n",
      "Epoch: [35][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6499) Grad: 0.0000  \n",
      "Epoch: [35][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6510(0.6504) Grad: 0.0000  \n",
      "Epoch 35 - avg_train_loss: 0.6271  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 35 - Score: 0.9125\n",
      "Epoch: [36][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6651(0.6651) Grad: 1682.6030  LR: 0.00313811  \n",
      "Epoch: [36][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6202(0.6307) Grad: 2396.5190  LR: 0.00313811  \n",
      "Epoch: [36][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6516(0.6516) Grad: 0.0000  \n",
      "Epoch: [36][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6504) Grad: 0.0000  \n",
      "Epoch 36 - avg_train_loss: 0.6307  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 36 - Score: 0.9083\n",
      "Epoch: [37][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6347(0.6347) Grad: 1688.9630  LR: 0.00313811  \n",
      "Epoch: [37][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5843(0.6251) Grad: 2661.4609  LR: 0.00313811  \n",
      "Epoch: [37][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6427(0.6427) Grad: 0.0000  \n",
      "Epoch: [37][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6381(0.6406) Grad: 0.0000  \n",
      "Epoch 37 - avg_train_loss: 0.6251  avg_val_loss: 0.6406  time: 0s\n",
      "Epoch 37 - Score: 0.9208\n",
      "Epoch: [38][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6457(0.6457) Grad: 1176.3976  LR: 0.00313811  \n",
      "Epoch: [38][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6278(0.6254) Grad: 666.6682  LR: 0.00282430  \n",
      "Epoch: [38][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6428(0.6428) Grad: 0.0000  \n",
      "Epoch: [38][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6376(0.6404) Grad: 0.0000  \n",
      "Epoch 38 - avg_train_loss: 0.6254  avg_val_loss: 0.6404  time: 0s\n",
      "Epoch 38 - Score: 0.9167\n",
      "Epoch: [39][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 1337.6270  LR: 0.00282430  \n",
      "Epoch: [39][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6223(0.6278) Grad: 1050.5190  LR: 0.00282430  \n",
      "Epoch: [39][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6429(0.6429) Grad: 0.0000  \n",
      "Epoch: [39][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6405(0.6418) Grad: 0.0000  \n",
      "Epoch 39 - avg_train_loss: 0.6278  avg_val_loss: 0.6418  time: 0s\n",
      "Epoch 39 - Score: 0.9208\n",
      "Epoch: [40][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6204(0.6204) Grad: 1411.9115  LR: 0.00282430  \n",
      "Epoch: [40][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6345(0.6269) Grad: 2809.7256  LR: 0.00282430  \n",
      "Epoch: [40][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6327(0.6327) Grad: 0.0000  \n",
      "Epoch: [40][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6338(0.6332) Grad: 0.0000  \n",
      "Epoch 40 - avg_train_loss: 0.6269  avg_val_loss: 0.6332  time: 0s\n",
      "Epoch 40 - Score: 0.9250\n",
      "Epoch 40 - Save Best Score: 0.9250 Model\n",
      "Epoch: [41][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 1889.4681  LR: 0.00282430  \n",
      "Epoch: [41][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5966(0.6227) Grad: 1490.8256  LR: 0.00254187  \n",
      "Epoch: [41][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6473(0.6473) Grad: 0.0000  \n",
      "Epoch: [41][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6436(0.6456) Grad: 0.0000  \n",
      "Epoch 41 - avg_train_loss: 0.6227  avg_val_loss: 0.6456  time: 0s\n",
      "Epoch 41 - Score: 0.9083\n",
      "Epoch: [42][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6037(0.6037) Grad: 1213.9740  LR: 0.00254187  \n",
      "Epoch: [42][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6456(0.6232) Grad: 1060.5273  LR: 0.00254187  \n",
      "Epoch: [42][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6352(0.6352) Grad: 0.0000  \n",
      "Epoch: [42][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6318(0.6336) Grad: 0.0000  \n",
      "Epoch 42 - avg_train_loss: 0.6232  avg_val_loss: 0.6336  time: 0s\n",
      "Epoch 42 - Score: 0.9292\n",
      "Epoch 42 - Save Best Score: 0.9292 Model\n",
      "Epoch: [43][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6154(0.6154) Grad: 798.9210  LR: 0.00254187  \n",
      "Epoch: [43][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6006(0.6238) Grad: 909.2502  LR: 0.00254187  \n",
      "Epoch: [43][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6311(0.6311) Grad: 0.0000  \n",
      "Epoch: [43][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6302(0.6307) Grad: 0.0000  \n",
      "Epoch 43 - avg_train_loss: 0.6238  avg_val_loss: 0.6307  time: 0s\n",
      "Epoch 43 - Score: 0.9292\n",
      "Epoch: [44][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5976(0.5976) Grad: 1166.9601  LR: 0.00254187  \n",
      "Epoch: [44][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6393(0.6213) Grad: 1535.3448  LR: 0.00205891  \n",
      "Epoch: [44][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6317(0.6317) Grad: 0.0000  \n",
      "Epoch: [44][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6316(0.6316) Grad: 0.0000  \n",
      "Epoch 44 - avg_train_loss: 0.6213  avg_val_loss: 0.6316  time: 0s\n",
      "Epoch 44 - Score: 0.9250\n",
      "Epoch: [45][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6050) Grad: 633.1556  LR: 0.00228768  \n",
      "Epoch: [45][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6469(0.6255) Grad: 2075.6260  LR: 0.00228768  \n",
      "Epoch: [45][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6361(0.6361) Grad: 0.0000  \n",
      "Epoch: [45][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6313(0.6339) Grad: 0.0000  \n",
      "Epoch 45 - avg_train_loss: 0.6255  avg_val_loss: 0.6339  time: 0s\n",
      "Epoch 45 - Score: 0.9250\n",
      "Epoch: [46][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5807(0.5807) Grad: 561.9157  LR: 0.00228768  \n",
      "Epoch: [46][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6225) Grad: 592.1524  LR: 0.00228768  \n",
      "Epoch: [46][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6387(0.6387) Grad: 0.0000  \n",
      "Epoch: [46][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6311(0.6352) Grad: 0.0000  \n",
      "Epoch 46 - avg_train_loss: 0.6225  avg_val_loss: 0.6352  time: 0s\n",
      "Epoch 46 - Score: 0.9250\n",
      "Epoch: [47][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6467(0.6467) Grad: 1219.7202  LR: 0.00228768  \n",
      "Epoch: [47][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6042(0.6243) Grad: 1396.8354  LR: 0.00228768  \n",
      "Epoch: [47][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6374(0.6374) Grad: 0.0000  \n",
      "Epoch: [47][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6323(0.6350) Grad: 0.0000  \n",
      "Epoch 47 - avg_train_loss: 0.6243  avg_val_loss: 0.6350  time: 0s\n",
      "Epoch 47 - Score: 0.9250\n",
      "Epoch: [48][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6419(0.6419) Grad: 1141.8148  LR: 0.00185302  \n",
      "Epoch: [48][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6268(0.6256) Grad: 427.5089  LR: 0.00205891  \n",
      "Epoch: [48][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6341(0.6341) Grad: 0.0000  \n",
      "Epoch: [48][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6303(0.6324) Grad: 0.0000  \n",
      "Epoch 48 - avg_train_loss: 0.6256  avg_val_loss: 0.6324  time: 0s\n",
      "Epoch 48 - Score: 0.9250\n",
      "Epoch: [49][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6211(0.6211) Grad: 1151.5110  LR: 0.00205891  \n",
      "Epoch: [49][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6113(0.6198) Grad: 456.6693  LR: 0.00205891  \n",
      "Epoch: [49][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6401(0.6401) Grad: 0.0000  \n",
      "Epoch: [49][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6334(0.6369) Grad: 0.0000  \n",
      "Epoch 49 - avg_train_loss: 0.6198  avg_val_loss: 0.6369  time: 0s\n",
      "Epoch 49 - Score: 0.9208\n",
      "Epoch: [50][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6019(0.6019) Grad: 3226.3364  LR: 0.00205891  \n",
      "Epoch: [50][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6720(0.6240) Grad: 598.8425  LR: 0.00205891  \n",
      "Epoch: [50][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6432(0.6432) Grad: 0.0000  \n",
      "Epoch: [50][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6362(0.6399) Grad: 0.0000  \n",
      "Epoch 50 - avg_train_loss: 0.6240  avg_val_loss: 0.6399  time: 0s\n",
      "Epoch 50 - Score: 0.9167\n",
      "Epoch: [51][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6149(0.6149) Grad: 824.9988  LR: 0.00205891  \n",
      "Epoch: [51][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6592(0.6206) Grad: 1316.9945  LR: 0.00185302  \n",
      "Epoch: [51][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6362(0.6362) Grad: 0.0000  \n",
      "Epoch: [51][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6312(0.6339) Grad: 0.0000  \n",
      "Epoch 51 - avg_train_loss: 0.6206  avg_val_loss: 0.6339  time: 0s\n",
      "Epoch 51 - Score: 0.9250\n",
      "Epoch: [52][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6346(0.6346) Grad: 1484.7238  LR: 0.00185302  \n",
      "Epoch: [52][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6372(0.6225) Grad: 2059.3918  LR: 0.00185302  \n",
      "Epoch: [52][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6361(0.6361) Grad: 0.0000  \n",
      "Epoch: [52][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6310(0.6337) Grad: 0.0000  \n",
      "Epoch 52 - avg_train_loss: 0.6225  avg_val_loss: 0.6337  time: 0s\n",
      "Epoch 52 - Score: 0.9250\n",
      "Epoch: [53][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6440(0.6440) Grad: 665.3441  LR: 0.00185302  \n",
      "Epoch: [53][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6132(0.6233) Grad: 1528.2875  LR: 0.00185302  \n",
      "Epoch: [53][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6305(0.6305) Grad: 0.0000  \n",
      "Epoch: [53][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6347(0.6324) Grad: 0.0000  \n",
      "Epoch 53 - avg_train_loss: 0.6233  avg_val_loss: 0.6324  time: 0s\n",
      "Epoch 53 - Score: 0.9250\n",
      "Epoch: [54][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6351(0.6351) Grad: 466.2439  LR: 0.00185302  \n",
      "Epoch: [54][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6283(0.6218) Grad: 819.7026  LR: 0.00166772  \n",
      "Epoch: [54][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6302(0.6302) Grad: 0.0000  \n",
      "Epoch: [54][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6343(0.6321) Grad: 0.0000  \n",
      "Epoch 54 - avg_train_loss: 0.6218  avg_val_loss: 0.6321  time: 0s\n",
      "Epoch 54 - Score: 0.9250\n",
      "Epoch: [55][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6122(0.6122) Grad: 514.1973  LR: 0.00166772  \n",
      "Epoch: [55][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6329(0.6228) Grad: 3206.2808  LR: 0.00166772  \n",
      "Epoch: [55][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6318(0.6318) Grad: 0.0000  \n",
      "Epoch: [55][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6310(0.6314) Grad: 0.0000  \n",
      "Epoch 55 - avg_train_loss: 0.6228  avg_val_loss: 0.6314  time: 0s\n",
      "Epoch 55 - Score: 0.9250\n",
      "Epoch: [56][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6266(0.6266) Grad: 752.4820  LR: 0.00166772  \n",
      "Epoch: [56][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6147(0.6225) Grad: 2016.7955  LR: 0.00166772  \n",
      "Epoch: [56][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6339(0.6339) Grad: 0.0000  \n",
      "Epoch: [56][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6302(0.6322) Grad: 0.0000  \n",
      "Epoch 56 - avg_train_loss: 0.6225  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 56 - Score: 0.9250\n",
      "Epoch: [57][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6043(0.6043) Grad: 2757.1956  LR: 0.00166772  \n",
      "Epoch: [57][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6218(0.6213) Grad: 2390.2651  LR: 0.00150095  \n",
      "Epoch: [57][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6348(0.6348) Grad: 0.0000  \n",
      "Epoch: [57][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6293(0.6323) Grad: 0.0000  \n",
      "Epoch 57 - avg_train_loss: 0.6213  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 57 - Score: 0.9250\n",
      "Epoch: [58][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6297(0.6297) Grad: 2131.5291  LR: 0.00150095  \n",
      "Epoch: [58][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6245) Grad: 635.5705  LR: 0.00150095  \n",
      "Epoch: [58][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6355(0.6355) Grad: 0.0000  \n",
      "Epoch: [58][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6295(0.6327) Grad: 0.0000  \n",
      "Epoch 58 - avg_train_loss: 0.6245  avg_val_loss: 0.6327  time: 0s\n",
      "Epoch 58 - Score: 0.9250\n",
      "Epoch: [59][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6051(0.6051) Grad: 626.4435  LR: 0.00150095  \n",
      "Epoch: [59][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6339(0.6211) Grad: 985.2064  LR: 0.00150095  \n",
      "Epoch: [59][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6371(0.6371) Grad: 0.0000  \n",
      "Epoch: [59][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6307(0.6341) Grad: 0.0000  \n",
      "Epoch 59 - avg_train_loss: 0.6211  avg_val_loss: 0.6341  time: 0s\n",
      "Epoch 59 - Score: 0.9250\n",
      "Epoch: [60][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6351(0.6351) Grad: 619.5466  LR: 0.00150095  \n",
      "Epoch: [60][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6131(0.6207) Grad: 717.1240  LR: 0.00135085  \n",
      "Epoch: [60][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6417(0.6417) Grad: 0.0000  \n",
      "Epoch: [60][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6347(0.6384) Grad: 0.0000  \n",
      "Epoch 60 - avg_train_loss: 0.6207  avg_val_loss: 0.6384  time: 0s\n",
      "Epoch 60 - Score: 0.9167\n",
      "Epoch: [61][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6043(0.6043) Grad: 551.5724  LR: 0.00135085  \n",
      "Epoch: [61][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6184) Grad: 2397.2820  LR: 0.00135085  \n",
      "Epoch: [61][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6460) Grad: 0.0000  \n",
      "Epoch: [61][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6359(0.6413) Grad: 0.0000  \n",
      "Epoch 61 - avg_train_loss: 0.6184  avg_val_loss: 0.6413  time: 0s\n",
      "Epoch 61 - Score: 0.9167\n",
      "Epoch: [62][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6269(0.6269) Grad: 839.9586  LR: 0.00135085  \n",
      "Epoch: [62][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6134(0.6191) Grad: 2082.7668  LR: 0.00135085  \n",
      "Epoch: [62][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6476(0.6476) Grad: 0.0000  \n",
      "Epoch: [62][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6357(0.6421) Grad: 0.0000  \n",
      "Epoch 62 - avg_train_loss: 0.6191  avg_val_loss: 0.6421  time: 0s\n",
      "Epoch 62 - Score: 0.9167\n",
      "Epoch: [63][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6194) Grad: 896.3629  LR: 0.00135085  \n",
      "Epoch: [63][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6190) Grad: 667.3943  LR: 0.00121577  \n",
      "Epoch: [63][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6412(0.6412) Grad: 0.0000  \n",
      "Epoch: [63][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6341(0.6379) Grad: 0.0000  \n",
      "Epoch 63 - avg_train_loss: 0.6190  avg_val_loss: 0.6379  time: 0s\n",
      "Epoch 63 - Score: 0.9208\n",
      "Epoch: [64][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6384(0.6384) Grad: 1595.5551  LR: 0.00121577  \n",
      "Epoch: [64][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6049(0.6180) Grad: 1277.1534  LR: 0.00121577  \n",
      "Epoch: [64][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6416(0.6416) Grad: 0.0000  \n",
      "Epoch: [64][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6345(0.6383) Grad: 0.0000  \n",
      "Epoch 64 - avg_train_loss: 0.6180  avg_val_loss: 0.6383  time: 0s\n",
      "Epoch 64 - Score: 0.9208\n",
      "Epoch: [65][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6076(0.6076) Grad: 3218.9353  LR: 0.00121577  \n",
      "Epoch: [65][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5995(0.6213) Grad: 4157.8647  LR: 0.00121577  \n",
      "Epoch: [65][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6525(0.6525) Grad: 0.0000  \n",
      "Epoch: [65][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6370(0.6453) Grad: 0.0000  \n",
      "Epoch 65 - avg_train_loss: 0.6213  avg_val_loss: 0.6453  time: 0s\n",
      "Epoch 65 - Score: 0.9125\n",
      "Epoch: [66][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5954(0.5954) Grad: 881.8596  LR: 0.00121577  \n",
      "Epoch: [66][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5981(0.6201) Grad: 2397.9531  LR: 0.00098477  \n",
      "Epoch: [66][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6524(0.6524) Grad: 0.0000  \n",
      "Epoch: [66][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6367(0.6451) Grad: 0.0000  \n",
      "Epoch 66 - avg_train_loss: 0.6201  avg_val_loss: 0.6451  time: 0s\n",
      "Epoch 66 - Score: 0.9083\n",
      "Epoch: [67][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6567(0.6567) Grad: 448.9516  LR: 0.00109419  \n",
      "Epoch: [67][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5995(0.6226) Grad: 1985.8263  LR: 0.00109419  \n",
      "Epoch: [67][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6418(0.6418) Grad: 0.0000  \n",
      "Epoch: [67][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6330(0.6377) Grad: 0.0000  \n",
      "Epoch 67 - avg_train_loss: 0.6226  avg_val_loss: 0.6377  time: 0s\n",
      "Epoch 67 - Score: 0.9208\n",
      "Epoch: [68][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6439(0.6439) Grad: 3382.4341  LR: 0.00109419  \n",
      "Epoch: [68][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6691(0.6196) Grad: 2008.1138  LR: 0.00109419  \n",
      "Epoch: [68][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6461(0.6461) Grad: 0.0000  \n",
      "Epoch: [68][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6310(0.6391) Grad: 0.0000  \n",
      "Epoch 68 - avg_train_loss: 0.6196  avg_val_loss: 0.6391  time: 0s\n",
      "Epoch 68 - Score: 0.9167\n",
      "Epoch: [69][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6021(0.6021) Grad: 1849.4766  LR: 0.00109419  \n",
      "Epoch: [69][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6202(0.6163) Grad: 1927.9326  LR: 0.00109419  \n",
      "Epoch: [69][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6459(0.6459) Grad: 0.0000  \n",
      "Epoch: [69][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6302(0.6386) Grad: 0.0000  \n",
      "Epoch 69 - avg_train_loss: 0.6163  avg_val_loss: 0.6386  time: 0s\n",
      "Epoch 69 - Score: 0.9167\n",
      "Epoch: [70][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6116(0.6116) Grad: 1018.1443  LR: 0.00088629  \n",
      "Epoch: [70][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6205) Grad: 1219.7709  LR: 0.00098477  \n",
      "Epoch: [70][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6395(0.6395) Grad: 0.0000  \n",
      "Epoch: [70][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6299(0.6350) Grad: 0.0000  \n",
      "Epoch 70 - avg_train_loss: 0.6205  avg_val_loss: 0.6350  time: 0s\n",
      "Epoch 70 - Score: 0.9250\n",
      "Epoch: [71][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6565(0.6565) Grad: 4954.1211  LR: 0.00098477  \n",
      "Epoch: [71][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5976(0.6196) Grad: 649.8700  LR: 0.00098477  \n",
      "Epoch: [71][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6396(0.6396) Grad: 0.0000  \n",
      "Epoch: [71][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6305(0.6353) Grad: 0.0000  \n",
      "Epoch 71 - avg_train_loss: 0.6196  avg_val_loss: 0.6353  time: 0s\n",
      "Epoch 71 - Score: 0.9250\n",
      "Epoch: [72][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5900(0.5900) Grad: 3291.8528  LR: 0.00098477  \n",
      "Epoch: [72][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6023(0.6169) Grad: 1614.2520  LR: 0.00098477  \n",
      "Epoch: [72][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6397(0.6397) Grad: 0.0000  \n",
      "Epoch: [72][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6303(0.6353) Grad: 0.0000  \n",
      "Epoch 72 - avg_train_loss: 0.6169  avg_val_loss: 0.6353  time: 0s\n",
      "Epoch 72 - Score: 0.9250\n",
      "Epoch: [73][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6174(0.6174) Grad: 4580.4834  LR: 0.00098477  \n",
      "Epoch: [73][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5908(0.6155) Grad: 3716.4858  LR: 0.00088629  \n",
      "Epoch: [73][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6460) Grad: 0.0000  \n",
      "Epoch: [73][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6358(0.6412) Grad: 0.0000  \n",
      "Epoch 73 - avg_train_loss: 0.6155  avg_val_loss: 0.6412  time: 0s\n",
      "Epoch 73 - Score: 0.9125\n",
      "Epoch: [74][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6132(0.6132) Grad: 2025.2174  LR: 0.00088629  \n",
      "Epoch: [74][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6641(0.6174) Grad: 4114.9380  LR: 0.00088629  \n",
      "Epoch: [74][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6429(0.6429) Grad: 0.0000  \n",
      "Epoch: [74][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6389(0.6410) Grad: 0.0000  \n",
      "Epoch 74 - avg_train_loss: 0.6174  avg_val_loss: 0.6410  time: 0s\n",
      "Epoch 74 - Score: 0.9125\n",
      "Epoch: [75][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 848.7885  LR: 0.00088629  \n",
      "Epoch: [75][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6204(0.6196) Grad: 3153.1594  LR: 0.00088629  \n",
      "Epoch: [75][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6470(0.6470) Grad: 0.0000  \n",
      "Epoch: [75][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6420(0.6446) Grad: 0.0000  \n",
      "Epoch 75 - avg_train_loss: 0.6196  avg_val_loss: 0.6446  time: 0s\n",
      "Epoch 75 - Score: 0.9125\n",
      "Epoch: [76][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5969(0.5969) Grad: 1408.2592  LR: 0.00088629  \n",
      "Epoch: [76][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6347(0.6165) Grad: 5187.7310  LR: 0.00079766  \n",
      "Epoch: [76][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6599(0.6599) Grad: 0.0000  \n",
      "Epoch: [76][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6527) Grad: 0.0000  \n",
      "Epoch 76 - avg_train_loss: 0.6165  avg_val_loss: 0.6527  time: 0s\n",
      "Epoch 76 - Score: 0.9000\n",
      "Epoch: [77][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6053(0.6053) Grad: 1293.5231  LR: 0.00079766  \n",
      "Epoch: [77][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6349(0.6174) Grad: 1021.3020  LR: 0.00079766  \n",
      "Epoch: [77][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6612(0.6612) Grad: 0.0000  \n",
      "Epoch: [77][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6427(0.6526) Grad: 0.0000  \n",
      "Epoch 77 - avg_train_loss: 0.6174  avg_val_loss: 0.6526  time: 0s\n",
      "Epoch 77 - Score: 0.9000\n",
      "Epoch: [78][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5852(0.5852) Grad: 7138.3765  LR: 0.00079766  \n",
      "Epoch: [78][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6361(0.6152) Grad: 3384.6284  LR: 0.00079766  \n",
      "Epoch: [78][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6537(0.6537) Grad: 0.0000  \n",
      "Epoch: [78][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6422(0.6483) Grad: 0.0000  \n",
      "Epoch 78 - avg_train_loss: 0.6152  avg_val_loss: 0.6483  time: 0s\n",
      "Epoch 78 - Score: 0.9083\n",
      "Epoch: [79][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6270(0.6270) Grad: 660.8426  LR: 0.00079766  \n",
      "Epoch: [79][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6164(0.6176) Grad: 2633.9775  LR: 0.00071790  \n",
      "Epoch: [79][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6519(0.6519) Grad: 0.0000  \n",
      "Epoch: [79][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6429(0.6477) Grad: 0.0000  \n",
      "Epoch 79 - avg_train_loss: 0.6176  avg_val_loss: 0.6477  time: 0s\n",
      "Epoch 79 - Score: 0.9083\n",
      "Epoch: [80][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6343(0.6343) Grad: 2589.1968  LR: 0.00071790  \n",
      "Epoch: [80][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6206) Grad: 436.4120  LR: 0.00071790  \n",
      "Epoch: [80][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [80][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6418(0.6464) Grad: 0.0000  \n",
      "Epoch 80 - avg_train_loss: 0.6206  avg_val_loss: 0.6464  time: 0s\n",
      "Epoch 80 - Score: 0.9083\n",
      "Epoch: [81][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5992(0.5992) Grad: 750.4364  LR: 0.00071790  \n",
      "Epoch: [81][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6253(0.6151) Grad: 764.5273  LR: 0.00071790  \n",
      "Epoch: [81][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6490(0.6490) Grad: 0.0000  \n",
      "Epoch: [81][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6430(0.6462) Grad: 0.0000  \n",
      "Epoch 81 - avg_train_loss: 0.6151  avg_val_loss: 0.6462  time: 0s\n",
      "Epoch 81 - Score: 0.9125\n",
      "Epoch: [82][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6012(0.6012) Grad: 4686.2173  LR: 0.00071790  \n",
      "Epoch: [82][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6114(0.6182) Grad: 1267.3208  LR: 0.00064611  \n",
      "Epoch: [82][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6462(0.6462) Grad: 0.0000  \n",
      "Epoch: [82][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6419(0.6442) Grad: 0.0000  \n",
      "Epoch 82 - avg_train_loss: 0.6182  avg_val_loss: 0.6442  time: 0s\n",
      "Epoch 82 - Score: 0.9083\n",
      "Epoch: [83][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6237(0.6237) Grad: 4221.1006  LR: 0.00064611  \n",
      "Epoch: [83][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6022(0.6137) Grad: 1070.1626  LR: 0.00064611  \n",
      "Epoch: [83][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6397(0.6397) Grad: 0.0000  \n",
      "Epoch: [83][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6389(0.6393) Grad: 0.0000  \n",
      "Epoch 83 - avg_train_loss: 0.6137  avg_val_loss: 0.6393  time: 0s\n",
      "Epoch 83 - Score: 0.9125\n",
      "Epoch: [84][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6062(0.6062) Grad: 3842.5117  LR: 0.00064611  \n",
      "Epoch: [84][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6272(0.6205) Grad: 480.9383  LR: 0.00064611  \n",
      "Epoch: [84][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6414(0.6414) Grad: 0.0000  \n",
      "Epoch: [84][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6371(0.6394) Grad: 0.0000  \n",
      "Epoch 84 - avg_train_loss: 0.6205  avg_val_loss: 0.6394  time: 0s\n",
      "Epoch 84 - Score: 0.9167\n",
      "Epoch: [85][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6087(0.6087) Grad: 364.5136  LR: 0.00064611  \n",
      "Epoch: [85][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6193(0.6167) Grad: 497.8406  LR: 0.00058150  \n",
      "Epoch: [85][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6474) Grad: 0.0000  \n",
      "Epoch: [85][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6368(0.6425) Grad: 0.0000  \n",
      "Epoch 85 - avg_train_loss: 0.6167  avg_val_loss: 0.6425  time: 0s\n",
      "Epoch 85 - Score: 0.9167\n",
      "Epoch: [86][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6301(0.6301) Grad: 3880.2312  LR: 0.00058150  \n",
      "Epoch: [86][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5914(0.6176) Grad: 4642.2266  LR: 0.00058150  \n",
      "Epoch: [86][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6471(0.6471) Grad: 0.0000  \n",
      "Epoch: [86][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6362(0.6420) Grad: 0.0000  \n",
      "Epoch 86 - avg_train_loss: 0.6176  avg_val_loss: 0.6420  time: 0s\n",
      "Epoch 86 - Score: 0.9167\n",
      "Epoch: [87][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6423(0.6423) Grad: 718.9628  LR: 0.00058150  \n",
      "Epoch: [87][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5896(0.6176) Grad: 2315.3931  LR: 0.00058150  \n",
      "Epoch: [87][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6446) Grad: 0.0000  \n",
      "Epoch: [87][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6359(0.6405) Grad: 0.0000  \n",
      "Epoch 87 - avg_train_loss: 0.6176  avg_val_loss: 0.6405  time: 0s\n",
      "Epoch 87 - Score: 0.9167\n",
      "Epoch: [88][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5855(0.5855) Grad: 4765.9463  LR: 0.00058150  \n",
      "Epoch: [88][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6617(0.6138) Grad: 4447.1738  LR: 0.00047101  \n",
      "Epoch: [88][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6469(0.6469) Grad: 0.0000  \n",
      "Epoch: [88][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6362(0.6419) Grad: 0.0000  \n",
      "Epoch 88 - avg_train_loss: 0.6138  avg_val_loss: 0.6419  time: 0s\n",
      "Epoch 88 - Score: 0.9125\n",
      "Epoch: [89][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6480) Grad: 2675.6858  LR: 0.00052335  \n",
      "Epoch: [89][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5895(0.6217) Grad: 4920.4224  LR: 0.00052335  \n",
      "Epoch: [89][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6483) Grad: 0.0000  \n",
      "Epoch: [89][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6367(0.6429) Grad: 0.0000  \n",
      "Epoch 89 - avg_train_loss: 0.6217  avg_val_loss: 0.6429  time: 0s\n",
      "Epoch 89 - Score: 0.9167\n",
      "Epoch: [90][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6176(0.6176) Grad: 794.8445  LR: 0.00052335  \n",
      "Epoch: [90][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6562(0.6182) Grad: 1493.6117  LR: 0.00052335  \n",
      "Epoch: [90][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6476(0.6476) Grad: 0.0000  \n",
      "Epoch: [90][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6368(0.6426) Grad: 0.0000  \n",
      "Epoch 90 - avg_train_loss: 0.6182  avg_val_loss: 0.6426  time: 0s\n",
      "Epoch 90 - Score: 0.9125\n",
      "Epoch: [91][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5765(0.5765) Grad: 3477.8743  LR: 0.00052335  \n",
      "Epoch: [91][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6079(0.6152) Grad: 2785.5635  LR: 0.00052335  \n",
      "Epoch: [91][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6456(0.6456) Grad: 0.0000  \n",
      "Epoch: [91][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6361(0.6412) Grad: 0.0000  \n",
      "Epoch 91 - avg_train_loss: 0.6152  avg_val_loss: 0.6412  time: 0s\n",
      "Epoch 91 - Score: 0.9167\n",
      "Epoch: [92][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5987(0.5987) Grad: 4586.8838  LR: 0.00042391  \n",
      "Epoch: [92][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6647(0.6180) Grad: 7682.3472  LR: 0.00047101  \n",
      "Epoch: [92][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6460) Grad: 0.0000  \n",
      "Epoch: [92][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6361(0.6414) Grad: 0.0000  \n",
      "Epoch 92 - avg_train_loss: 0.6180  avg_val_loss: 0.6414  time: 0s\n",
      "Epoch 92 - Score: 0.9167\n",
      "Epoch: [93][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5944(0.5944) Grad: 366.2439  LR: 0.00047101  \n",
      "Epoch: [93][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6174) Grad: 705.6722  LR: 0.00047101  \n",
      "Epoch: [93][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6480) Grad: 0.0000  \n",
      "Epoch: [93][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6362(0.6425) Grad: 0.0000  \n",
      "Epoch 93 - avg_train_loss: 0.6174  avg_val_loss: 0.6425  time: 0s\n",
      "Epoch 93 - Score: 0.9125\n",
      "Epoch: [94][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6254(0.6254) Grad: 555.6685  LR: 0.00047101  \n",
      "Epoch: [94][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6236(0.6116) Grad: 3958.5134  LR: 0.00047101  \n",
      "Epoch: [94][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6482(0.6482) Grad: 0.0000  \n",
      "Epoch: [94][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6364(0.6427) Grad: 0.0000  \n",
      "Epoch 94 - avg_train_loss: 0.6116  avg_val_loss: 0.6427  time: 0s\n",
      "Epoch 94 - Score: 0.9167\n",
      "Epoch: [95][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6704(0.6704) Grad: 2393.1577  LR: 0.00047101  \n",
      "Epoch: [95][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6105(0.6175) Grad: 439.6190  LR: 0.00042391  \n",
      "Epoch: [95][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6488(0.6488) Grad: 0.0000  \n",
      "Epoch: [95][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6374(0.6435) Grad: 0.0000  \n",
      "Epoch 95 - avg_train_loss: 0.6175  avg_val_loss: 0.6435  time: 0s\n",
      "Epoch 95 - Score: 0.9167\n",
      "Epoch: [96][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5992(0.5992) Grad: 5835.8794  LR: 0.00042391  \n",
      "Epoch: [96][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6236(0.6153) Grad: 780.0745  LR: 0.00042391  \n",
      "Epoch: [96][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6500) Grad: 0.0000  \n",
      "Epoch: [96][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6393(0.6450) Grad: 0.0000  \n",
      "Epoch 96 - avg_train_loss: 0.6153  avg_val_loss: 0.6450  time: 0s\n",
      "Epoch 96 - Score: 0.9125\n",
      "Epoch: [97][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6272(0.6272) Grad: 3516.4392  LR: 0.00042391  \n",
      "Epoch: [97][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6019(0.6141) Grad: 2110.8154  LR: 0.00042391  \n",
      "Epoch: [97][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6530(0.6530) Grad: 0.0000  \n",
      "Epoch: [97][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6401(0.6470) Grad: 0.0000  \n",
      "Epoch 97 - avg_train_loss: 0.6141  avg_val_loss: 0.6470  time: 0s\n",
      "Epoch 97 - Score: 0.9042\n",
      "Epoch: [98][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6110(0.6110) Grad: 1365.5535  LR: 0.00042391  \n",
      "Epoch: [98][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5852(0.6174) Grad: 1515.1996  LR: 0.00038152  \n",
      "Epoch: [98][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6534(0.6534) Grad: 0.0000  \n",
      "Epoch: [98][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6416(0.6479) Grad: 0.0000  \n",
      "Epoch 98 - avg_train_loss: 0.6174  avg_val_loss: 0.6479  time: 0s\n",
      "Epoch 98 - Score: 0.9042\n",
      "Epoch: [99][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6127(0.6127) Grad: 6483.4990  LR: 0.00038152  \n",
      "Epoch: [99][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6201(0.6164) Grad: 2451.5615  LR: 0.00038152  \n",
      "Epoch: [99][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [99][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6431(0.6468) Grad: 0.0000  \n",
      "Epoch 99 - avg_train_loss: 0.6164  avg_val_loss: 0.6468  time: 0s\n",
      "Epoch 99 - Score: 0.9125\n",
      "Epoch: [100][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6108(0.6108) Grad: 652.0765  LR: 0.00038152  \n",
      "Epoch: [100][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5877(0.6123) Grad: 466.3217  LR: 0.00038152  \n",
      "Epoch: [100][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [100][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6423(0.6463) Grad: 0.0000  \n",
      "Epoch 100 - avg_train_loss: 0.6123  avg_val_loss: 0.6463  time: 0s\n",
      "Epoch 100 - Score: 0.9125\n",
      "Epoch: [101][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6213(0.6213) Grad: 3473.9626  LR: 0.00038152  \n",
      "Epoch: [101][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6346(0.6163) Grad: 1136.9912  LR: 0.00034337  \n",
      "Epoch: [101][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6511(0.6511) Grad: 0.0000  \n",
      "Epoch: [101][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6419(0.6468) Grad: 0.0000  \n",
      "Epoch 101 - avg_train_loss: 0.6163  avg_val_loss: 0.6468  time: 0s\n",
      "Epoch 101 - Score: 0.9083\n",
      "Epoch: [102][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6157(0.6157) Grad: 3230.6206  LR: 0.00034337  \n",
      "Epoch: [102][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6120(0.6161) Grad: 975.1944  LR: 0.00034337  \n",
      "Epoch: [102][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6494) Grad: 0.0000  \n",
      "Epoch: [102][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6415(0.6457) Grad: 0.0000  \n",
      "Epoch 102 - avg_train_loss: 0.6161  avg_val_loss: 0.6457  time: 0s\n",
      "Epoch 102 - Score: 0.9125\n",
      "Epoch: [103][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6084(0.6084) Grad: 3165.2014  LR: 0.00034337  \n",
      "Epoch: [103][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5797(0.6181) Grad: 826.7812  LR: 0.00034337  \n",
      "Epoch: [103][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6464(0.6464) Grad: 0.0000  \n",
      "Epoch: [103][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6416(0.6441) Grad: 0.0000  \n",
      "Epoch 103 - avg_train_loss: 0.6181  avg_val_loss: 0.6441  time: 0s\n",
      "Epoch 103 - Score: 0.9125\n",
      "Epoch: [104][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6199(0.6199) Grad: 2498.8674  LR: 0.00034337  \n",
      "Epoch: [104][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6030(0.6145) Grad: 1463.9961  LR: 0.00030903  \n",
      "Epoch: [104][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6456(0.6456) Grad: 0.0000  \n",
      "Epoch: [104][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6427(0.6442) Grad: 0.0000  \n",
      "Epoch 104 - avg_train_loss: 0.6145  avg_val_loss: 0.6442  time: 0s\n",
      "Epoch 104 - Score: 0.9125\n",
      "Epoch: [105][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6111(0.6111) Grad: 696.3403  LR: 0.00030903  \n",
      "Epoch: [105][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5992(0.6149) Grad: 4518.4902  LR: 0.00030903  \n",
      "Epoch: [105][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6459(0.6459) Grad: 0.0000  \n",
      "Epoch: [105][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6430(0.6446) Grad: 0.0000  \n",
      "Epoch 105 - avg_train_loss: 0.6149  avg_val_loss: 0.6446  time: 0s\n",
      "Epoch 105 - Score: 0.9125\n",
      "Epoch: [106][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6259(0.6259) Grad: 393.1610  LR: 0.00030903  \n",
      "Epoch: [106][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6206(0.6167) Grad: 602.5654  LR: 0.00030903  \n",
      "Epoch: [106][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6468(0.6468) Grad: 0.0000  \n",
      "Epoch: [106][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6442(0.6456) Grad: 0.0000  \n",
      "Epoch 106 - avg_train_loss: 0.6167  avg_val_loss: 0.6456  time: 0s\n",
      "Epoch 106 - Score: 0.9125\n",
      "Epoch: [107][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6253(0.6253) Grad: 1320.1198  LR: 0.00030903  \n",
      "Epoch: [107][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6309(0.6176) Grad: 3727.2666  LR: 0.00027813  \n",
      "Epoch: [107][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6474) Grad: 0.0000  \n",
      "Epoch: [107][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6441(0.6458) Grad: 0.0000  \n",
      "Epoch 107 - avg_train_loss: 0.6176  avg_val_loss: 0.6458  time: 0s\n",
      "Epoch 107 - Score: 0.9125\n",
      "Epoch: [108][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5810(0.5810) Grad: 1230.6062  LR: 0.00027813  \n",
      "Epoch: [108][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6113(0.6161) Grad: 467.0232  LR: 0.00027813  \n",
      "Epoch: [108][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6443(0.6443) Grad: 0.0000  \n",
      "Epoch: [108][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6444(0.6443) Grad: 0.0000  \n",
      "Epoch 108 - avg_train_loss: 0.6161  avg_val_loss: 0.6443  time: 0s\n",
      "Epoch 108 - Score: 0.9125\n",
      "Epoch: [109][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6268(0.6268) Grad: 748.4559  LR: 0.00027813  \n",
      "Epoch: [109][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6089(0.6155) Grad: 3680.9585  LR: 0.00027813  \n",
      "Epoch: [109][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6432(0.6432) Grad: 0.0000  \n",
      "Epoch: [109][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6438) Grad: 0.0000  \n",
      "Epoch 109 - avg_train_loss: 0.6155  avg_val_loss: 0.6438  time: 0s\n",
      "Epoch 109 - Score: 0.9125\n",
      "Epoch: [110][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6260(0.6260) Grad: 1027.0770  LR: 0.00027813  \n",
      "Epoch: [110][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5880(0.6150) Grad: 484.8187  LR: 0.00022528  \n",
      "Epoch: [110][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6420(0.6420) Grad: 0.0000  \n",
      "Epoch: [110][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6442(0.6431) Grad: 0.0000  \n",
      "Epoch 110 - avg_train_loss: 0.6150  avg_val_loss: 0.6431  time: 0s\n",
      "Epoch 110 - Score: 0.9125\n",
      "Epoch: [111][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6215(0.6215) Grad: 2616.2122  LR: 0.00025032  \n",
      "Epoch: [111][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6393(0.6204) Grad: 918.4449  LR: 0.00025032  \n",
      "Epoch: [111][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6410(0.6410) Grad: 0.0000  \n",
      "Epoch: [111][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6439(0.6423) Grad: 0.0000  \n",
      "Epoch 111 - avg_train_loss: 0.6204  avg_val_loss: 0.6423  time: 0s\n",
      "Epoch 111 - Score: 0.9125\n",
      "Epoch: [112][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6295(0.6295) Grad: 3955.7278  LR: 0.00025032  \n",
      "Epoch: [112][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6169) Grad: 1281.0588  LR: 0.00025032  \n",
      "Epoch: [112][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6402(0.6402) Grad: 0.0000  \n",
      "Epoch: [112][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6442(0.6421) Grad: 0.0000  \n",
      "Epoch 112 - avg_train_loss: 0.6169  avg_val_loss: 0.6421  time: 0s\n",
      "Epoch 112 - Score: 0.9125\n",
      "Epoch: [113][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6278(0.6278) Grad: 1522.5272  LR: 0.00025032  \n",
      "Epoch: [113][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6171) Grad: 1857.8662  LR: 0.00025032  \n",
      "Epoch: [113][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6428(0.6428) Grad: 0.0000  \n",
      "Epoch: [113][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6437) Grad: 0.0000  \n",
      "Epoch 113 - avg_train_loss: 0.6171  avg_val_loss: 0.6437  time: 0s\n",
      "Epoch 113 - Score: 0.9125\n",
      "Epoch: [114][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6135(0.6135) Grad: 3779.7131  LR: 0.00020276  \n",
      "Epoch: [114][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6344(0.6172) Grad: 1589.4108  LR: 0.00022528  \n",
      "Epoch: [114][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6438(0.6438) Grad: 0.0000  \n",
      "Epoch: [114][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6444) Grad: 0.0000  \n",
      "Epoch 114 - avg_train_loss: 0.6172  avg_val_loss: 0.6444  time: 0s\n",
      "Epoch 114 - Score: 0.9125\n",
      "Epoch: [115][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6291) Grad: 2520.2983  LR: 0.00022528  \n",
      "Epoch: [115][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6015(0.6161) Grad: 1781.5461  LR: 0.00022528  \n",
      "Epoch: [115][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6426(0.6426) Grad: 0.0000  \n",
      "Epoch: [115][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6437) Grad: 0.0000  \n",
      "Epoch 115 - avg_train_loss: 0.6161  avg_val_loss: 0.6437  time: 0s\n",
      "Epoch 115 - Score: 0.9125\n",
      "Epoch: [116][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5908(0.5908) Grad: 2950.1892  LR: 0.00022528  \n",
      "Epoch: [116][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6007(0.6196) Grad: 4050.2800  LR: 0.00022528  \n",
      "Epoch: [116][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6421(0.6421) Grad: 0.0000  \n",
      "Epoch: [116][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6452(0.6436) Grad: 0.0000  \n",
      "Epoch 116 - avg_train_loss: 0.6196  avg_val_loss: 0.6436  time: 0s\n",
      "Epoch 116 - Score: 0.9125\n",
      "Epoch: [117][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6217(0.6217) Grad: 3221.8779  LR: 0.00022528  \n",
      "Epoch: [117][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6275(0.6160) Grad: 3220.0769  LR: 0.00020276  \n",
      "Epoch: [117][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6422(0.6422) Grad: 0.0000  \n",
      "Epoch: [117][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6455(0.6437) Grad: 0.0000  \n",
      "Epoch 117 - avg_train_loss: 0.6160  avg_val_loss: 0.6437  time: 0s\n",
      "Epoch 117 - Score: 0.9125\n",
      "Epoch: [118][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5869(0.5869) Grad: 4160.9683  LR: 0.00020276  \n",
      "Epoch: [118][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6164(0.6180) Grad: 3269.8762  LR: 0.00020276  \n",
      "Epoch: [118][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6424(0.6424) Grad: 0.0000  \n",
      "Epoch: [118][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6459(0.6440) Grad: 0.0000  \n",
      "Epoch 118 - avg_train_loss: 0.6180  avg_val_loss: 0.6440  time: 0s\n",
      "Epoch 118 - Score: 0.9125\n",
      "Epoch: [119][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6548(0.6548) Grad: 4956.7324  LR: 0.00020276  \n",
      "Epoch: [119][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6051(0.6141) Grad: 3860.7068  LR: 0.00020276  \n",
      "Epoch: [119][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6432(0.6432) Grad: 0.0000  \n",
      "Epoch: [119][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6464(0.6447) Grad: 0.0000  \n",
      "Epoch 119 - avg_train_loss: 0.6141  avg_val_loss: 0.6447  time: 0s\n",
      "Epoch 119 - Score: 0.9083\n",
      "Epoch: [120][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6247(0.6247) Grad: 1314.4508  LR: 0.00020276  \n",
      "Epoch: [120][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6188) Grad: 3612.1257  LR: 0.00018248  \n",
      "Epoch: [120][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6450) Grad: 0.0000  \n",
      "Epoch: [120][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6477(0.6463) Grad: 0.0000  \n",
      "Epoch 120 - avg_train_loss: 0.6188  avg_val_loss: 0.6463  time: 0s\n",
      "Epoch 120 - Score: 0.9083\n",
      "Epoch: [121][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6265(0.6265) Grad: 1740.9746  LR: 0.00018248  \n",
      "Epoch: [121][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6097) Grad: 3955.9861  LR: 0.00018248  \n",
      "Epoch: [121][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6463(0.6463) Grad: 0.0000  \n",
      "Epoch: [121][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6472) Grad: 0.0000  \n",
      "Epoch 121 - avg_train_loss: 0.6097  avg_val_loss: 0.6472  time: 0s\n",
      "Epoch 121 - Score: 0.9083\n",
      "Epoch: [122][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6177(0.6177) Grad: 594.2183  LR: 0.00018248  \n",
      "Epoch: [122][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6107(0.6151) Grad: 526.6245  LR: 0.00018248  \n",
      "Epoch: [122][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6460) Grad: 0.0000  \n",
      "Epoch: [122][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6486(0.6472) Grad: 0.0000  \n",
      "Epoch 122 - avg_train_loss: 0.6151  avg_val_loss: 0.6472  time: 0s\n",
      "Epoch 122 - Score: 0.9083\n",
      "Epoch: [123][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6126(0.6126) Grad: 1283.1018  LR: 0.00018248  \n",
      "Epoch: [123][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6098(0.6120) Grad: 2089.2585  LR: 0.00016423  \n",
      "Epoch: [123][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6460) Grad: 0.0000  \n",
      "Epoch: [123][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6489(0.6473) Grad: 0.0000  \n",
      "Epoch 123 - avg_train_loss: 0.6120  avg_val_loss: 0.6473  time: 0s\n",
      "Epoch 123 - Score: 0.9083\n",
      "Epoch: [124][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6187) Grad: 1340.7794  LR: 0.00016423  \n",
      "Epoch: [124][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6098(0.6134) Grad: 3792.7668  LR: 0.00016423  \n",
      "Epoch: [124][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6472(0.6472) Grad: 0.0000  \n",
      "Epoch: [124][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6483) Grad: 0.0000  \n",
      "Epoch 124 - avg_train_loss: 0.6134  avg_val_loss: 0.6483  time: 0s\n",
      "Epoch 124 - Score: 0.9083\n",
      "Epoch: [125][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6038(0.6038) Grad: 665.8611  LR: 0.00016423  \n",
      "Epoch: [125][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6156(0.6147) Grad: 6004.0376  LR: 0.00016423  \n",
      "Epoch: [125][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6484(0.6484) Grad: 0.0000  \n",
      "Epoch: [125][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6491) Grad: 0.0000  \n",
      "Epoch 125 - avg_train_loss: 0.6147  avg_val_loss: 0.6491  time: 0s\n",
      "Epoch 125 - Score: 0.9042\n",
      "Epoch: [126][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6367(0.6367) Grad: 4384.7671  LR: 0.00016423  \n",
      "Epoch: [126][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5944(0.6180) Grad: 557.1522  LR: 0.00014781  \n",
      "Epoch: [126][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6485(0.6485) Grad: 0.0000  \n",
      "Epoch: [126][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6492) Grad: 0.0000  \n",
      "Epoch 126 - avg_train_loss: 0.6180  avg_val_loss: 0.6492  time: 0s\n",
      "Epoch 126 - Score: 0.9042\n",
      "Epoch: [127][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6259(0.6259) Grad: 3953.1003  LR: 0.00014781  \n",
      "Epoch: [127][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6136) Grad: 317.5912  LR: 0.00014781  \n",
      "Epoch: [127][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6492) Grad: 0.0000  \n",
      "Epoch: [127][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6496) Grad: 0.0000  \n",
      "Epoch 127 - avg_train_loss: 0.6136  avg_val_loss: 0.6496  time: 0s\n",
      "Epoch 127 - Score: 0.9042\n",
      "Epoch: [128][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6257(0.6257) Grad: 860.2393  LR: 0.00014781  \n",
      "Epoch: [128][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6032(0.6158) Grad: 2733.4998  LR: 0.00014781  \n",
      "Epoch: [128][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6484(0.6484) Grad: 0.0000  \n",
      "Epoch: [128][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6493) Grad: 0.0000  \n",
      "Epoch 128 - avg_train_loss: 0.6158  avg_val_loss: 0.6493  time: 0s\n",
      "Epoch 128 - Score: 0.9042\n",
      "Epoch: [129][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5727(0.5727) Grad: 5985.0947  LR: 0.00014781  \n",
      "Epoch: [129][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5946(0.6183) Grad: 731.7064  LR: 0.00013303  \n",
      "Epoch: [129][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6480) Grad: 0.0000  \n",
      "Epoch: [129][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6490) Grad: 0.0000  \n",
      "Epoch 129 - avg_train_loss: 0.6183  avg_val_loss: 0.6490  time: 0s\n",
      "Epoch 129 - Score: 0.9042\n",
      "Epoch: [130][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6015(0.6015) Grad: 683.2101  LR: 0.00013303  \n",
      "Epoch: [130][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5973(0.6092) Grad: 3357.9888  LR: 0.00013303  \n",
      "Epoch: [130][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6468(0.6468) Grad: 0.0000  \n",
      "Epoch: [130][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6484) Grad: 0.0000  \n",
      "Epoch 130 - avg_train_loss: 0.6092  avg_val_loss: 0.6484  time: 0s\n",
      "Epoch 130 - Score: 0.9083\n",
      "Epoch: [131][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5970(0.5970) Grad: 4010.5945  LR: 0.00013303  \n",
      "Epoch: [131][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6189(0.6151) Grad: 2740.3389  LR: 0.00013303  \n",
      "Epoch: [131][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6490(0.6490) Grad: 0.0000  \n",
      "Epoch: [131][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6497) Grad: 0.0000  \n",
      "Epoch 131 - avg_train_loss: 0.6151  avg_val_loss: 0.6497  time: 0s\n",
      "Epoch 131 - Score: 0.9042\n",
      "Epoch: [132][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5810(0.5810) Grad: 2036.9821  LR: 0.00013303  \n",
      "Epoch: [132][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6138(0.6129) Grad: 4565.1899  LR: 0.00010775  \n",
      "Epoch: [132][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6480) Grad: 0.0000  \n",
      "Epoch: [132][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6507(0.6492) Grad: 0.0000  \n",
      "Epoch 132 - avg_train_loss: 0.6129  avg_val_loss: 0.6492  time: 0s\n",
      "Epoch 132 - Score: 0.9042\n",
      "Epoch: [133][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5792(0.5792) Grad: 1085.4867  LR: 0.00011973  \n",
      "Epoch: [133][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6510(0.6121) Grad: 2059.2698  LR: 0.00011973  \n",
      "Epoch: [133][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6479(0.6479) Grad: 0.0000  \n",
      "Epoch: [133][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6491) Grad: 0.0000  \n",
      "Epoch 133 - avg_train_loss: 0.6121  avg_val_loss: 0.6491  time: 0s\n",
      "Epoch 133 - Score: 0.9083\n",
      "Epoch: [134][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6400(0.6400) Grad: 426.1116  LR: 0.00011973  \n",
      "Epoch: [134][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6283(0.6139) Grad: 2581.2222  LR: 0.00011973  \n",
      "Epoch: [134][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6479(0.6479) Grad: 0.0000  \n",
      "Epoch: [134][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6491) Grad: 0.0000  \n",
      "Epoch 134 - avg_train_loss: 0.6139  avg_val_loss: 0.6491  time: 0s\n",
      "Epoch 134 - Score: 0.9083\n",
      "Epoch: [135][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5972(0.5972) Grad: 1351.1049  LR: 0.00011973  \n",
      "Epoch: [135][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6420(0.6167) Grad: 819.6502  LR: 0.00011973  \n",
      "Epoch: [135][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6475(0.6475) Grad: 0.0000  \n",
      "Epoch: [135][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6488) Grad: 0.0000  \n",
      "Epoch 135 - avg_train_loss: 0.6167  avg_val_loss: 0.6488  time: 0s\n",
      "Epoch 135 - Score: 0.9083\n",
      "Epoch: [136][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6125(0.6125) Grad: 3215.6770  LR: 0.00009698  \n",
      "Epoch: [136][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6017(0.6120) Grad: 2671.3987  LR: 0.00010775  \n",
      "Epoch: [136][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6484(0.6484) Grad: 0.0000  \n",
      "Epoch: [136][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6493) Grad: 0.0000  \n",
      "Epoch 136 - avg_train_loss: 0.6120  avg_val_loss: 0.6493  time: 0s\n",
      "Epoch 136 - Score: 0.9042\n",
      "Epoch: [137][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6171(0.6171) Grad: 2509.1743  LR: 0.00010775  \n",
      "Epoch: [137][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6093) Grad: 2142.6731  LR: 0.00010775  \n",
      "Epoch: [137][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6478(0.6478) Grad: 0.0000  \n",
      "Epoch: [137][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6490) Grad: 0.0000  \n",
      "Epoch 137 - avg_train_loss: 0.6093  avg_val_loss: 0.6490  time: 0s\n",
      "Epoch 137 - Score: 0.9042\n",
      "Epoch: [138][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6182) Grad: 499.7297  LR: 0.00010775  \n",
      "Epoch: [138][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5935(0.6100) Grad: 590.1814  LR: 0.00010775  \n",
      "Epoch: [138][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6500) Grad: 0.0000  \n",
      "Epoch: [138][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6500) Grad: 0.0000  \n",
      "Epoch 138 - avg_train_loss: 0.6100  avg_val_loss: 0.6500  time: 0s\n",
      "Epoch 138 - Score: 0.9042\n",
      "Epoch: [139][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6035) Grad: 5481.5303  LR: 0.00010775  \n",
      "Epoch: [139][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6098(0.6160) Grad: 6502.8257  LR: 0.00009698  \n",
      "Epoch: [139][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [139][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6497) Grad: 0.0000  \n",
      "Epoch 139 - avg_train_loss: 0.6160  avg_val_loss: 0.6497  time: 0s\n",
      "Epoch 139 - Score: 0.9042\n",
      "Epoch: [140][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6127(0.6127) Grad: 1454.3103  LR: 0.00009698  \n",
      "Epoch: [140][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6103(0.6138) Grad: 633.8992  LR: 0.00009698  \n",
      "Epoch: [140][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [140][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6498) Grad: 0.0000  \n",
      "Epoch 140 - avg_train_loss: 0.6138  avg_val_loss: 0.6498  time: 0s\n",
      "Epoch 140 - Score: 0.9042\n",
      "Epoch: [141][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5962(0.5962) Grad: 2042.1497  LR: 0.00009698  \n",
      "Epoch: [141][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6537(0.6156) Grad: 5941.1436  LR: 0.00009698  \n",
      "Epoch: [141][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6510(0.6510) Grad: 0.0000  \n",
      "Epoch: [141][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6503) Grad: 0.0000  \n",
      "Epoch 141 - avg_train_loss: 0.6156  avg_val_loss: 0.6503  time: 0s\n",
      "Epoch 141 - Score: 0.9042\n",
      "Epoch: [142][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6105(0.6105) Grad: 679.5837  LR: 0.00009698  \n",
      "Epoch: [142][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6108(0.6158) Grad: 1235.1327  LR: 0.00008728  \n",
      "Epoch: [142][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6514(0.6514) Grad: 0.0000  \n",
      "Epoch: [142][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6506) Grad: 0.0000  \n",
      "Epoch 142 - avg_train_loss: 0.6158  avg_val_loss: 0.6506  time: 0s\n",
      "Epoch 142 - Score: 0.9042\n",
      "Epoch: [143][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 6442.3618  LR: 0.00008728  \n",
      "Epoch: [143][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6036(0.6095) Grad: 1726.1298  LR: 0.00008728  \n",
      "Epoch: [143][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6517(0.6517) Grad: 0.0000  \n",
      "Epoch: [143][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6510) Grad: 0.0000  \n",
      "Epoch 143 - avg_train_loss: 0.6095  avg_val_loss: 0.6510  time: 0s\n",
      "Epoch 143 - Score: 0.9042\n",
      "Epoch: [144][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6036(0.6036) Grad: 839.2661  LR: 0.00008728  \n",
      "Epoch: [144][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6321(0.6121) Grad: 3136.4023  LR: 0.00008728  \n",
      "Epoch: [144][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6521(0.6521) Grad: 0.0000  \n",
      "Epoch: [144][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6507(0.6514) Grad: 0.0000  \n",
      "Epoch 144 - avg_train_loss: 0.6121  avg_val_loss: 0.6514  time: 0s\n",
      "Epoch 144 - Score: 0.9042\n",
      "Epoch: [145][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6336(0.6336) Grad: 860.5953  LR: 0.00008728  \n",
      "Epoch: [145][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6258(0.6149) Grad: 464.8703  LR: 0.00007855  \n",
      "Epoch: [145][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6506(0.6506) Grad: 0.0000  \n",
      "Epoch: [145][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6507) Grad: 0.0000  \n",
      "Epoch 145 - avg_train_loss: 0.6149  avg_val_loss: 0.6507  time: 0s\n",
      "Epoch 145 - Score: 0.9042\n",
      "Epoch: [146][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6335(0.6335) Grad: 5671.7817  LR: 0.00007855  \n",
      "Epoch: [146][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6486(0.6136) Grad: 1810.1394  LR: 0.00007855  \n",
      "Epoch: [146][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6494) Grad: 0.0000  \n",
      "Epoch: [146][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6506(0.6500) Grad: 0.0000  \n",
      "Epoch 146 - avg_train_loss: 0.6136  avg_val_loss: 0.6500  time: 0s\n",
      "Epoch 146 - Score: 0.9042\n",
      "Epoch: [147][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6282(0.6282) Grad: 694.8422  LR: 0.00007855  \n",
      "Epoch: [147][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6110(0.6140) Grad: 1184.9131  LR: 0.00007855  \n",
      "Epoch: [147][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6491) Grad: 0.0000  \n",
      "Epoch: [147][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6496) Grad: 0.0000  \n",
      "Epoch 147 - avg_train_loss: 0.6140  avg_val_loss: 0.6496  time: 0s\n",
      "Epoch 147 - Score: 0.9042\n",
      "Epoch: [148][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6286(0.6286) Grad: 2826.8677  LR: 0.00007855  \n",
      "Epoch: [148][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6401(0.6129) Grad: 6295.8750  LR: 0.00007070  \n",
      "Epoch: [148][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [148][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6496) Grad: 0.0000  \n",
      "Epoch 148 - avg_train_loss: 0.6129  avg_val_loss: 0.6496  time: 0s\n",
      "Epoch 148 - Score: 0.9042\n",
      "Epoch: [149][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6612(0.6612) Grad: 5063.5093  LR: 0.00007070  \n",
      "Epoch: [149][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5884(0.6135) Grad: 757.3643  LR: 0.00007070  \n",
      "Epoch: [149][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6492) Grad: 0.0000  \n",
      "Epoch: [149][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6494) Grad: 0.0000  \n",
      "Epoch 149 - avg_train_loss: 0.6135  avg_val_loss: 0.6494  time: 0s\n",
      "Epoch 149 - Score: 0.9042\n",
      "Epoch: [150][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6036(0.6036) Grad: 650.4327  LR: 0.00007070  \n",
      "Epoch: [150][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6114(0.6137) Grad: 3804.3062  LR: 0.00007070  \n",
      "Epoch: [150][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6485(0.6485) Grad: 0.0000  \n",
      "Epoch: [150][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6492) Grad: 0.0000  \n",
      "Epoch 150 - avg_train_loss: 0.6137  avg_val_loss: 0.6492  time: 0s\n",
      "Epoch 150 - Score: 0.9042\n",
      "Epoch: [151][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6170(0.6170) Grad: 2288.3779  LR: 0.00007070  \n",
      "Epoch: [151][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5942(0.6086) Grad: 403.6286  LR: 0.00006363  \n",
      "Epoch: [151][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6497) Grad: 0.0000  \n",
      "Epoch: [151][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6498) Grad: 0.0000  \n",
      "Epoch 151 - avg_train_loss: 0.6086  avg_val_loss: 0.6498  time: 0s\n",
      "Epoch 151 - Score: 0.9042\n",
      "Epoch: [152][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5839(0.5839) Grad: 3420.0759  LR: 0.00006363  \n",
      "Epoch: [152][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6266(0.6107) Grad: 1005.2933  LR: 0.00006363  \n",
      "Epoch: [152][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6505) Grad: 0.0000  \n",
      "Epoch: [152][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6501) Grad: 0.0000  \n",
      "Epoch 152 - avg_train_loss: 0.6107  avg_val_loss: 0.6501  time: 0s\n",
      "Epoch 152 - Score: 0.9042\n",
      "Epoch: [153][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6157(0.6157) Grad: 3681.1653  LR: 0.00006363  \n",
      "Epoch: [153][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6227(0.6138) Grad: 7520.9712  LR: 0.00006363  \n",
      "Epoch: [153][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6503) Grad: 0.0000  \n",
      "Epoch: [153][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6501) Grad: 0.0000  \n",
      "Epoch 153 - avg_train_loss: 0.6138  avg_val_loss: 0.6501  time: 0s\n",
      "Epoch 153 - Score: 0.9042\n",
      "Epoch: [154][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6366(0.6366) Grad: 3366.1313  LR: 0.00006363  \n",
      "Epoch: [154][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6153(0.6046) Grad: 2642.0122  LR: 0.00005154  \n",
      "Epoch: [154][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [154][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6499) Grad: 0.0000  \n",
      "Epoch 154 - avg_train_loss: 0.6046  avg_val_loss: 0.6499  time: 0s\n",
      "Epoch 154 - Score: 0.9042\n",
      "Epoch: [155][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6034(0.6034) Grad: 747.4291  LR: 0.00005726  \n",
      "Epoch: [155][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5804(0.6113) Grad: 1431.0748  LR: 0.00005726  \n",
      "Epoch: [155][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6506(0.6506) Grad: 0.0000  \n",
      "Epoch: [155][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6493(0.6500) Grad: 0.0000  \n",
      "Epoch 155 - avg_train_loss: 0.6113  avg_val_loss: 0.6500  time: 0s\n",
      "Epoch 155 - Score: 0.9042\n",
      "Epoch: [156][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6048(0.6048) Grad: 1512.2491  LR: 0.00005726  \n",
      "Epoch: [156][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6306(0.6154) Grad: 3927.7124  LR: 0.00005726  \n",
      "Epoch: [156][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6515(0.6515) Grad: 0.0000  \n",
      "Epoch: [156][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6504) Grad: 0.0000  \n",
      "Epoch 156 - avg_train_loss: 0.6154  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 156 - Score: 0.9042\n",
      "Epoch: [157][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5876(0.5876) Grad: 582.2932  LR: 0.00005726  \n",
      "Epoch: [157][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6468(0.6117) Grad: 1731.5347  LR: 0.00005726  \n",
      "Epoch: [157][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6518(0.6518) Grad: 0.0000  \n",
      "Epoch: [157][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6488(0.6504) Grad: 0.0000  \n",
      "Epoch 157 - avg_train_loss: 0.6117  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 157 - Score: 0.9042\n",
      "Epoch: [158][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6114(0.6114) Grad: 5032.9189  LR: 0.00004638  \n",
      "Epoch: [158][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6368(0.6145) Grad: 5096.7871  LR: 0.00005154  \n",
      "Epoch: [158][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6522(0.6522) Grad: 0.0000  \n",
      "Epoch: [158][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6487(0.6506) Grad: 0.0000  \n",
      "Epoch 158 - avg_train_loss: 0.6145  avg_val_loss: 0.6506  time: 0s\n",
      "Epoch 158 - Score: 0.9042\n",
      "Epoch: [159][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6176(0.6176) Grad: 3575.9602  LR: 0.00005154  \n",
      "Epoch: [159][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6133(0.6131) Grad: 4513.9453  LR: 0.00005154  \n",
      "Epoch: [159][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6522(0.6522) Grad: 0.0000  \n",
      "Epoch: [159][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6487(0.6505) Grad: 0.0000  \n",
      "Epoch 159 - avg_train_loss: 0.6131  avg_val_loss: 0.6505  time: 0s\n",
      "Epoch 159 - Score: 0.9042\n",
      "Epoch: [160][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 4003.1790  LR: 0.00005154  \n",
      "Epoch: [160][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6248(0.6168) Grad: 2553.3594  LR: 0.00005154  \n",
      "Epoch: [160][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6511(0.6511) Grad: 0.0000  \n",
      "Epoch: [160][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6488(0.6500) Grad: 0.0000  \n",
      "Epoch 160 - avg_train_loss: 0.6168  avg_val_loss: 0.6500  time: 0s\n",
      "Epoch 160 - Score: 0.9042\n",
      "Epoch: [161][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6420(0.6420) Grad: 4225.2500  LR: 0.00005154  \n",
      "Epoch: [161][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6215(0.6131) Grad: 2980.5457  LR: 0.00004638  \n",
      "Epoch: [161][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6513) Grad: 0.0000  \n",
      "Epoch: [161][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6503) Grad: 0.0000  \n",
      "Epoch 161 - avg_train_loss: 0.6131  avg_val_loss: 0.6503  time: 0s\n",
      "Epoch 161 - Score: 0.9042\n",
      "Epoch: [162][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6264(0.6264) Grad: 1681.1742  LR: 0.00004638  \n",
      "Epoch: [162][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5725(0.6143) Grad: 1413.3087  LR: 0.00004638  \n",
      "Epoch: [162][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [162][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch 162 - avg_train_loss: 0.6143  avg_val_loss: 0.6498  time: 0s\n",
      "Epoch 162 - Score: 0.9042\n",
      "Epoch: [163][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5653(0.5653) Grad: 2103.2717  LR: 0.00004638  \n",
      "Epoch: [163][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6115(0.6081) Grad: 1664.6261  LR: 0.00004638  \n",
      "Epoch: [163][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [163][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6502) Grad: 0.0000  \n",
      "Epoch 163 - avg_train_loss: 0.6081  avg_val_loss: 0.6502  time: 0s\n",
      "Epoch 163 - Score: 0.9042\n",
      "Epoch: [164][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6096) Grad: 1239.5171  LR: 0.00004638  \n",
      "Epoch: [164][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6277(0.6124) Grad: 3470.7861  LR: 0.00004175  \n",
      "Epoch: [164][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6505) Grad: 0.0000  \n",
      "Epoch: [164][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6504) Grad: 0.0000  \n",
      "Epoch 164 - avg_train_loss: 0.6124  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 164 - Score: 0.9042\n",
      "Epoch: [165][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6567(0.6567) Grad: 5059.2495  LR: 0.00004175  \n",
      "Epoch: [165][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6049(0.6093) Grad: 3970.5549  LR: 0.00004175  \n",
      "Epoch: [165][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6522(0.6522) Grad: 0.0000  \n",
      "Epoch: [165][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6513) Grad: 0.0000  \n",
      "Epoch 165 - avg_train_loss: 0.6093  avg_val_loss: 0.6513  time: 0s\n",
      "Epoch 165 - Score: 0.9042\n",
      "Epoch: [166][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5891(0.5891) Grad: 2427.7642  LR: 0.00004175  \n",
      "Epoch: [166][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6147) Grad: 8148.6064  LR: 0.00004175  \n",
      "Epoch: [166][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [166][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6504) Grad: 0.0000  \n",
      "Epoch 166 - avg_train_loss: 0.6147  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 166 - Score: 0.9042\n",
      "Epoch: [167][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5951(0.5951) Grad: 887.5891  LR: 0.00004175  \n",
      "Epoch: [167][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6056(0.6150) Grad: 2451.2173  LR: 0.00003757  \n",
      "Epoch: [167][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6508) Grad: 0.0000  \n",
      "Epoch: [167][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6507(0.6508) Grad: 0.0000  \n",
      "Epoch 167 - avg_train_loss: 0.6150  avg_val_loss: 0.6508  time: 0s\n",
      "Epoch 167 - Score: 0.9042\n",
      "Epoch: [168][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5974(0.5974) Grad: 2629.0435  LR: 0.00003757  \n",
      "Epoch: [168][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6119) Grad: 1043.6041  LR: 0.00003757  \n",
      "Epoch: [168][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6513) Grad: 0.0000  \n",
      "Epoch: [168][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6511) Grad: 0.0000  \n",
      "Epoch 168 - avg_train_loss: 0.6119  avg_val_loss: 0.6511  time: 0s\n",
      "Epoch 168 - Score: 0.9042\n",
      "Epoch: [169][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6642(0.6642) Grad: 5453.7427  LR: 0.00003757  \n",
      "Epoch: [169][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6197) Grad: 624.8051  LR: 0.00003757  \n",
      "Epoch: [169][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6512(0.6512) Grad: 0.0000  \n",
      "Epoch: [169][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6506(0.6509) Grad: 0.0000  \n",
      "Epoch 169 - avg_train_loss: 0.6197  avg_val_loss: 0.6509  time: 0s\n",
      "Epoch 169 - Score: 0.9042\n",
      "Epoch: [170][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5948(0.5948) Grad: 941.3859  LR: 0.00003757  \n",
      "Epoch: [170][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6159) Grad: 3447.2615  LR: 0.00003381  \n",
      "Epoch: [170][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6506(0.6506) Grad: 0.0000  \n",
      "Epoch: [170][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6506(0.6506) Grad: 0.0000  \n",
      "Epoch 170 - avg_train_loss: 0.6159  avg_val_loss: 0.6506  time: 0s\n",
      "Epoch 170 - Score: 0.9042\n",
      "Epoch: [171][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6130(0.6130) Grad: 4970.9395  LR: 0.00003381  \n",
      "Epoch: [171][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5894(0.6120) Grad: 3012.8940  LR: 0.00003381  \n",
      "Epoch: [171][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6509(0.6509) Grad: 0.0000  \n",
      "Epoch: [171][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6506) Grad: 0.0000  \n",
      "Epoch 171 - avg_train_loss: 0.6120  avg_val_loss: 0.6506  time: 0s\n",
      "Epoch 171 - Score: 0.9042\n",
      "Epoch: [172][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5884(0.5884) Grad: 2506.6553  LR: 0.00003381  \n",
      "Epoch: [172][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6197(0.6102) Grad: 1737.9541  LR: 0.00003381  \n",
      "Epoch: [172][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6519(0.6519) Grad: 0.0000  \n",
      "Epoch: [172][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6510) Grad: 0.0000  \n",
      "Epoch 172 - avg_train_loss: 0.6102  avg_val_loss: 0.6510  time: 0s\n",
      "Epoch 172 - Score: 0.9042\n",
      "Epoch: [173][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6073(0.6073) Grad: 3348.8599  LR: 0.00003381  \n",
      "Epoch: [173][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6292(0.6146) Grad: 5534.9111  LR: 0.00003043  \n",
      "Epoch: [173][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6521(0.6521) Grad: 0.0000  \n",
      "Epoch: [173][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6510) Grad: 0.0000  \n",
      "Epoch 173 - avg_train_loss: 0.6146  avg_val_loss: 0.6510  time: 0s\n",
      "Epoch 173 - Score: 0.9042\n",
      "Epoch: [174][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6448) Grad: 5065.4009  LR: 0.00003043  \n",
      "Epoch: [174][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6489(0.6202) Grad: 1394.5688  LR: 0.00003043  \n",
      "Epoch: [174][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6513) Grad: 0.0000  \n",
      "Epoch: [174][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6505) Grad: 0.0000  \n",
      "Epoch 174 - avg_train_loss: 0.6202  avg_val_loss: 0.6505  time: 0s\n",
      "Epoch 174 - Score: 0.9042\n",
      "Epoch: [175][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5839(0.5839) Grad: 2209.0361  LR: 0.00003043  \n",
      "Epoch: [175][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6256(0.6107) Grad: 2342.8381  LR: 0.00003043  \n",
      "Epoch: [175][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6508) Grad: 0.0000  \n",
      "Epoch: [175][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6502) Grad: 0.0000  \n",
      "Epoch 175 - avg_train_loss: 0.6107  avg_val_loss: 0.6502  time: 0s\n",
      "Epoch 175 - Score: 0.9042\n",
      "Epoch: [176][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6176(0.6176) Grad: 651.2950  LR: 0.00003043  \n",
      "Epoch: [176][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5788(0.6101) Grad: 583.9080  LR: 0.00002465  \n",
      "Epoch: [176][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6512(0.6512) Grad: 0.0000  \n",
      "Epoch: [176][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6503) Grad: 0.0000  \n",
      "Epoch 176 - avg_train_loss: 0.6101  avg_val_loss: 0.6503  time: 0s\n",
      "Epoch 176 - Score: 0.9042\n",
      "Epoch: [177][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 1736.1898  LR: 0.00002739  \n",
      "Epoch: [177][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6309(0.6113) Grad: 3177.1663  LR: 0.00002739  \n",
      "Epoch: [177][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6520(0.6520) Grad: 0.0000  \n",
      "Epoch: [177][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6508) Grad: 0.0000  \n",
      "Epoch 177 - avg_train_loss: 0.6113  avg_val_loss: 0.6508  time: 0s\n",
      "Epoch 177 - Score: 0.9042\n",
      "Epoch: [178][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6170(0.6170) Grad: 4380.3267  LR: 0.00002739  \n",
      "Epoch: [178][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6089(0.6110) Grad: 627.7462  LR: 0.00002739  \n",
      "Epoch: [178][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6513) Grad: 0.0000  \n",
      "Epoch: [178][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6505) Grad: 0.0000  \n",
      "Epoch 178 - avg_train_loss: 0.6110  avg_val_loss: 0.6505  time: 0s\n",
      "Epoch 178 - Score: 0.9042\n",
      "Epoch: [179][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6036(0.6036) Grad: 553.1172  LR: 0.00002739  \n",
      "Epoch: [179][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5957(0.6096) Grad: 2646.5283  LR: 0.00002739  \n",
      "Epoch: [179][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6515(0.6515) Grad: 0.0000  \n",
      "Epoch: [179][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6507) Grad: 0.0000  \n",
      "Epoch 179 - avg_train_loss: 0.6096  avg_val_loss: 0.6507  time: 0s\n",
      "Epoch 179 - Score: 0.9042\n",
      "Epoch: [180][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 1062.5624  LR: 0.00002219  \n",
      "Epoch: [180][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6003(0.6150) Grad: 5070.5200  LR: 0.00002465  \n",
      "Epoch: [180][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6512(0.6512) Grad: 0.0000  \n",
      "Epoch: [180][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6506) Grad: 0.0000  \n",
      "Epoch 180 - avg_train_loss: 0.6150  avg_val_loss: 0.6506  time: 0s\n",
      "Epoch 180 - Score: 0.9042\n",
      "Epoch: [181][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6207(0.6207) Grad: 2402.5532  LR: 0.00002465  \n",
      "Epoch: [181][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6015(0.6165) Grad: 953.4992  LR: 0.00002465  \n",
      "Epoch: [181][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6508) Grad: 0.0000  \n",
      "Epoch: [181][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6504) Grad: 0.0000  \n",
      "Epoch 181 - avg_train_loss: 0.6165  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 181 - Score: 0.9042\n",
      "Epoch: [182][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5842(0.5842) Grad: 5961.7603  LR: 0.00002465  \n",
      "Epoch: [182][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6252(0.6136) Grad: 4174.0508  LR: 0.00002465  \n",
      "Epoch: [182][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [182][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6502) Grad: 0.0000  \n",
      "Epoch 182 - avg_train_loss: 0.6136  avg_val_loss: 0.6502  time: 0s\n",
      "Epoch 182 - Score: 0.9042\n",
      "Epoch: [183][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6363(0.6363) Grad: 3726.6917  LR: 0.00002465  \n",
      "Epoch: [183][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6318(0.6118) Grad: 1198.1115  LR: 0.00002219  \n",
      "Epoch: [183][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6487(0.6487) Grad: 0.0000  \n",
      "Epoch: [183][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6495) Grad: 0.0000  \n",
      "Epoch 183 - avg_train_loss: 0.6118  avg_val_loss: 0.6495  time: 0s\n",
      "Epoch 183 - Score: 0.9042\n",
      "Epoch: [184][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6169(0.6169) Grad: 6035.3354  LR: 0.00002219  \n",
      "Epoch: [184][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5800(0.6121) Grad: 575.3964  LR: 0.00002219  \n",
      "Epoch: [184][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6488(0.6488) Grad: 0.0000  \n",
      "Epoch: [184][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6496) Grad: 0.0000  \n",
      "Epoch 184 - avg_train_loss: 0.6121  avg_val_loss: 0.6496  time: 0s\n",
      "Epoch 184 - Score: 0.9042\n",
      "Epoch: [185][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5978(0.5978) Grad: 3964.8413  LR: 0.00002219  \n",
      "Epoch: [185][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6171(0.6158) Grad: 1261.3706  LR: 0.00002219  \n",
      "Epoch: [185][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6492) Grad: 0.0000  \n",
      "Epoch: [185][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6497) Grad: 0.0000  \n",
      "Epoch 185 - avg_train_loss: 0.6158  avg_val_loss: 0.6497  time: 0s\n",
      "Epoch 185 - Score: 0.9042\n",
      "Epoch: [186][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6130(0.6130) Grad: 2877.4414  LR: 0.00002219  \n",
      "Epoch: [186][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6130(0.6135) Grad: 2485.7185  LR: 0.00001997  \n",
      "Epoch: [186][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [186][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6500) Grad: 0.0000  \n",
      "Epoch 186 - avg_train_loss: 0.6135  avg_val_loss: 0.6500  time: 0s\n",
      "Epoch 186 - Score: 0.9042\n",
      "Epoch: [187][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6019(0.6019) Grad: 1906.2909  LR: 0.00001997  \n",
      "Epoch: [187][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5859(0.6078) Grad: 3268.3699  LR: 0.00001997  \n",
      "Epoch: [187][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6505) Grad: 0.0000  \n",
      "Epoch: [187][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6504) Grad: 0.0000  \n",
      "Epoch 187 - avg_train_loss: 0.6078  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 187 - Score: 0.9042\n",
      "Epoch: [188][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6005(0.6005) Grad: 3084.0991  LR: 0.00001997  \n",
      "Epoch: [188][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6409(0.6148) Grad: 4714.0874  LR: 0.00001997  \n",
      "Epoch: [188][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6501) Grad: 0.0000  \n",
      "Epoch: [188][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6503) Grad: 0.0000  \n",
      "Epoch 188 - avg_train_loss: 0.6148  avg_val_loss: 0.6503  time: 0s\n",
      "Epoch 188 - Score: 0.9042\n",
      "Epoch: [189][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6317(0.6317) Grad: 6102.3315  LR: 0.00001997  \n",
      "Epoch: [189][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6397(0.6134) Grad: 1721.0374  LR: 0.00001797  \n",
      "Epoch: [189][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6502) Grad: 0.0000  \n",
      "Epoch: [189][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6503) Grad: 0.0000  \n",
      "Epoch 189 - avg_train_loss: 0.6134  avg_val_loss: 0.6503  time: 0s\n",
      "Epoch 189 - Score: 0.9042\n",
      "Epoch: [190][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6176(0.6176) Grad: 2848.2849  LR: 0.00001797  \n",
      "Epoch: [190][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5873(0.6145) Grad: 793.2056  LR: 0.00001797  \n",
      "Epoch: [190][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6495(0.6495) Grad: 0.0000  \n",
      "Epoch: [190][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6499) Grad: 0.0000  \n",
      "Epoch 190 - avg_train_loss: 0.6145  avg_val_loss: 0.6499  time: 0s\n",
      "Epoch 190 - Score: 0.9042\n",
      "Epoch: [191][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5806(0.5806) Grad: 1057.6312  LR: 0.00001797  \n",
      "Epoch: [191][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6109) Grad: 6021.3774  LR: 0.00001797  \n",
      "Epoch: [191][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [191][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6500) Grad: 0.0000  \n",
      "Epoch 191 - avg_train_loss: 0.6109  avg_val_loss: 0.6500  time: 0s\n",
      "Epoch 191 - Score: 0.9042\n",
      "Epoch: [192][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6191(0.6191) Grad: 1224.3881  LR: 0.00001797  \n",
      "Epoch: [192][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6014(0.6134) Grad: 1901.0540  LR: 0.00001617  \n",
      "Epoch: [192][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6498) Grad: 0.0000  \n",
      "Epoch: [192][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6499) Grad: 0.0000  \n",
      "Epoch 192 - avg_train_loss: 0.6134  avg_val_loss: 0.6499  time: 0s\n",
      "Epoch 192 - Score: 0.9042\n",
      "Epoch: [193][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6265(0.6265) Grad: 1207.1843  LR: 0.00001617  \n",
      "Epoch: [193][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6144) Grad: 1721.0070  LR: 0.00001617  \n",
      "Epoch: [193][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6499) Grad: 0.0000  \n",
      "Epoch: [193][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6500) Grad: 0.0000  \n",
      "Epoch 193 - avg_train_loss: 0.6144  avg_val_loss: 0.6500  time: 0s\n",
      "Epoch 193 - Score: 0.9042\n",
      "Epoch: [194][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5987(0.5987) Grad: 5393.8037  LR: 0.00001617  \n",
      "Epoch: [194][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6344(0.6157) Grad: 2768.1956  LR: 0.00001617  \n",
      "Epoch: [194][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6504) Grad: 0.0000  \n",
      "Epoch: [194][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6502) Grad: 0.0000  \n",
      "Epoch 194 - avg_train_loss: 0.6157  avg_val_loss: 0.6502  time: 0s\n",
      "Epoch 194 - Score: 0.9042\n",
      "Epoch: [195][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5840(0.5840) Grad: 8209.5879  LR: 0.00001617  \n",
      "Epoch: [195][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6222(0.6152) Grad: 4841.0864  LR: 0.00001456  \n",
      "Epoch: [195][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6507(0.6507) Grad: 0.0000  \n",
      "Epoch: [195][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6502) Grad: 0.0000  \n",
      "Epoch 195 - avg_train_loss: 0.6152  avg_val_loss: 0.6502  time: 0s\n",
      "Epoch 195 - Score: 0.9042\n",
      "Epoch: [196][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5934(0.5934) Grad: 942.6516  LR: 0.00001456  \n",
      "Epoch: [196][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6110) Grad: 1368.0619  LR: 0.00001456  \n",
      "Epoch: [196][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6508) Grad: 0.0000  \n",
      "Epoch: [196][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6504) Grad: 0.0000  \n",
      "Epoch 196 - avg_train_loss: 0.6110  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 196 - Score: 0.9042\n",
      "Epoch: [197][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6103(0.6103) Grad: 1236.2833  LR: 0.00001456  \n",
      "Epoch: [197][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6031(0.6112) Grad: 3590.1167  LR: 0.00001456  \n",
      "Epoch: [197][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6513(0.6513) Grad: 0.0000  \n",
      "Epoch: [197][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6507) Grad: 0.0000  \n",
      "Epoch 197 - avg_train_loss: 0.6112  avg_val_loss: 0.6507  time: 0s\n",
      "Epoch 197 - Score: 0.9042\n",
      "Epoch: [198][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6322(0.6322) Grad: 3320.3750  LR: 0.00001456  \n",
      "Epoch: [198][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5822(0.6152) Grad: 2199.7866  LR: 0.00001179  \n",
      "Epoch: [198][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6518(0.6518) Grad: 0.0000  \n",
      "Epoch: [198][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6508) Grad: 0.0000  \n",
      "Epoch 198 - avg_train_loss: 0.6152  avg_val_loss: 0.6508  time: 0s\n",
      "Epoch 198 - Score: 0.9042\n",
      "Epoch: [199][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6486(0.6486) Grad: 1727.2764  LR: 0.00001310  \n",
      "Epoch: [199][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6259(0.6148) Grad: 455.5913  LR: 0.00001310  \n",
      "Epoch: [199][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6507(0.6507) Grad: 0.0000  \n",
      "Epoch: [199][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6503) Grad: 0.0000  \n",
      "Epoch 199 - avg_train_loss: 0.6148  avg_val_loss: 0.6503  time: 0s\n",
      "Epoch 199 - Score: 0.9042\n",
      "Epoch: [200][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6048(0.6048) Grad: 5401.3857  LR: 0.00001310  \n",
      "Epoch: [200][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6192(0.6129) Grad: 1395.7898  LR: 0.00001310  \n",
      "Epoch: [200][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6511(0.6511) Grad: 0.0000  \n",
      "Epoch: [200][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6504) Grad: 0.0000  \n",
      "Epoch 200 - avg_train_loss: 0.6129  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 200 - Score: 0.9042\n",
      "========== fold: 2 result ==========\n",
      "Score: 0.9292\n",
      "========== fold: 3 training ==========\n",
      "Epoch: [1][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 1.1533(1.1533) Grad: 245460.9375  LR: 0.01000000  \n",
      "Epoch: [1][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8891(0.9550) Grad: 14759.6299  LR: 0.01000000  \n",
      "Epoch: [1][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.3576(1.3576) Grad: 0.0000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4220(1.3877) Grad: 0.0000  \n",
      "Epoch 1 - avg_train_loss: 0.9550  avg_val_loss: 1.3877  time: 0s\n",
      "Epoch 1 - Score: 0.0875\n",
      "Epoch 1 - Save Best Score: 0.0875 Model\n",
      "Epoch: [2][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8576(0.8576) Grad: 15362.8984  LR: 0.01000000  \n",
      "Epoch: [2][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8601(0.8676) Grad: 15113.0430  LR: 0.01000000  \n",
      "Epoch: [2][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4412(1.4412) Grad: 0.0000  \n",
      "Epoch: [2][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.5148(1.4756) Grad: 0.0000  \n",
      "Epoch 2 - avg_train_loss: 0.8676  avg_val_loss: 1.4756  time: 0s\n",
      "Epoch 2 - Score: 0.0750\n",
      "Epoch: [3][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8277(0.8277) Grad: 14232.0439  LR: 0.01000000  \n",
      "Epoch: [3][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7801(0.8273) Grad: 12308.4238  LR: 0.01000000  \n",
      "Epoch: [3][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.0105(1.0105) Grad: 0.0000  \n",
      "Epoch: [3][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.9963(1.0039) Grad: 0.0000  \n",
      "Epoch 3 - avg_train_loss: 0.8273  avg_val_loss: 1.0039  time: 0s\n",
      "Epoch 3 - Score: 0.6667\n",
      "Epoch 3 - Save Best Score: 0.6667 Model\n",
      "Epoch: [4][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8197(0.8197) Grad: 13284.9893  LR: 0.00810000  \n",
      "Epoch: [4][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7673(0.8030) Grad: 11538.5488  LR: 0.00900000  \n",
      "Epoch: [4][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.0263(1.0263) Grad: 0.0000  \n",
      "Epoch: [4][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.0029(1.0153) Grad: 0.0000  \n",
      "Epoch 4 - avg_train_loss: 0.8030  avg_val_loss: 1.0153  time: 0s\n",
      "Epoch 4 - Score: 0.6083\n",
      "Epoch: [5][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7699(0.7699) Grad: 11276.8096  LR: 0.00900000  \n",
      "Epoch: [5][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7944(0.7780) Grad: 10685.9160  LR: 0.00900000  \n",
      "Epoch: [5][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8313(0.8313) Grad: 0.0000  \n",
      "Epoch: [5][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7961(0.8149) Grad: 0.0000  \n",
      "Epoch 5 - avg_train_loss: 0.7780  avg_val_loss: 0.8149  time: 0s\n",
      "Epoch 5 - Score: 0.8458\n",
      "Epoch 5 - Save Best Score: 0.8458 Model\n",
      "Epoch: [6][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7694(0.7694) Grad: 9767.7695  LR: 0.00900000  \n",
      "Epoch: [6][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7378(0.7572) Grad: 10708.2119  LR: 0.00900000  \n",
      "Epoch: [6][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.1197(1.1197) Grad: 0.0000  \n",
      "Epoch: [6][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.0827(1.1024) Grad: 0.0000  \n",
      "Epoch 6 - avg_train_loss: 0.7572  avg_val_loss: 1.1024  time: 0s\n",
      "Epoch 6 - Score: 0.4167\n",
      "Epoch: [7][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7604(0.7604) Grad: 9157.0781  LR: 0.00900000  \n",
      "Epoch: [7][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7151(0.7389) Grad: 7808.4380  LR: 0.00810000  \n",
      "Epoch: [7][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8324(0.8324) Grad: 0.0000  \n",
      "Epoch: [7][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8176(0.8255) Grad: 0.0000  \n",
      "Epoch 7 - avg_train_loss: 0.7389  avg_val_loss: 0.8255  time: 0s\n",
      "Epoch 7 - Score: 0.8833\n",
      "Epoch 7 - Save Best Score: 0.8833 Model\n",
      "Epoch: [8][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7213(0.7213) Grad: 8482.3438  LR: 0.00810000  \n",
      "Epoch: [8][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7346(0.7175) Grad: 6718.7354  LR: 0.00810000  \n",
      "Epoch: [8][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7874(0.7874) Grad: 0.0000  \n",
      "Epoch: [8][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7690(0.7788) Grad: 0.0000  \n",
      "Epoch 8 - avg_train_loss: 0.7175  avg_val_loss: 0.7788  time: 0s\n",
      "Epoch 8 - Score: 0.8833\n",
      "Epoch: [9][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7163(0.7163) Grad: 6881.7876  LR: 0.00810000  \n",
      "Epoch: [9][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7268(0.7085) Grad: 7487.2480  LR: 0.00810000  \n",
      "Epoch: [9][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8227(0.8227) Grad: 0.0000  \n",
      "Epoch: [9][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7982(0.8112) Grad: 0.0000  \n",
      "Epoch 9 - avg_train_loss: 0.7085  avg_val_loss: 0.8112  time: 0s\n",
      "Epoch 9 - Score: 0.8583\n",
      "Epoch: [10][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6814(0.6814) Grad: 6057.0015  LR: 0.00810000  \n",
      "Epoch: [10][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7107(0.6995) Grad: 7156.5664  LR: 0.00729000  \n",
      "Epoch: [10][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7603(0.7603) Grad: 0.0000  \n",
      "Epoch: [10][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7326(0.7474) Grad: 0.0000  \n",
      "Epoch 10 - avg_train_loss: 0.6995  avg_val_loss: 0.7474  time: 0s\n",
      "Epoch 10 - Score: 0.8708\n",
      "Epoch: [11][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6870(0.6870) Grad: 5131.1123  LR: 0.00729000  \n",
      "Epoch: [11][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6680(0.6888) Grad: 5868.9414  LR: 0.00729000  \n",
      "Epoch: [11][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7626(0.7626) Grad: 0.0000  \n",
      "Epoch: [11][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7230(0.7441) Grad: 0.0000  \n",
      "Epoch 11 - avg_train_loss: 0.6888  avg_val_loss: 0.7441  time: 0s\n",
      "Epoch 11 - Score: 0.8458\n",
      "Epoch: [12][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6701(0.6701) Grad: 5134.7661  LR: 0.00729000  \n",
      "Epoch: [12][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6879(0.6820) Grad: 3996.9155  LR: 0.00729000  \n",
      "Epoch: [12][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7520(0.7520) Grad: 0.0000  \n",
      "Epoch: [12][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7169(0.7356) Grad: 0.0000  \n",
      "Epoch 12 - avg_train_loss: 0.6820  avg_val_loss: 0.7356  time: 0s\n",
      "Epoch 12 - Score: 0.8583\n",
      "Epoch: [13][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6610(0.6610) Grad: 4038.2258  LR: 0.00729000  \n",
      "Epoch: [13][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7040(0.6767) Grad: 3504.4009  LR: 0.00656100  \n",
      "Epoch: [13][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7423(0.7423) Grad: 0.0000  \n",
      "Epoch: [13][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7063(0.7255) Grad: 0.0000  \n",
      "Epoch 13 - avg_train_loss: 0.6767  avg_val_loss: 0.7255  time: 0s\n",
      "Epoch 13 - Score: 0.8708\n",
      "Epoch: [14][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6953(0.6953) Grad: 4375.0820  LR: 0.00656100  \n",
      "Epoch: [14][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6823(0.6723) Grad: 4047.9221  LR: 0.00656100  \n",
      "Epoch: [14][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8374(0.8374) Grad: 0.0000  \n",
      "Epoch: [14][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7832(0.8121) Grad: 0.0000  \n",
      "Epoch 14 - avg_train_loss: 0.6723  avg_val_loss: 0.8121  time: 0s\n",
      "Epoch 14 - Score: 0.7917\n",
      "Epoch: [15][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6826(0.6826) Grad: 3316.3535  LR: 0.00656100  \n",
      "Epoch: [15][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6782(0.6639) Grad: 3928.0659  LR: 0.00656100  \n",
      "Epoch: [15][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7123(0.7123) Grad: 0.0000  \n",
      "Epoch: [15][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6892(0.7015) Grad: 0.0000  \n",
      "Epoch 15 - avg_train_loss: 0.6639  avg_val_loss: 0.7015  time: 0s\n",
      "Epoch 15 - Score: 0.8833\n",
      "Epoch: [16][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6442(0.6442) Grad: 3157.5498  LR: 0.00656100  \n",
      "Epoch: [16][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6944(0.6684) Grad: 2281.0283  LR: 0.00590490  \n",
      "Epoch: [16][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7910(0.7910) Grad: 0.0000  \n",
      "Epoch: [16][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7561(0.7747) Grad: 0.0000  \n",
      "Epoch 16 - avg_train_loss: 0.6684  avg_val_loss: 0.7747  time: 0s\n",
      "Epoch 16 - Score: 0.8292\n",
      "Epoch: [17][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6266(0.6266) Grad: 2988.7632  LR: 0.00590490  \n",
      "Epoch: [17][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6666(0.6607) Grad: 3636.6492  LR: 0.00590490  \n",
      "Epoch: [17][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7323(0.7323) Grad: 0.0000  \n",
      "Epoch: [17][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6978(0.7162) Grad: 0.0000  \n",
      "Epoch 17 - avg_train_loss: 0.6607  avg_val_loss: 0.7162  time: 0s\n",
      "Epoch 17 - Score: 0.8542\n",
      "Epoch: [18][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6594(0.6594) Grad: 2583.7336  LR: 0.00590490  \n",
      "Epoch: [18][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6832(0.6575) Grad: 2256.8665  LR: 0.00590490  \n",
      "Epoch: [18][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7764(0.7764) Grad: 0.0000  \n",
      "Epoch: [18][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7011(0.7413) Grad: 0.0000  \n",
      "Epoch 18 - avg_train_loss: 0.6575  avg_val_loss: 0.7413  time: 0s\n",
      "Epoch 18 - Score: 0.8208\n",
      "Epoch: [19][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6645(0.6645) Grad: 4082.2168  LR: 0.00590490  \n",
      "Epoch: [19][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6523(0.6550) Grad: 2708.2542  LR: 0.00531441  \n",
      "Epoch: [19][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7059(0.7059) Grad: 0.0000  \n",
      "Epoch: [19][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6930(0.6999) Grad: 0.0000  \n",
      "Epoch 19 - avg_train_loss: 0.6550  avg_val_loss: 0.6999  time: 0s\n",
      "Epoch 19 - Score: 0.8792\n",
      "Epoch: [20][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6554(0.6554) Grad: 2213.2888  LR: 0.00531441  \n",
      "Epoch: [20][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6456(0.6458) Grad: 2589.9983  LR: 0.00531441  \n",
      "Epoch: [20][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7016(0.7016) Grad: 0.0000  \n",
      "Epoch: [20][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6865(0.6945) Grad: 0.0000  \n",
      "Epoch 20 - avg_train_loss: 0.6458  avg_val_loss: 0.6945  time: 0s\n",
      "Epoch 20 - Score: 0.8833\n",
      "Epoch: [21][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6536) Grad: 2181.5110  LR: 0.00531441  \n",
      "Epoch: [21][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6274(0.6471) Grad: 2568.3022  LR: 0.00531441  \n",
      "Epoch: [21][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6995(0.6995) Grad: 0.0000  \n",
      "Epoch: [21][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6813(0.6910) Grad: 0.0000  \n",
      "Epoch 21 - avg_train_loss: 0.6471  avg_val_loss: 0.6910  time: 0s\n",
      "Epoch 21 - Score: 0.8833\n",
      "Epoch: [22][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6451(0.6451) Grad: 2925.0234  LR: 0.00531441  \n",
      "Epoch: [22][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6565(0.6461) Grad: 2189.2744  LR: 0.00430467  \n",
      "Epoch: [22][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6935(0.6935) Grad: 0.0000  \n",
      "Epoch: [22][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6625(0.6790) Grad: 0.0000  \n",
      "Epoch 22 - avg_train_loss: 0.6461  avg_val_loss: 0.6790  time: 0s\n",
      "Epoch 22 - Score: 0.8917\n",
      "Epoch 22 - Save Best Score: 0.8917 Model\n",
      "Epoch: [23][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6568(0.6568) Grad: 2974.3794  LR: 0.00478297  \n",
      "Epoch: [23][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6586(0.6417) Grad: 2267.7576  LR: 0.00478297  \n",
      "Epoch: [23][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6929(0.6929) Grad: 0.0000  \n",
      "Epoch: [23][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6848(0.6891) Grad: 0.0000  \n",
      "Epoch 23 - avg_train_loss: 0.6417  avg_val_loss: 0.6891  time: 0s\n",
      "Epoch 23 - Score: 0.8792\n",
      "Epoch: [24][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6294(0.6294) Grad: 1855.9567  LR: 0.00478297  \n",
      "Epoch: [24][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6614(0.6418) Grad: 2274.6204  LR: 0.00478297  \n",
      "Epoch: [24][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6947(0.6947) Grad: 0.0000  \n",
      "Epoch: [24][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6859(0.6906) Grad: 0.0000  \n",
      "Epoch 24 - avg_train_loss: 0.6418  avg_val_loss: 0.6906  time: 0s\n",
      "Epoch 24 - Score: 0.8792\n",
      "Epoch: [25][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6251(0.6251) Grad: 1841.8003  LR: 0.00478297  \n",
      "Epoch: [25][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6604(0.6412) Grad: 4983.8076  LR: 0.00478297  \n",
      "Epoch: [25][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6908(0.6908) Grad: 0.0000  \n",
      "Epoch: [25][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6709(0.6815) Grad: 0.0000  \n",
      "Epoch 25 - avg_train_loss: 0.6412  avg_val_loss: 0.6815  time: 0s\n",
      "Epoch 25 - Score: 0.8792\n",
      "Epoch: [26][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6020(0.6020) Grad: 2108.1238  LR: 0.00387420  \n",
      "Epoch: [26][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6634(0.6460) Grad: 1625.9900  LR: 0.00430467  \n",
      "Epoch: [26][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6972(0.6972) Grad: 0.0000  \n",
      "Epoch: [26][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6798(0.6891) Grad: 0.0000  \n",
      "Epoch 26 - avg_train_loss: 0.6460  avg_val_loss: 0.6891  time: 0s\n",
      "Epoch 26 - Score: 0.8750\n",
      "Epoch: [27][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6851(0.6851) Grad: 1237.3297  LR: 0.00430467  \n",
      "Epoch: [27][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6069(0.6424) Grad: 2062.3401  LR: 0.00430467  \n",
      "Epoch: [27][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6817(0.6817) Grad: 0.0000  \n",
      "Epoch: [27][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6635(0.6732) Grad: 0.0000  \n",
      "Epoch 27 - avg_train_loss: 0.6424  avg_val_loss: 0.6732  time: 0s\n",
      "Epoch 27 - Score: 0.8917\n",
      "Epoch: [28][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6123(0.6123) Grad: 1263.2067  LR: 0.00430467  \n",
      "Epoch: [28][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6457(0.6408) Grad: 1368.5156  LR: 0.00430467  \n",
      "Epoch: [28][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6746(0.6746) Grad: 0.0000  \n",
      "Epoch: [28][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6538(0.6649) Grad: 0.0000  \n",
      "Epoch 28 - avg_train_loss: 0.6408  avg_val_loss: 0.6649  time: 0s\n",
      "Epoch 28 - Score: 0.8917\n",
      "Epoch: [29][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6130(0.6130) Grad: 1714.1260  LR: 0.00430467  \n",
      "Epoch: [29][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6368) Grad: 1604.1768  LR: 0.00387420  \n",
      "Epoch: [29][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6627(0.6627) Grad: 0.0000  \n",
      "Epoch: [29][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6413(0.6527) Grad: 0.0000  \n",
      "Epoch 29 - avg_train_loss: 0.6368  avg_val_loss: 0.6527  time: 0s\n",
      "Epoch 29 - Score: 0.9083\n",
      "Epoch 29 - Save Best Score: 0.9083 Model\n",
      "Epoch: [30][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6338(0.6338) Grad: 1113.7974  LR: 0.00387420  \n",
      "Epoch: [30][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6357(0.6334) Grad: 1234.1764  LR: 0.00387420  \n",
      "Epoch: [30][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6793(0.6793) Grad: 0.0000  \n",
      "Epoch: [30][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6710(0.6755) Grad: 0.0000  \n",
      "Epoch 30 - avg_train_loss: 0.6334  avg_val_loss: 0.6755  time: 0s\n",
      "Epoch 30 - Score: 0.8875\n",
      "Epoch: [31][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6170(0.6170) Grad: 1245.5374  LR: 0.00387420  \n",
      "Epoch: [31][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6056(0.6376) Grad: 1087.1057  LR: 0.00387420  \n",
      "Epoch: [31][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6762(0.6762) Grad: 0.0000  \n",
      "Epoch: [31][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6660(0.6714) Grad: 0.0000  \n",
      "Epoch 31 - avg_train_loss: 0.6376  avg_val_loss: 0.6714  time: 0s\n",
      "Epoch 31 - Score: 0.8875\n",
      "Epoch: [32][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6378(0.6378) Grad: 1654.9640  LR: 0.00387420  \n",
      "Epoch: [32][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6082(0.6299) Grad: 1488.3885  LR: 0.00348678  \n",
      "Epoch: [32][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6600(0.6600) Grad: 0.0000  \n",
      "Epoch: [32][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6393(0.6504) Grad: 0.0000  \n",
      "Epoch 32 - avg_train_loss: 0.6299  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 32 - Score: 0.9083\n",
      "Epoch: [33][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6311(0.6311) Grad: 1353.2545  LR: 0.00348678  \n",
      "Epoch: [33][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6205(0.6350) Grad: 2494.7124  LR: 0.00348678  \n",
      "Epoch: [33][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6505(0.6505) Grad: 0.0000  \n",
      "Epoch: [33][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6213(0.6368) Grad: 0.0000  \n",
      "Epoch 33 - avg_train_loss: 0.6350  avg_val_loss: 0.6368  time: 0s\n",
      "Epoch 33 - Score: 0.9250\n",
      "Epoch 33 - Save Best Score: 0.9250 Model\n",
      "Epoch: [34][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6536(0.6536) Grad: 1680.8109  LR: 0.00348678  \n",
      "Epoch: [34][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5959(0.6296) Grad: 1211.0979  LR: 0.00348678  \n",
      "Epoch: [34][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6575(0.6575) Grad: 0.0000  \n",
      "Epoch: [34][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6342(0.6466) Grad: 0.0000  \n",
      "Epoch 34 - avg_train_loss: 0.6296  avg_val_loss: 0.6466  time: 0s\n",
      "Epoch 34 - Score: 0.9167\n",
      "Epoch: [35][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6306(0.6306) Grad: 816.8134  LR: 0.00348678  \n",
      "Epoch: [35][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6265(0.6320) Grad: 1816.6340  LR: 0.00313811  \n",
      "Epoch: [35][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6604(0.6604) Grad: 0.0000  \n",
      "Epoch: [35][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6532) Grad: 0.0000  \n",
      "Epoch 35 - avg_train_loss: 0.6320  avg_val_loss: 0.6532  time: 0s\n",
      "Epoch 35 - Score: 0.9000\n",
      "Epoch: [36][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5999(0.5999) Grad: 894.2894  LR: 0.00313811  \n",
      "Epoch: [36][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6412(0.6324) Grad: 1067.2390  LR: 0.00313811  \n",
      "Epoch: [36][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6566(0.6566) Grad: 0.0000  \n",
      "Epoch: [36][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6325(0.6454) Grad: 0.0000  \n",
      "Epoch 36 - avg_train_loss: 0.6324  avg_val_loss: 0.6454  time: 0s\n",
      "Epoch 36 - Score: 0.9167\n",
      "Epoch: [37][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6152(0.6152) Grad: 816.0719  LR: 0.00313811  \n",
      "Epoch: [37][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6310) Grad: 1013.9692  LR: 0.00313811  \n",
      "Epoch: [37][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6445) Grad: 0.0000  \n",
      "Epoch: [37][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6134(0.6300) Grad: 0.0000  \n",
      "Epoch 37 - avg_train_loss: 0.6310  avg_val_loss: 0.6300  time: 0s\n",
      "Epoch 37 - Score: 0.9292\n",
      "Epoch 37 - Save Best Score: 0.9292 Model\n",
      "Epoch: [38][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6401(0.6401) Grad: 1847.8311  LR: 0.00313811  \n",
      "Epoch: [38][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6290(0.6313) Grad: 835.3932  LR: 0.00282430  \n",
      "Epoch: [38][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6449) Grad: 0.0000  \n",
      "Epoch: [38][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6138(0.6304) Grad: 0.0000  \n",
      "Epoch 38 - avg_train_loss: 0.6313  avg_val_loss: 0.6304  time: 0s\n",
      "Epoch 38 - Score: 0.9333\n",
      "Epoch 38 - Save Best Score: 0.9333 Model\n",
      "Epoch: [39][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6321(0.6321) Grad: 1180.9397  LR: 0.00282430  \n",
      "Epoch: [39][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6804(0.6329) Grad: 2110.2820  LR: 0.00282430  \n",
      "Epoch: [39][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6520(0.6520) Grad: 0.0000  \n",
      "Epoch: [39][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6197(0.6369) Grad: 0.0000  \n",
      "Epoch 39 - avg_train_loss: 0.6329  avg_val_loss: 0.6369  time: 0s\n",
      "Epoch 39 - Score: 0.9250\n",
      "Epoch: [40][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6001(0.6001) Grad: 859.7996  LR: 0.00282430  \n",
      "Epoch: [40][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6208(0.6257) Grad: 582.2947  LR: 0.00282430  \n",
      "Epoch: [40][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6578(0.6578) Grad: 0.0000  \n",
      "Epoch: [40][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6320(0.6457) Grad: 0.0000  \n",
      "Epoch 40 - avg_train_loss: 0.6257  avg_val_loss: 0.6457  time: 0s\n",
      "Epoch 40 - Score: 0.9083\n",
      "Epoch: [41][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6280(0.6280) Grad: 650.9088  LR: 0.00282430  \n",
      "Epoch: [41][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6290(0.6291) Grad: 1648.9128  LR: 0.00254187  \n",
      "Epoch: [41][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6446) Grad: 0.0000  \n",
      "Epoch: [41][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6148(0.6307) Grad: 0.0000  \n",
      "Epoch 41 - avg_train_loss: 0.6291  avg_val_loss: 0.6307  time: 0s\n",
      "Epoch 41 - Score: 0.9292\n",
      "Epoch: [42][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6297(0.6297) Grad: 1075.6769  LR: 0.00254187  \n",
      "Epoch: [42][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6205(0.6283) Grad: 604.0897  LR: 0.00254187  \n",
      "Epoch: [42][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6486(0.6486) Grad: 0.0000  \n",
      "Epoch: [42][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6083(0.6298) Grad: 0.0000  \n",
      "Epoch 42 - avg_train_loss: 0.6283  avg_val_loss: 0.6298  time: 0s\n",
      "Epoch 42 - Score: 0.9292\n",
      "Epoch: [43][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6563(0.6563) Grad: 1263.5227  LR: 0.00254187  \n",
      "Epoch: [43][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6535(0.6344) Grad: 818.8188  LR: 0.00254187  \n",
      "Epoch: [43][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6461(0.6461) Grad: 0.0000  \n",
      "Epoch: [43][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6090(0.6288) Grad: 0.0000  \n",
      "Epoch 43 - avg_train_loss: 0.6344  avg_val_loss: 0.6288  time: 0s\n",
      "Epoch 43 - Score: 0.9292\n",
      "Epoch: [44][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6095) Grad: 1790.8475  LR: 0.00254187  \n",
      "Epoch: [44][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6346(0.6266) Grad: 1169.9865  LR: 0.00205891  \n",
      "Epoch: [44][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6547(0.6547) Grad: 0.0000  \n",
      "Epoch: [44][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6144(0.6359) Grad: 0.0000  \n",
      "Epoch 44 - avg_train_loss: 0.6266  avg_val_loss: 0.6359  time: 0s\n",
      "Epoch 44 - Score: 0.9250\n",
      "Epoch: [45][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6374(0.6374) Grad: 1429.1022  LR: 0.00228768  \n",
      "Epoch: [45][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6385(0.6277) Grad: 801.5186  LR: 0.00228768  \n",
      "Epoch: [45][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6710(0.6710) Grad: 0.0000  \n",
      "Epoch: [45][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6190(0.6467) Grad: 0.0000  \n",
      "Epoch 45 - avg_train_loss: 0.6277  avg_val_loss: 0.6467  time: 0s\n",
      "Epoch 45 - Score: 0.9125\n",
      "Epoch: [46][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6168(0.6168) Grad: 2189.0767  LR: 0.00228768  \n",
      "Epoch: [46][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6397(0.6216) Grad: 3853.8125  LR: 0.00228768  \n",
      "Epoch: [46][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6776(0.6776) Grad: 0.0000  \n",
      "Epoch: [46][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6149(0.6483) Grad: 0.0000  \n",
      "Epoch 46 - avg_train_loss: 0.6216  avg_val_loss: 0.6483  time: 0s\n",
      "Epoch 46 - Score: 0.9125\n",
      "Epoch: [47][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5990(0.5990) Grad: 1876.9501  LR: 0.00228768  \n",
      "Epoch: [47][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6555(0.6298) Grad: 935.4424  LR: 0.00228768  \n",
      "Epoch: [47][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6597(0.6597) Grad: 0.0000  \n",
      "Epoch: [47][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6121(0.6375) Grad: 0.0000  \n",
      "Epoch 47 - avg_train_loss: 0.6298  avg_val_loss: 0.6375  time: 0s\n",
      "Epoch 47 - Score: 0.9208\n",
      "Epoch: [48][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6290(0.6290) Grad: 1126.1543  LR: 0.00185302  \n",
      "Epoch: [48][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6484(0.6270) Grad: 3263.4458  LR: 0.00205891  \n",
      "Epoch: [48][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6521(0.6521) Grad: 0.0000  \n",
      "Epoch: [48][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6120(0.6334) Grad: 0.0000  \n",
      "Epoch 48 - avg_train_loss: 0.6270  avg_val_loss: 0.6334  time: 0s\n",
      "Epoch 48 - Score: 0.9250\n",
      "Epoch: [49][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6168(0.6168) Grad: 2121.7139  LR: 0.00205891  \n",
      "Epoch: [49][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6042(0.6307) Grad: 647.4783  LR: 0.00205891  \n",
      "Epoch: [49][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6542(0.6542) Grad: 0.0000  \n",
      "Epoch: [49][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6125(0.6348) Grad: 0.0000  \n",
      "Epoch 49 - avg_train_loss: 0.6307  avg_val_loss: 0.6348  time: 0s\n",
      "Epoch 49 - Score: 0.9250\n",
      "Epoch: [50][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6088(0.6088) Grad: 1308.0817  LR: 0.00205891  \n",
      "Epoch: [50][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6250(0.6286) Grad: 766.5333  LR: 0.00205891  \n",
      "Epoch: [50][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6530(0.6530) Grad: 0.0000  \n",
      "Epoch: [50][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6127(0.6342) Grad: 0.0000  \n",
      "Epoch 50 - avg_train_loss: 0.6286  avg_val_loss: 0.6342  time: 0s\n",
      "Epoch 50 - Score: 0.9208\n",
      "Epoch: [51][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6407(0.6407) Grad: 2481.6958  LR: 0.00205891  \n",
      "Epoch: [51][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6423(0.6240) Grad: 3606.9297  LR: 0.00185302  \n",
      "Epoch: [51][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6611(0.6611) Grad: 0.0000  \n",
      "Epoch: [51][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6118(0.6381) Grad: 0.0000  \n",
      "Epoch 51 - avg_train_loss: 0.6240  avg_val_loss: 0.6381  time: 0s\n",
      "Epoch 51 - Score: 0.9167\n",
      "Epoch: [52][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6269(0.6269) Grad: 3697.1116  LR: 0.00185302  \n",
      "Epoch: [52][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6428(0.6234) Grad: 476.0269  LR: 0.00185302  \n",
      "Epoch: [52][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6563(0.6563) Grad: 0.0000  \n",
      "Epoch: [52][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6125(0.6359) Grad: 0.0000  \n",
      "Epoch 52 - avg_train_loss: 0.6234  avg_val_loss: 0.6359  time: 0s\n",
      "Epoch 52 - Score: 0.9250\n",
      "Epoch: [53][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5850(0.5850) Grad: 3226.1699  LR: 0.00185302  \n",
      "Epoch: [53][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6207(0.6296) Grad: 865.8354  LR: 0.00185302  \n",
      "Epoch: [53][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6663(0.6663) Grad: 0.0000  \n",
      "Epoch: [53][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6159(0.6428) Grad: 0.0000  \n",
      "Epoch 53 - avg_train_loss: 0.6296  avg_val_loss: 0.6428  time: 0s\n",
      "Epoch 53 - Score: 0.9125\n",
      "Epoch: [54][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6215(0.6215) Grad: 654.1232  LR: 0.00185302  \n",
      "Epoch: [54][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6479(0.6209) Grad: 2676.9893  LR: 0.00166772  \n",
      "Epoch: [54][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6643(0.6643) Grad: 0.0000  \n",
      "Epoch: [54][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6101(0.6390) Grad: 0.0000  \n",
      "Epoch 54 - avg_train_loss: 0.6209  avg_val_loss: 0.6390  time: 0s\n",
      "Epoch 54 - Score: 0.9167\n",
      "Epoch: [55][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6384(0.6384) Grad: 1393.0334  LR: 0.00166772  \n",
      "Epoch: [55][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6486(0.6200) Grad: 1611.0142  LR: 0.00166772  \n",
      "Epoch: [55][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6558(0.6558) Grad: 0.0000  \n",
      "Epoch: [55][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6086(0.6338) Grad: 0.0000  \n",
      "Epoch 55 - avg_train_loss: 0.6200  avg_val_loss: 0.6338  time: 0s\n",
      "Epoch 55 - Score: 0.9208\n",
      "Epoch: [56][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6112(0.6112) Grad: 3947.8411  LR: 0.00166772  \n",
      "Epoch: [56][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5958(0.6226) Grad: 625.5771  LR: 0.00166772  \n",
      "Epoch: [56][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6523(0.6523) Grad: 0.0000  \n",
      "Epoch: [56][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6104(0.6328) Grad: 0.0000  \n",
      "Epoch 56 - avg_train_loss: 0.6226  avg_val_loss: 0.6328  time: 0s\n",
      "Epoch 56 - Score: 0.9208\n",
      "Epoch: [57][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6446) Grad: 953.4360  LR: 0.00166772  \n",
      "Epoch: [57][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6238) Grad: 670.4643  LR: 0.00150095  \n",
      "Epoch: [57][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6511(0.6511) Grad: 0.0000  \n",
      "Epoch: [57][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6103(0.6320) Grad: 0.0000  \n",
      "Epoch 57 - avg_train_loss: 0.6238  avg_val_loss: 0.6320  time: 0s\n",
      "Epoch 57 - Score: 0.9208\n",
      "Epoch: [58][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6290(0.6290) Grad: 1061.8380  LR: 0.00150095  \n",
      "Epoch: [58][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6391(0.6180) Grad: 2963.1199  LR: 0.00150095  \n",
      "Epoch: [58][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6529(0.6529) Grad: 0.0000  \n",
      "Epoch: [58][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6105(0.6331) Grad: 0.0000  \n",
      "Epoch 58 - avg_train_loss: 0.6180  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 58 - Score: 0.9208\n",
      "Epoch: [59][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6135(0.6135) Grad: 2007.8590  LR: 0.00150095  \n",
      "Epoch: [59][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6226(0.6208) Grad: 3683.5232  LR: 0.00150095  \n",
      "Epoch: [59][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6543(0.6543) Grad: 0.0000  \n",
      "Epoch: [59][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6106(0.6339) Grad: 0.0000  \n",
      "Epoch 59 - avg_train_loss: 0.6208  avg_val_loss: 0.6339  time: 0s\n",
      "Epoch 59 - Score: 0.9250\n",
      "Epoch: [60][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6029(0.6029) Grad: 2748.2732  LR: 0.00150095  \n",
      "Epoch: [60][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6217(0.6135) Grad: 1993.8964  LR: 0.00135085  \n",
      "Epoch: [60][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6620(0.6620) Grad: 0.0000  \n",
      "Epoch: [60][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6108(0.6381) Grad: 0.0000  \n",
      "Epoch 60 - avg_train_loss: 0.6135  avg_val_loss: 0.6381  time: 0s\n",
      "Epoch 60 - Score: 0.9208\n",
      "Epoch: [61][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6318(0.6318) Grad: 4224.8569  LR: 0.00135085  \n",
      "Epoch: [61][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6287(0.6183) Grad: 634.5700  LR: 0.00135085  \n",
      "Epoch: [61][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6758(0.6758) Grad: 0.0000  \n",
      "Epoch: [61][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6112(0.6456) Grad: 0.0000  \n",
      "Epoch 61 - avg_train_loss: 0.6183  avg_val_loss: 0.6456  time: 0s\n",
      "Epoch 61 - Score: 0.9083\n",
      "Epoch: [62][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6277(0.6277) Grad: 5019.8330  LR: 0.00135085  \n",
      "Epoch: [62][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6564(0.6177) Grad: 1436.7452  LR: 0.00135085  \n",
      "Epoch: [62][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6753(0.6753) Grad: 0.0000  \n",
      "Epoch: [62][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6108(0.6452) Grad: 0.0000  \n",
      "Epoch 62 - avg_train_loss: 0.6177  avg_val_loss: 0.6452  time: 0s\n",
      "Epoch 62 - Score: 0.9083\n",
      "Epoch: [63][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6418(0.6418) Grad: 466.0238  LR: 0.00135085  \n",
      "Epoch: [63][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6208) Grad: 3691.9658  LR: 0.00121577  \n",
      "Epoch: [63][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6633(0.6633) Grad: 0.0000  \n",
      "Epoch: [63][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6103(0.6386) Grad: 0.0000  \n",
      "Epoch 63 - avg_train_loss: 0.6208  avg_val_loss: 0.6386  time: 0s\n",
      "Epoch 63 - Score: 0.9125\n",
      "Epoch: [64][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6174(0.6174) Grad: 3158.2705  LR: 0.00121577  \n",
      "Epoch: [64][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6180) Grad: 2599.1838  LR: 0.00121577  \n",
      "Epoch: [64][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6595) Grad: 0.0000  \n",
      "Epoch: [64][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6104(0.6366) Grad: 0.0000  \n",
      "Epoch 64 - avg_train_loss: 0.6180  avg_val_loss: 0.6366  time: 0s\n",
      "Epoch 64 - Score: 0.9208\n",
      "Epoch: [65][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5759(0.5759) Grad: 3479.4104  LR: 0.00121577  \n",
      "Epoch: [65][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6206) Grad: 2222.2625  LR: 0.00121577  \n",
      "Epoch: [65][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6606(0.6606) Grad: 0.0000  \n",
      "Epoch: [65][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6102(0.6371) Grad: 0.0000  \n",
      "Epoch 65 - avg_train_loss: 0.6206  avg_val_loss: 0.6371  time: 0s\n",
      "Epoch 65 - Score: 0.9167\n",
      "Epoch: [66][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6251(0.6251) Grad: 1217.9487  LR: 0.00121577  \n",
      "Epoch: [66][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5731(0.6154) Grad: 1261.6431  LR: 0.00098477  \n",
      "Epoch: [66][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6585(0.6585) Grad: 0.0000  \n",
      "Epoch: [66][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6100(0.6359) Grad: 0.0000  \n",
      "Epoch 66 - avg_train_loss: 0.6154  avg_val_loss: 0.6359  time: 0s\n",
      "Epoch 66 - Score: 0.9167\n",
      "Epoch: [67][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6418(0.6418) Grad: 2007.7625  LR: 0.00109419  \n",
      "Epoch: [67][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6429(0.6198) Grad: 1557.3058  LR: 0.00109419  \n",
      "Epoch: [67][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6550(0.6550) Grad: 0.0000  \n",
      "Epoch: [67][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6101(0.6340) Grad: 0.0000  \n",
      "Epoch 67 - avg_train_loss: 0.6198  avg_val_loss: 0.6340  time: 0s\n",
      "Epoch 67 - Score: 0.9208\n",
      "Epoch: [68][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6251(0.6251) Grad: 3914.4453  LR: 0.00109419  \n",
      "Epoch: [68][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6033(0.6182) Grad: 554.1658  LR: 0.00109419  \n",
      "Epoch: [68][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6584(0.6584) Grad: 0.0000  \n",
      "Epoch: [68][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6104(0.6360) Grad: 0.0000  \n",
      "Epoch 68 - avg_train_loss: 0.6182  avg_val_loss: 0.6360  time: 0s\n",
      "Epoch 68 - Score: 0.9208\n",
      "Epoch: [69][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6022(0.6022) Grad: 4849.8345  LR: 0.00109419  \n",
      "Epoch: [69][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6045(0.6202) Grad: 1437.5529  LR: 0.00109419  \n",
      "Epoch: [69][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6577(0.6577) Grad: 0.0000  \n",
      "Epoch: [69][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6109(0.6359) Grad: 0.0000  \n",
      "Epoch 69 - avg_train_loss: 0.6202  avg_val_loss: 0.6359  time: 0s\n",
      "Epoch 69 - Score: 0.9208\n",
      "Epoch: [70][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6299(0.6299) Grad: 4615.0415  LR: 0.00088629  \n",
      "Epoch: [70][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6244) Grad: 1752.5146  LR: 0.00098477  \n",
      "Epoch: [70][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6631(0.6631) Grad: 0.0000  \n",
      "Epoch: [70][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6121(0.6393) Grad: 0.0000  \n",
      "Epoch 70 - avg_train_loss: 0.6244  avg_val_loss: 0.6393  time: 0s\n",
      "Epoch 70 - Score: 0.9125\n",
      "Epoch: [71][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6093) Grad: 958.9089  LR: 0.00098477  \n",
      "Epoch: [71][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6340(0.6218) Grad: 2893.7065  LR: 0.00098477  \n",
      "Epoch: [71][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6621(0.6621) Grad: 0.0000  \n",
      "Epoch: [71][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6105(0.6380) Grad: 0.0000  \n",
      "Epoch 71 - avg_train_loss: 0.6218  avg_val_loss: 0.6380  time: 0s\n",
      "Epoch 71 - Score: 0.9167\n",
      "Epoch: [72][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6177(0.6177) Grad: 640.3127  LR: 0.00098477  \n",
      "Epoch: [72][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6278(0.6142) Grad: 1961.2303  LR: 0.00098477  \n",
      "Epoch: [72][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6585(0.6585) Grad: 0.0000  \n",
      "Epoch: [72][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6099(0.6358) Grad: 0.0000  \n",
      "Epoch 72 - avg_train_loss: 0.6142  avg_val_loss: 0.6358  time: 0s\n",
      "Epoch 72 - Score: 0.9208\n",
      "Epoch: [73][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6205(0.6205) Grad: 5532.4639  LR: 0.00098477  \n",
      "Epoch: [73][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6280(0.6161) Grad: 3831.2473  LR: 0.00088629  \n",
      "Epoch: [73][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6565(0.6565) Grad: 0.0000  \n",
      "Epoch: [73][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6099(0.6347) Grad: 0.0000  \n",
      "Epoch 73 - avg_train_loss: 0.6161  avg_val_loss: 0.6347  time: 0s\n",
      "Epoch 73 - Score: 0.9208\n",
      "Epoch: [74][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6294(0.6294) Grad: 3149.5728  LR: 0.00088629  \n",
      "Epoch: [74][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6110(0.6181) Grad: 441.8090  LR: 0.00088629  \n",
      "Epoch: [74][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6577(0.6577) Grad: 0.0000  \n",
      "Epoch: [74][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6100(0.6354) Grad: 0.0000  \n",
      "Epoch 74 - avg_train_loss: 0.6181  avg_val_loss: 0.6354  time: 0s\n",
      "Epoch 74 - Score: 0.9208\n",
      "Epoch: [75][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6146(0.6146) Grad: 5340.0396  LR: 0.00088629  \n",
      "Epoch: [75][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6389(0.6172) Grad: 4519.9385  LR: 0.00088629  \n",
      "Epoch: [75][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6573(0.6573) Grad: 0.0000  \n",
      "Epoch: [75][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6103(0.6353) Grad: 0.0000  \n",
      "Epoch 75 - avg_train_loss: 0.6172  avg_val_loss: 0.6353  time: 0s\n",
      "Epoch 75 - Score: 0.9208\n",
      "Epoch: [76][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6208(0.6208) Grad: 2402.8440  LR: 0.00088629  \n",
      "Epoch: [76][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6161) Grad: 1446.3865  LR: 0.00079766  \n",
      "Epoch: [76][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6589(0.6589) Grad: 0.0000  \n",
      "Epoch: [76][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6100(0.6361) Grad: 0.0000  \n",
      "Epoch 76 - avg_train_loss: 0.6161  avg_val_loss: 0.6361  time: 0s\n",
      "Epoch 76 - Score: 0.9208\n",
      "Epoch: [77][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5972(0.5972) Grad: 4087.7312  LR: 0.00079766  \n",
      "Epoch: [77][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6221(0.6171) Grad: 4363.9639  LR: 0.00079766  \n",
      "Epoch: [77][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6621(0.6621) Grad: 0.0000  \n",
      "Epoch: [77][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6099(0.6377) Grad: 0.0000  \n",
      "Epoch 77 - avg_train_loss: 0.6171  avg_val_loss: 0.6377  time: 0s\n",
      "Epoch 77 - Score: 0.9167\n",
      "Epoch: [78][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6450) Grad: 2705.5032  LR: 0.00079766  \n",
      "Epoch: [78][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6410(0.6157) Grad: 507.9220  LR: 0.00079766  \n",
      "Epoch: [78][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6642(0.6642) Grad: 0.0000  \n",
      "Epoch: [78][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6388) Grad: 0.0000  \n",
      "Epoch 78 - avg_train_loss: 0.6157  avg_val_loss: 0.6388  time: 0s\n",
      "Epoch 78 - Score: 0.9167\n",
      "Epoch: [79][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5955(0.5955) Grad: 980.3556  LR: 0.00079766  \n",
      "Epoch: [79][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6152) Grad: 845.8505  LR: 0.00071790  \n",
      "Epoch: [79][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6615(0.6615) Grad: 0.0000  \n",
      "Epoch: [79][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6373) Grad: 0.0000  \n",
      "Epoch 79 - avg_train_loss: 0.6152  avg_val_loss: 0.6373  time: 0s\n",
      "Epoch 79 - Score: 0.9167\n",
      "Epoch: [80][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5950(0.5950) Grad: 1572.8273  LR: 0.00071790  \n",
      "Epoch: [80][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6234(0.6164) Grad: 930.0251  LR: 0.00071790  \n",
      "Epoch: [80][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6605(0.6605) Grad: 0.0000  \n",
      "Epoch: [80][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6368) Grad: 0.0000  \n",
      "Epoch 80 - avg_train_loss: 0.6164  avg_val_loss: 0.6368  time: 0s\n",
      "Epoch 80 - Score: 0.9167\n",
      "Epoch: [81][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6345(0.6345) Grad: 1700.1470  LR: 0.00071790  \n",
      "Epoch: [81][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5916(0.6123) Grad: 1231.4825  LR: 0.00071790  \n",
      "Epoch: [81][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6609(0.6609) Grad: 0.0000  \n",
      "Epoch: [81][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6099(0.6371) Grad: 0.0000  \n",
      "Epoch 81 - avg_train_loss: 0.6123  avg_val_loss: 0.6371  time: 0s\n",
      "Epoch 81 - Score: 0.9167\n",
      "Epoch: [82][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6152(0.6152) Grad: 1513.5667  LR: 0.00071790  \n",
      "Epoch: [82][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6298(0.6125) Grad: 2642.9294  LR: 0.00064611  \n",
      "Epoch: [82][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6672(0.6672) Grad: 0.0000  \n",
      "Epoch: [82][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6104(0.6407) Grad: 0.0000  \n",
      "Epoch 82 - avg_train_loss: 0.6125  avg_val_loss: 0.6407  time: 0s\n",
      "Epoch 82 - Score: 0.9167\n",
      "Epoch: [83][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5935(0.5935) Grad: 4388.2495  LR: 0.00064611  \n",
      "Epoch: [83][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6287(0.6129) Grad: 3367.4592  LR: 0.00064611  \n",
      "Epoch: [83][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6619(0.6619) Grad: 0.0000  \n",
      "Epoch: [83][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6098(0.6376) Grad: 0.0000  \n",
      "Epoch 83 - avg_train_loss: 0.6129  avg_val_loss: 0.6376  time: 0s\n",
      "Epoch 83 - Score: 0.9167\n",
      "Epoch: [84][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5988(0.5988) Grad: 2460.3425  LR: 0.00064611  \n",
      "Epoch: [84][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6015(0.6107) Grad: 1769.9783  LR: 0.00064611  \n",
      "Epoch: [84][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6607(0.6607) Grad: 0.0000  \n",
      "Epoch: [84][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6099(0.6370) Grad: 0.0000  \n",
      "Epoch 84 - avg_train_loss: 0.6107  avg_val_loss: 0.6370  time: 0s\n",
      "Epoch 84 - Score: 0.9167\n",
      "Epoch: [85][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5966(0.5966) Grad: 746.3852  LR: 0.00064611  \n",
      "Epoch: [85][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5988(0.6153) Grad: 3357.8772  LR: 0.00058150  \n",
      "Epoch: [85][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6610(0.6610) Grad: 0.0000  \n",
      "Epoch: [85][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6370) Grad: 0.0000  \n",
      "Epoch 85 - avg_train_loss: 0.6153  avg_val_loss: 0.6370  time: 0s\n",
      "Epoch 85 - Score: 0.9208\n",
      "Epoch: [86][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6315(0.6315) Grad: 967.2102  LR: 0.00058150  \n",
      "Epoch: [86][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5910(0.6154) Grad: 4953.2085  LR: 0.00058150  \n",
      "Epoch: [86][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6646(0.6646) Grad: 0.0000  \n",
      "Epoch: [86][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6390) Grad: 0.0000  \n",
      "Epoch 86 - avg_train_loss: 0.6154  avg_val_loss: 0.6390  time: 0s\n",
      "Epoch 86 - Score: 0.9167\n",
      "Epoch: [87][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5803(0.5803) Grad: 652.4765  LR: 0.00058150  \n",
      "Epoch: [87][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5991(0.6092) Grad: 2251.0142  LR: 0.00058150  \n",
      "Epoch: [87][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6718(0.6718) Grad: 0.0000  \n",
      "Epoch: [87][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6099(0.6429) Grad: 0.0000  \n",
      "Epoch 87 - avg_train_loss: 0.6092  avg_val_loss: 0.6429  time: 0s\n",
      "Epoch 87 - Score: 0.9125\n",
      "Epoch: [88][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6057(0.6057) Grad: 1905.5192  LR: 0.00058150  \n",
      "Epoch: [88][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6323(0.6116) Grad: 5019.9448  LR: 0.00047101  \n",
      "Epoch: [88][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6697(0.6697) Grad: 0.0000  \n",
      "Epoch: [88][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6098(0.6417) Grad: 0.0000  \n",
      "Epoch 88 - avg_train_loss: 0.6116  avg_val_loss: 0.6417  time: 0s\n",
      "Epoch 88 - Score: 0.9125\n",
      "Epoch: [89][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6556(0.6556) Grad: 8187.7622  LR: 0.00052335  \n",
      "Epoch: [89][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6319(0.6146) Grad: 4673.3027  LR: 0.00052335  \n",
      "Epoch: [89][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6638(0.6638) Grad: 0.0000  \n",
      "Epoch: [89][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6385) Grad: 0.0000  \n",
      "Epoch 89 - avg_train_loss: 0.6146  avg_val_loss: 0.6385  time: 0s\n",
      "Epoch 89 - Score: 0.9167\n",
      "Epoch: [90][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6127(0.6127) Grad: 3939.9102  LR: 0.00052335  \n",
      "Epoch: [90][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6180(0.6129) Grad: 423.2447  LR: 0.00052335  \n",
      "Epoch: [90][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6659(0.6659) Grad: 0.0000  \n",
      "Epoch: [90][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6396) Grad: 0.0000  \n",
      "Epoch 90 - avg_train_loss: 0.6129  avg_val_loss: 0.6396  time: 0s\n",
      "Epoch 90 - Score: 0.9167\n",
      "Epoch: [91][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6404(0.6404) Grad: 3478.4478  LR: 0.00052335  \n",
      "Epoch: [91][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6134(0.6179) Grad: 1072.7609  LR: 0.00052335  \n",
      "Epoch: [91][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6656(0.6656) Grad: 0.0000  \n",
      "Epoch: [91][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6394) Grad: 0.0000  \n",
      "Epoch 91 - avg_train_loss: 0.6179  avg_val_loss: 0.6394  time: 0s\n",
      "Epoch 91 - Score: 0.9167\n",
      "Epoch: [92][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6264(0.6264) Grad: 1077.6840  LR: 0.00042391  \n",
      "Epoch: [92][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5899(0.6151) Grad: 2259.9539  LR: 0.00047101  \n",
      "Epoch: [92][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6674(0.6674) Grad: 0.0000  \n",
      "Epoch: [92][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6403) Grad: 0.0000  \n",
      "Epoch 92 - avg_train_loss: 0.6151  avg_val_loss: 0.6403  time: 0s\n",
      "Epoch 92 - Score: 0.9125\n",
      "Epoch: [93][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5926(0.5926) Grad: 3433.6436  LR: 0.00047101  \n",
      "Epoch: [93][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6140(0.6125) Grad: 2488.8882  LR: 0.00047101  \n",
      "Epoch: [93][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6723(0.6723) Grad: 0.0000  \n",
      "Epoch: [93][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6430) Grad: 0.0000  \n",
      "Epoch 93 - avg_train_loss: 0.6125  avg_val_loss: 0.6430  time: 0s\n",
      "Epoch 93 - Score: 0.9083\n",
      "Epoch: [94][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6126(0.6126) Grad: 3427.0444  LR: 0.00047101  \n",
      "Epoch: [94][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6037(0.6127) Grad: 1740.7805  LR: 0.00047101  \n",
      "Epoch: [94][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6733(0.6733) Grad: 0.0000  \n",
      "Epoch: [94][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6436) Grad: 0.0000  \n",
      "Epoch 94 - avg_train_loss: 0.6127  avg_val_loss: 0.6436  time: 0s\n",
      "Epoch 94 - Score: 0.9083\n",
      "Epoch: [95][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5902(0.5902) Grad: 1557.4980  LR: 0.00047101  \n",
      "Epoch: [95][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6436(0.6145) Grad: 2124.7415  LR: 0.00042391  \n",
      "Epoch: [95][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6725(0.6725) Grad: 0.0000  \n",
      "Epoch: [95][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6431) Grad: 0.0000  \n",
      "Epoch 95 - avg_train_loss: 0.6145  avg_val_loss: 0.6431  time: 0s\n",
      "Epoch 95 - Score: 0.9125\n",
      "Epoch: [96][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6204(0.6204) Grad: 3703.9648  LR: 0.00042391  \n",
      "Epoch: [96][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6358(0.6158) Grad: 638.8989  LR: 0.00042391  \n",
      "Epoch: [96][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6705(0.6705) Grad: 0.0000  \n",
      "Epoch: [96][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6421) Grad: 0.0000  \n",
      "Epoch 96 - avg_train_loss: 0.6158  avg_val_loss: 0.6421  time: 0s\n",
      "Epoch 96 - Score: 0.9125\n",
      "Epoch: [97][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6363(0.6363) Grad: 3948.7434  LR: 0.00042391  \n",
      "Epoch: [97][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6114(0.6112) Grad: 501.5652  LR: 0.00042391  \n",
      "Epoch: [97][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6755(0.6755) Grad: 0.0000  \n",
      "Epoch: [97][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6447) Grad: 0.0000  \n",
      "Epoch 97 - avg_train_loss: 0.6112  avg_val_loss: 0.6447  time: 0s\n",
      "Epoch 97 - Score: 0.9042\n",
      "Epoch: [98][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6294(0.6294) Grad: 2251.3979  LR: 0.00042391  \n",
      "Epoch: [98][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6272(0.6124) Grad: 7297.5186  LR: 0.00038152  \n",
      "Epoch: [98][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6783(0.6783) Grad: 0.0000  \n",
      "Epoch: [98][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6462) Grad: 0.0000  \n",
      "Epoch 98 - avg_train_loss: 0.6124  avg_val_loss: 0.6462  time: 0s\n",
      "Epoch 98 - Score: 0.9042\n",
      "Epoch: [99][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6317(0.6317) Grad: 3965.7046  LR: 0.00038152  \n",
      "Epoch: [99][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5871(0.6119) Grad: 391.3349  LR: 0.00038152  \n",
      "Epoch: [99][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6768(0.6768) Grad: 0.0000  \n",
      "Epoch: [99][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6454) Grad: 0.0000  \n",
      "Epoch 99 - avg_train_loss: 0.6119  avg_val_loss: 0.6454  time: 0s\n",
      "Epoch 99 - Score: 0.9083\n",
      "Epoch: [100][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5897(0.5897) Grad: 2662.4189  LR: 0.00038152  \n",
      "Epoch: [100][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6156(0.6103) Grad: 2913.7058  LR: 0.00038152  \n",
      "Epoch: [100][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6801(0.6801) Grad: 0.0000  \n",
      "Epoch: [100][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6472) Grad: 0.0000  \n",
      "Epoch 100 - avg_train_loss: 0.6103  avg_val_loss: 0.6472  time: 0s\n",
      "Epoch 100 - Score: 0.9042\n",
      "Epoch: [101][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6299(0.6299) Grad: 2813.2681  LR: 0.00038152  \n",
      "Epoch: [101][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5914(0.6147) Grad: 4605.8018  LR: 0.00034337  \n",
      "Epoch: [101][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6847(0.6847) Grad: 0.0000  \n",
      "Epoch: [101][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6496) Grad: 0.0000  \n",
      "Epoch 101 - avg_train_loss: 0.6147  avg_val_loss: 0.6496  time: 0s\n",
      "Epoch 101 - Score: 0.9000\n",
      "Epoch: [102][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6222(0.6222) Grad: 4021.3843  LR: 0.00034337  \n",
      "Epoch: [102][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6065(0.6133) Grad: 4894.5513  LR: 0.00034337  \n",
      "Epoch: [102][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6732(0.6732) Grad: 0.0000  \n",
      "Epoch: [102][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6434) Grad: 0.0000  \n",
      "Epoch 102 - avg_train_loss: 0.6133  avg_val_loss: 0.6434  time: 0s\n",
      "Epoch 102 - Score: 0.9083\n",
      "Epoch: [103][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5977(0.5977) Grad: 3658.5847  LR: 0.00034337  \n",
      "Epoch: [103][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6375(0.6139) Grad: 4446.3262  LR: 0.00034337  \n",
      "Epoch: [103][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6710(0.6710) Grad: 0.0000  \n",
      "Epoch: [103][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6423) Grad: 0.0000  \n",
      "Epoch 103 - avg_train_loss: 0.6139  avg_val_loss: 0.6423  time: 0s\n",
      "Epoch 103 - Score: 0.9125\n",
      "Epoch: [104][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5939(0.5939) Grad: 3272.7891  LR: 0.00034337  \n",
      "Epoch: [104][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5783(0.6116) Grad: 2599.9631  LR: 0.00030903  \n",
      "Epoch: [104][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6690(0.6690) Grad: 0.0000  \n",
      "Epoch: [104][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6412) Grad: 0.0000  \n",
      "Epoch 104 - avg_train_loss: 0.6116  avg_val_loss: 0.6412  time: 0s\n",
      "Epoch 104 - Score: 0.9125\n",
      "Epoch: [105][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6124(0.6124) Grad: 1759.6902  LR: 0.00030903  \n",
      "Epoch: [105][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6340(0.6113) Grad: 790.6962  LR: 0.00030903  \n",
      "Epoch: [105][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6708(0.6708) Grad: 0.0000  \n",
      "Epoch: [105][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6421) Grad: 0.0000  \n",
      "Epoch 105 - avg_train_loss: 0.6113  avg_val_loss: 0.6421  time: 0s\n",
      "Epoch 105 - Score: 0.9125\n",
      "Epoch: [106][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5855(0.5855) Grad: 2389.6025  LR: 0.00030903  \n",
      "Epoch: [106][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6100) Grad: 524.2571  LR: 0.00030903  \n",
      "Epoch: [106][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6812(0.6812) Grad: 0.0000  \n",
      "Epoch: [106][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6477) Grad: 0.0000  \n",
      "Epoch 106 - avg_train_loss: 0.6100  avg_val_loss: 0.6477  time: 0s\n",
      "Epoch 106 - Score: 0.8958\n",
      "Epoch: [107][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6207(0.6207) Grad: 3088.1150  LR: 0.00030903  \n",
      "Epoch: [107][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5881(0.6143) Grad: 5537.0298  LR: 0.00027813  \n",
      "Epoch: [107][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6857(0.6857) Grad: 0.0000  \n",
      "Epoch: [107][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6502) Grad: 0.0000  \n",
      "Epoch 107 - avg_train_loss: 0.6143  avg_val_loss: 0.6502  time: 0s\n",
      "Epoch 107 - Score: 0.8958\n",
      "Epoch: [108][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 1075.4202  LR: 0.00027813  \n",
      "Epoch: [108][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6246(0.6075) Grad: 2707.7297  LR: 0.00027813  \n",
      "Epoch: [108][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6762(0.6762) Grad: 0.0000  \n",
      "Epoch: [108][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6451) Grad: 0.0000  \n",
      "Epoch 108 - avg_train_loss: 0.6075  avg_val_loss: 0.6451  time: 0s\n",
      "Epoch 108 - Score: 0.9083\n",
      "Epoch: [109][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6564(0.6564) Grad: 4722.6680  LR: 0.00027813  \n",
      "Epoch: [109][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6115) Grad: 6150.3828  LR: 0.00027813  \n",
      "Epoch: [109][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6725(0.6725) Grad: 0.0000  \n",
      "Epoch: [109][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6431) Grad: 0.0000  \n",
      "Epoch 109 - avg_train_loss: 0.6115  avg_val_loss: 0.6431  time: 0s\n",
      "Epoch 109 - Score: 0.9125\n",
      "Epoch: [110][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5811(0.5811) Grad: 566.2535  LR: 0.00027813  \n",
      "Epoch: [110][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6144(0.6133) Grad: 4850.5181  LR: 0.00022528  \n",
      "Epoch: [110][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6709(0.6709) Grad: 0.0000  \n",
      "Epoch: [110][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6422) Grad: 0.0000  \n",
      "Epoch 110 - avg_train_loss: 0.6133  avg_val_loss: 0.6422  time: 0s\n",
      "Epoch 110 - Score: 0.9125\n",
      "Epoch: [111][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6208(0.6208) Grad: 4815.4150  LR: 0.00025032  \n",
      "Epoch: [111][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6149) Grad: 933.5239  LR: 0.00025032  \n",
      "Epoch: [111][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6762(0.6762) Grad: 0.0000  \n",
      "Epoch: [111][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6449) Grad: 0.0000  \n",
      "Epoch 111 - avg_train_loss: 0.6149  avg_val_loss: 0.6449  time: 0s\n",
      "Epoch 111 - Score: 0.9083\n",
      "Epoch: [112][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6222(0.6222) Grad: 4528.3218  LR: 0.00025032  \n",
      "Epoch: [112][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5874(0.6080) Grad: 783.3137  LR: 0.00025032  \n",
      "Epoch: [112][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6825(0.6825) Grad: 0.0000  \n",
      "Epoch: [112][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6483) Grad: 0.0000  \n",
      "Epoch 112 - avg_train_loss: 0.6080  avg_val_loss: 0.6483  time: 0s\n",
      "Epoch 112 - Score: 0.9000\n",
      "Epoch: [113][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6125(0.6125) Grad: 2841.3596  LR: 0.00025032  \n",
      "Epoch: [113][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6297(0.6143) Grad: 3047.0364  LR: 0.00025032  \n",
      "Epoch: [113][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6791(0.6791) Grad: 0.0000  \n",
      "Epoch: [113][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6465) Grad: 0.0000  \n",
      "Epoch 113 - avg_train_loss: 0.6143  avg_val_loss: 0.6465  time: 0s\n",
      "Epoch 113 - Score: 0.9125\n",
      "Epoch: [114][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5951(0.5951) Grad: 576.1910  LR: 0.00020276  \n",
      "Epoch: [114][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5948(0.6088) Grad: 710.2599  LR: 0.00022528  \n",
      "Epoch: [114][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6711(0.6711) Grad: 0.0000  \n",
      "Epoch: [114][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6423) Grad: 0.0000  \n",
      "Epoch 114 - avg_train_loss: 0.6088  avg_val_loss: 0.6423  time: 0s\n",
      "Epoch 114 - Score: 0.9167\n",
      "Epoch: [115][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5984(0.5984) Grad: 5931.8916  LR: 0.00022528  \n",
      "Epoch: [115][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6018(0.6042) Grad: 794.0875  LR: 0.00022528  \n",
      "Epoch: [115][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6710(0.6710) Grad: 0.0000  \n",
      "Epoch: [115][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6422) Grad: 0.0000  \n",
      "Epoch 115 - avg_train_loss: 0.6042  avg_val_loss: 0.6422  time: 0s\n",
      "Epoch 115 - Score: 0.9125\n",
      "Epoch: [116][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5805(0.5805) Grad: 2539.9724  LR: 0.00022528  \n",
      "Epoch: [116][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6270(0.6100) Grad: 2653.8350  LR: 0.00022528  \n",
      "Epoch: [116][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6703(0.6703) Grad: 0.0000  \n",
      "Epoch: [116][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6418) Grad: 0.0000  \n",
      "Epoch 116 - avg_train_loss: 0.6100  avg_val_loss: 0.6418  time: 0s\n",
      "Epoch 116 - Score: 0.9125\n",
      "Epoch: [117][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6198(0.6198) Grad: 6547.3809  LR: 0.00022528  \n",
      "Epoch: [117][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6277(0.6116) Grad: 1002.8387  LR: 0.00020276  \n",
      "Epoch: [117][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6768(0.6768) Grad: 0.0000  \n",
      "Epoch: [117][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6454) Grad: 0.0000  \n",
      "Epoch 117 - avg_train_loss: 0.6116  avg_val_loss: 0.6454  time: 0s\n",
      "Epoch 117 - Score: 0.9042\n",
      "Epoch: [118][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6224(0.6224) Grad: 5052.9160  LR: 0.00020276  \n",
      "Epoch: [118][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6149) Grad: 2222.1572  LR: 0.00020276  \n",
      "Epoch: [118][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6801(0.6801) Grad: 0.0000  \n",
      "Epoch: [118][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6472) Grad: 0.0000  \n",
      "Epoch 118 - avg_train_loss: 0.6149  avg_val_loss: 0.6472  time: 0s\n",
      "Epoch 118 - Score: 0.9042\n",
      "Epoch: [119][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6045(0.6045) Grad: 2425.1448  LR: 0.00020276  \n",
      "Epoch: [119][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5870(0.6086) Grad: 1580.1976  LR: 0.00020276  \n",
      "Epoch: [119][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6900(0.6900) Grad: 0.0000  \n",
      "Epoch: [119][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6524) Grad: 0.0000  \n",
      "Epoch 119 - avg_train_loss: 0.6086  avg_val_loss: 0.6524  time: 0s\n",
      "Epoch 119 - Score: 0.8958\n",
      "Epoch: [120][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6221(0.6221) Grad: 1784.0181  LR: 0.00020276  \n",
      "Epoch: [120][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6098(0.6132) Grad: 1302.0474  LR: 0.00018248  \n",
      "Epoch: [120][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6894(0.6894) Grad: 0.0000  \n",
      "Epoch: [120][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6521) Grad: 0.0000  \n",
      "Epoch 120 - avg_train_loss: 0.6132  avg_val_loss: 0.6521  time: 0s\n",
      "Epoch 120 - Score: 0.8958\n",
      "Epoch: [121][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 500.7752  LR: 0.00018248  \n",
      "Epoch: [121][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6137) Grad: 4686.2622  LR: 0.00018248  \n",
      "Epoch: [121][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6804(0.6804) Grad: 0.0000  \n",
      "Epoch: [121][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6473) Grad: 0.0000  \n",
      "Epoch 121 - avg_train_loss: 0.6137  avg_val_loss: 0.6473  time: 0s\n",
      "Epoch 121 - Score: 0.9083\n",
      "Epoch: [122][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5900(0.5900) Grad: 2513.5046  LR: 0.00018248  \n",
      "Epoch: [122][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5967(0.6109) Grad: 2500.7844  LR: 0.00018248  \n",
      "Epoch: [122][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6774(0.6774) Grad: 0.0000  \n",
      "Epoch: [122][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6457) Grad: 0.0000  \n",
      "Epoch 122 - avg_train_loss: 0.6109  avg_val_loss: 0.6457  time: 0s\n",
      "Epoch 122 - Score: 0.9083\n",
      "Epoch: [123][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6386(0.6386) Grad: 4469.1265  LR: 0.00018248  \n",
      "Epoch: [123][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6438(0.6103) Grad: 6064.0298  LR: 0.00016423  \n",
      "Epoch: [123][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6768(0.6768) Grad: 0.0000  \n",
      "Epoch: [123][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6453) Grad: 0.0000  \n",
      "Epoch 123 - avg_train_loss: 0.6103  avg_val_loss: 0.6453  time: 0s\n",
      "Epoch 123 - Score: 0.9083\n",
      "Epoch: [124][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6387(0.6387) Grad: 3378.4768  LR: 0.00016423  \n",
      "Epoch: [124][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6131(0.6125) Grad: 7015.7231  LR: 0.00016423  \n",
      "Epoch: [124][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6751(0.6751) Grad: 0.0000  \n",
      "Epoch: [124][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6445) Grad: 0.0000  \n",
      "Epoch 124 - avg_train_loss: 0.6125  avg_val_loss: 0.6445  time: 0s\n",
      "Epoch 124 - Score: 0.9083\n",
      "Epoch: [125][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6032(0.6032) Grad: 1041.1067  LR: 0.00016423  \n",
      "Epoch: [125][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6017(0.6059) Grad: 4234.3018  LR: 0.00016423  \n",
      "Epoch: [125][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6777(0.6777) Grad: 0.0000  \n",
      "Epoch: [125][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6459) Grad: 0.0000  \n",
      "Epoch 125 - avg_train_loss: 0.6059  avg_val_loss: 0.6459  time: 0s\n",
      "Epoch 125 - Score: 0.9083\n",
      "Epoch: [126][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5942(0.5942) Grad: 372.2762  LR: 0.00016423  \n",
      "Epoch: [126][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6006(0.6137) Grad: 3720.7874  LR: 0.00014781  \n",
      "Epoch: [126][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6808(0.6808) Grad: 0.0000  \n",
      "Epoch: [126][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6476) Grad: 0.0000  \n",
      "Epoch 126 - avg_train_loss: 0.6137  avg_val_loss: 0.6476  time: 0s\n",
      "Epoch 126 - Score: 0.9042\n",
      "Epoch: [127][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6071(0.6071) Grad: 3095.5481  LR: 0.00014781  \n",
      "Epoch: [127][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6048(0.6137) Grad: 2672.1560  LR: 0.00014781  \n",
      "Epoch: [127][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6832(0.6832) Grad: 0.0000  \n",
      "Epoch: [127][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6489) Grad: 0.0000  \n",
      "Epoch 127 - avg_train_loss: 0.6137  avg_val_loss: 0.6489  time: 0s\n",
      "Epoch 127 - Score: 0.9042\n",
      "Epoch: [128][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6101(0.6101) Grad: 674.1437  LR: 0.00014781  \n",
      "Epoch: [128][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6100) Grad: 1820.7966  LR: 0.00014781  \n",
      "Epoch: [128][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6837(0.6837) Grad: 0.0000  \n",
      "Epoch: [128][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6490) Grad: 0.0000  \n",
      "Epoch 128 - avg_train_loss: 0.6100  avg_val_loss: 0.6490  time: 0s\n",
      "Epoch 128 - Score: 0.9042\n",
      "Epoch: [129][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5885(0.5885) Grad: 3383.4387  LR: 0.00014781  \n",
      "Epoch: [129][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6439(0.6091) Grad: 1395.8398  LR: 0.00013303  \n",
      "Epoch: [129][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6810(0.6810) Grad: 0.0000  \n",
      "Epoch: [129][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6096(0.6476) Grad: 0.0000  \n",
      "Epoch 129 - avg_train_loss: 0.6091  avg_val_loss: 0.6476  time: 0s\n",
      "Epoch 129 - Score: 0.9083\n",
      "Epoch: [130][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5912(0.5912) Grad: 3473.0278  LR: 0.00013303  \n",
      "Epoch: [130][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6328(0.6109) Grad: 622.4011  LR: 0.00013303  \n",
      "Epoch: [130][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6812(0.6812) Grad: 0.0000  \n",
      "Epoch: [130][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6478) Grad: 0.0000  \n",
      "Epoch 130 - avg_train_loss: 0.6109  avg_val_loss: 0.6478  time: 0s\n",
      "Epoch 130 - Score: 0.9083\n",
      "Epoch: [131][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5959(0.5959) Grad: 1122.9308  LR: 0.00013303  \n",
      "Epoch: [131][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6111(0.6090) Grad: 841.9977  LR: 0.00013303  \n",
      "Epoch: [131][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6778(0.6778) Grad: 0.0000  \n",
      "Epoch: [131][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6459) Grad: 0.0000  \n",
      "Epoch 131 - avg_train_loss: 0.6090  avg_val_loss: 0.6459  time: 0s\n",
      "Epoch 131 - Score: 0.9083\n",
      "Epoch: [132][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6035) Grad: 636.5946  LR: 0.00013303  \n",
      "Epoch: [132][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6107) Grad: 2296.1106  LR: 0.00010775  \n",
      "Epoch: [132][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6794(0.6794) Grad: 0.0000  \n",
      "Epoch: [132][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6467) Grad: 0.0000  \n",
      "Epoch 132 - avg_train_loss: 0.6107  avg_val_loss: 0.6467  time: 0s\n",
      "Epoch 132 - Score: 0.9083\n",
      "Epoch: [133][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6481(0.6481) Grad: 2251.5732  LR: 0.00011973  \n",
      "Epoch: [133][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6144(0.6104) Grad: 6438.7354  LR: 0.00011973  \n",
      "Epoch: [133][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6809(0.6809) Grad: 0.0000  \n",
      "Epoch: [133][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6476) Grad: 0.0000  \n",
      "Epoch 133 - avg_train_loss: 0.6104  avg_val_loss: 0.6476  time: 0s\n",
      "Epoch 133 - Score: 0.9042\n",
      "Epoch: [134][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6167(0.6167) Grad: 1616.3601  LR: 0.00011973  \n",
      "Epoch: [134][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6041(0.6108) Grad: 512.6022  LR: 0.00011973  \n",
      "Epoch: [134][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6804(0.6804) Grad: 0.0000  \n",
      "Epoch: [134][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6473) Grad: 0.0000  \n",
      "Epoch 134 - avg_train_loss: 0.6108  avg_val_loss: 0.6473  time: 0s\n",
      "Epoch 134 - Score: 0.9042\n",
      "Epoch: [135][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5890(0.5890) Grad: 877.9354  LR: 0.00011973  \n",
      "Epoch: [135][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5802(0.6106) Grad: 2700.7351  LR: 0.00011973  \n",
      "Epoch: [135][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6815(0.6815) Grad: 0.0000  \n",
      "Epoch: [135][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6479) Grad: 0.0000  \n",
      "Epoch 135 - avg_train_loss: 0.6106  avg_val_loss: 0.6479  time: 0s\n",
      "Epoch 135 - Score: 0.9042\n",
      "Epoch: [136][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6027(0.6027) Grad: 503.9237  LR: 0.00009698  \n",
      "Epoch: [136][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6127(0.6118) Grad: 2177.2002  LR: 0.00010775  \n",
      "Epoch: [136][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6806(0.6806) Grad: 0.0000  \n",
      "Epoch: [136][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6474) Grad: 0.0000  \n",
      "Epoch 136 - avg_train_loss: 0.6118  avg_val_loss: 0.6474  time: 0s\n",
      "Epoch 136 - Score: 0.9042\n",
      "Epoch: [137][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 1518.4963  LR: 0.00010775  \n",
      "Epoch: [137][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5658(0.6096) Grad: 817.2776  LR: 0.00010775  \n",
      "Epoch: [137][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6823(0.6823) Grad: 0.0000  \n",
      "Epoch: [137][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6483) Grad: 0.0000  \n",
      "Epoch 137 - avg_train_loss: 0.6096  avg_val_loss: 0.6483  time: 0s\n",
      "Epoch 137 - Score: 0.9042\n",
      "Epoch: [138][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6112(0.6112) Grad: 4406.3926  LR: 0.00010775  \n",
      "Epoch: [138][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6029(0.6101) Grad: 502.3974  LR: 0.00010775  \n",
      "Epoch: [138][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6828(0.6828) Grad: 0.0000  \n",
      "Epoch: [138][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6486) Grad: 0.0000  \n",
      "Epoch 138 - avg_train_loss: 0.6101  avg_val_loss: 0.6486  time: 0s\n",
      "Epoch 138 - Score: 0.9042\n",
      "Epoch: [139][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6099(0.6099) Grad: 766.1461  LR: 0.00010775  \n",
      "Epoch: [139][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6197(0.6089) Grad: 3768.1646  LR: 0.00009698  \n",
      "Epoch: [139][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6841(0.6841) Grad: 0.0000  \n",
      "Epoch: [139][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6492) Grad: 0.0000  \n",
      "Epoch 139 - avg_train_loss: 0.6089  avg_val_loss: 0.6492  time: 0s\n",
      "Epoch 139 - Score: 0.9042\n",
      "Epoch: [140][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6245(0.6245) Grad: 656.2581  LR: 0.00009698  \n",
      "Epoch: [140][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6260(0.6110) Grad: 1629.1342  LR: 0.00009698  \n",
      "Epoch: [140][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6835(0.6835) Grad: 0.0000  \n",
      "Epoch: [140][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6489) Grad: 0.0000  \n",
      "Epoch 140 - avg_train_loss: 0.6110  avg_val_loss: 0.6489  time: 0s\n",
      "Epoch 140 - Score: 0.9042\n",
      "Epoch: [141][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6417(0.6417) Grad: 1146.8049  LR: 0.00009698  \n",
      "Epoch: [141][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5872(0.6111) Grad: 661.0981  LR: 0.00009698  \n",
      "Epoch: [141][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6820(0.6820) Grad: 0.0000  \n",
      "Epoch: [141][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6481) Grad: 0.0000  \n",
      "Epoch 141 - avg_train_loss: 0.6111  avg_val_loss: 0.6481  time: 0s\n",
      "Epoch 141 - Score: 0.9042\n",
      "Epoch: [142][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6405(0.6405) Grad: 4606.0396  LR: 0.00009698  \n",
      "Epoch: [142][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6100(0.6102) Grad: 3536.3770  LR: 0.00008728  \n",
      "Epoch: [142][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6798(0.6798) Grad: 0.0000  \n",
      "Epoch: [142][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6469) Grad: 0.0000  \n",
      "Epoch 142 - avg_train_loss: 0.6102  avg_val_loss: 0.6469  time: 0s\n",
      "Epoch 142 - Score: 0.9042\n",
      "Epoch: [143][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5879(0.5879) Grad: 626.5325  LR: 0.00008728  \n",
      "Epoch: [143][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6403(0.6085) Grad: 4171.4814  LR: 0.00008728  \n",
      "Epoch: [143][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6777(0.6777) Grad: 0.0000  \n",
      "Epoch: [143][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6458) Grad: 0.0000  \n",
      "Epoch 143 - avg_train_loss: 0.6085  avg_val_loss: 0.6458  time: 0s\n",
      "Epoch 143 - Score: 0.9083\n",
      "Epoch: [144][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6109(0.6109) Grad: 508.3172  LR: 0.00008728  \n",
      "Epoch: [144][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6112) Grad: 1800.8276  LR: 0.00008728  \n",
      "Epoch: [144][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6784(0.6784) Grad: 0.0000  \n",
      "Epoch: [144][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6462) Grad: 0.0000  \n",
      "Epoch 144 - avg_train_loss: 0.6112  avg_val_loss: 0.6462  time: 0s\n",
      "Epoch 144 - Score: 0.9042\n",
      "Epoch: [145][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6369(0.6369) Grad: 3138.0061  LR: 0.00008728  \n",
      "Epoch: [145][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6123) Grad: 6917.5469  LR: 0.00007855  \n",
      "Epoch: [145][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6777(0.6777) Grad: 0.0000  \n",
      "Epoch: [145][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6458) Grad: 0.0000  \n",
      "Epoch 145 - avg_train_loss: 0.6123  avg_val_loss: 0.6458  time: 0s\n",
      "Epoch 145 - Score: 0.9083\n",
      "Epoch: [146][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5895(0.5895) Grad: 4072.4890  LR: 0.00007855  \n",
      "Epoch: [146][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6119(0.6143) Grad: 1852.2065  LR: 0.00007855  \n",
      "Epoch: [146][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6768(0.6768) Grad: 0.0000  \n",
      "Epoch: [146][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6452) Grad: 0.0000  \n",
      "Epoch 146 - avg_train_loss: 0.6143  avg_val_loss: 0.6452  time: 0s\n",
      "Epoch 146 - Score: 0.9083\n",
      "Epoch: [147][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6024(0.6024) Grad: 843.8787  LR: 0.00007855  \n",
      "Epoch: [147][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5805(0.6050) Grad: 494.9362  LR: 0.00007855  \n",
      "Epoch: [147][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6792(0.6792) Grad: 0.0000  \n",
      "Epoch: [147][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6466) Grad: 0.0000  \n",
      "Epoch 147 - avg_train_loss: 0.6050  avg_val_loss: 0.6466  time: 0s\n",
      "Epoch 147 - Score: 0.9042\n",
      "Epoch: [148][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6057(0.6057) Grad: 988.2130  LR: 0.00007855  \n",
      "Epoch: [148][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6253(0.6085) Grad: 3821.9055  LR: 0.00007070  \n",
      "Epoch: [148][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6798(0.6798) Grad: 0.0000  \n",
      "Epoch: [148][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6470) Grad: 0.0000  \n",
      "Epoch 148 - avg_train_loss: 0.6085  avg_val_loss: 0.6470  time: 0s\n",
      "Epoch 148 - Score: 0.9042\n",
      "Epoch: [149][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6162(0.6162) Grad: 4792.1841  LR: 0.00007070  \n",
      "Epoch: [149][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5917(0.6054) Grad: 3719.5610  LR: 0.00007070  \n",
      "Epoch: [149][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6798(0.6798) Grad: 0.0000  \n",
      "Epoch: [149][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6469) Grad: 0.0000  \n",
      "Epoch 149 - avg_train_loss: 0.6054  avg_val_loss: 0.6469  time: 0s\n",
      "Epoch 149 - Score: 0.9042\n",
      "Epoch: [150][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6330(0.6330) Grad: 313.8487  LR: 0.00007070  \n",
      "Epoch: [150][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6231(0.6121) Grad: 6938.6265  LR: 0.00007070  \n",
      "Epoch: [150][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6808(0.6808) Grad: 0.0000  \n",
      "Epoch: [150][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6474) Grad: 0.0000  \n",
      "Epoch 150 - avg_train_loss: 0.6121  avg_val_loss: 0.6474  time: 0s\n",
      "Epoch 150 - Score: 0.9042\n",
      "Epoch: [151][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6117(0.6117) Grad: 1389.4194  LR: 0.00007070  \n",
      "Epoch: [151][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6193(0.6094) Grad: 3762.8657  LR: 0.00006363  \n",
      "Epoch: [151][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6786(0.6786) Grad: 0.0000  \n",
      "Epoch: [151][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6462) Grad: 0.0000  \n",
      "Epoch 151 - avg_train_loss: 0.6094  avg_val_loss: 0.6462  time: 0s\n",
      "Epoch 151 - Score: 0.9083\n",
      "Epoch: [152][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6126(0.6126) Grad: 4167.6787  LR: 0.00006363  \n",
      "Epoch: [152][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5997(0.6099) Grad: 4378.0747  LR: 0.00006363  \n",
      "Epoch: [152][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6802(0.6802) Grad: 0.0000  \n",
      "Epoch: [152][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6471) Grad: 0.0000  \n",
      "Epoch 152 - avg_train_loss: 0.6099  avg_val_loss: 0.6471  time: 0s\n",
      "Epoch 152 - Score: 0.9042\n",
      "Epoch: [153][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5909(0.5909) Grad: 8836.9707  LR: 0.00006363  \n",
      "Epoch: [153][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6081(0.6090) Grad: 3899.9775  LR: 0.00006363  \n",
      "Epoch: [153][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6820(0.6820) Grad: 0.0000  \n",
      "Epoch: [153][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6480) Grad: 0.0000  \n",
      "Epoch 153 - avg_train_loss: 0.6090  avg_val_loss: 0.6480  time: 0s\n",
      "Epoch 153 - Score: 0.9042\n",
      "Epoch: [154][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5969(0.5969) Grad: 1096.4185  LR: 0.00006363  \n",
      "Epoch: [154][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6129(0.6122) Grad: 2243.5862  LR: 0.00005154  \n",
      "Epoch: [154][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6849(0.6849) Grad: 0.0000  \n",
      "Epoch: [154][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6496) Grad: 0.0000  \n",
      "Epoch 154 - avg_train_loss: 0.6122  avg_val_loss: 0.6496  time: 0s\n",
      "Epoch 154 - Score: 0.9042\n",
      "Epoch: [155][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5735(0.5735) Grad: 753.7634  LR: 0.00005726  \n",
      "Epoch: [155][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6102(0.6100) Grad: 1634.6727  LR: 0.00005726  \n",
      "Epoch: [155][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6862(0.6862) Grad: 0.0000  \n",
      "Epoch: [155][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6503) Grad: 0.0000  \n",
      "Epoch 155 - avg_train_loss: 0.6100  avg_val_loss: 0.6503  time: 0s\n",
      "Epoch 155 - Score: 0.9000\n",
      "Epoch: [156][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6033(0.6033) Grad: 1615.8489  LR: 0.00005726  \n",
      "Epoch: [156][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6054(0.6091) Grad: 2965.4919  LR: 0.00005726  \n",
      "Epoch: [156][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6856(0.6856) Grad: 0.0000  \n",
      "Epoch: [156][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6500) Grad: 0.0000  \n",
      "Epoch 156 - avg_train_loss: 0.6091  avg_val_loss: 0.6500  time: 0s\n",
      "Epoch 156 - Score: 0.9042\n",
      "Epoch: [157][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6328(0.6328) Grad: 317.7029  LR: 0.00005726  \n",
      "Epoch: [157][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6266(0.6135) Grad: 1197.6790  LR: 0.00005726  \n",
      "Epoch: [157][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6864(0.6864) Grad: 0.0000  \n",
      "Epoch: [157][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6504) Grad: 0.0000  \n",
      "Epoch 157 - avg_train_loss: 0.6135  avg_val_loss: 0.6504  time: 0s\n",
      "Epoch 157 - Score: 0.9000\n",
      "Epoch: [158][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5953(0.5953) Grad: 789.7083  LR: 0.00004638  \n",
      "Epoch: [158][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6277(0.6088) Grad: 3867.1838  LR: 0.00005154  \n",
      "Epoch: [158][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6853(0.6853) Grad: 0.0000  \n",
      "Epoch: [158][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6498) Grad: 0.0000  \n",
      "Epoch 158 - avg_train_loss: 0.6088  avg_val_loss: 0.6498  time: 0s\n",
      "Epoch 158 - Score: 0.9042\n",
      "Epoch: [159][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6190(0.6190) Grad: 9277.6377  LR: 0.00005154  \n",
      "Epoch: [159][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5870(0.6101) Grad: 465.2290  LR: 0.00005154  \n",
      "Epoch: [159][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6836(0.6836) Grad: 0.0000  \n",
      "Epoch: [159][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6489) Grad: 0.0000  \n",
      "Epoch 159 - avg_train_loss: 0.6101  avg_val_loss: 0.6489  time: 0s\n",
      "Epoch 159 - Score: 0.9042\n",
      "Epoch: [160][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5967(0.5967) Grad: 1434.0367  LR: 0.00005154  \n",
      "Epoch: [160][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6109(0.6115) Grad: 758.4656  LR: 0.00005154  \n",
      "Epoch: [160][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6817(0.6817) Grad: 0.0000  \n",
      "Epoch: [160][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6478) Grad: 0.0000  \n",
      "Epoch 160 - avg_train_loss: 0.6115  avg_val_loss: 0.6478  time: 0s\n",
      "Epoch 160 - Score: 0.9042\n",
      "Epoch: [161][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6044(0.6044) Grad: 1556.8632  LR: 0.00005154  \n",
      "Epoch: [161][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6347(0.6108) Grad: 2723.9419  LR: 0.00004638  \n",
      "Epoch: [161][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6804(0.6804) Grad: 0.0000  \n",
      "Epoch: [161][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6471) Grad: 0.0000  \n",
      "Epoch 161 - avg_train_loss: 0.6108  avg_val_loss: 0.6471  time: 0s\n",
      "Epoch 161 - Score: 0.9042\n",
      "Epoch: [162][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6265(0.6265) Grad: 8671.1484  LR: 0.00004638  \n",
      "Epoch: [162][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5884(0.6133) Grad: 876.8753  LR: 0.00004638  \n",
      "Epoch: [162][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6813(0.6813) Grad: 0.0000  \n",
      "Epoch: [162][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6477) Grad: 0.0000  \n",
      "Epoch 162 - avg_train_loss: 0.6133  avg_val_loss: 0.6477  time: 0s\n",
      "Epoch 162 - Score: 0.9042\n",
      "Epoch: [163][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6270(0.6270) Grad: 525.2856  LR: 0.00004638  \n",
      "Epoch: [163][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6338(0.6121) Grad: 486.8897  LR: 0.00004638  \n",
      "Epoch: [163][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6811(0.6811) Grad: 0.0000  \n",
      "Epoch: [163][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6476) Grad: 0.0000  \n",
      "Epoch 163 - avg_train_loss: 0.6121  avg_val_loss: 0.6476  time: 0s\n",
      "Epoch 163 - Score: 0.9042\n",
      "Epoch: [164][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6045(0.6045) Grad: 2155.2327  LR: 0.00004638  \n",
      "Epoch: [164][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6207(0.6169) Grad: 2214.6667  LR: 0.00004175  \n",
      "Epoch: [164][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6816(0.6816) Grad: 0.0000  \n",
      "Epoch: [164][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6478) Grad: 0.0000  \n",
      "Epoch 164 - avg_train_loss: 0.6169  avg_val_loss: 0.6478  time: 0s\n",
      "Epoch 164 - Score: 0.9042\n",
      "Epoch: [165][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5890(0.5890) Grad: 3688.2102  LR: 0.00004175  \n",
      "Epoch: [165][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6254(0.6091) Grad: 1028.7542  LR: 0.00004175  \n",
      "Epoch: [165][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6798(0.6798) Grad: 0.0000  \n",
      "Epoch: [165][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6468) Grad: 0.0000  \n",
      "Epoch 165 - avg_train_loss: 0.6091  avg_val_loss: 0.6468  time: 0s\n",
      "Epoch 165 - Score: 0.9042\n",
      "Epoch: [166][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5992(0.5992) Grad: 5752.5503  LR: 0.00004175  \n",
      "Epoch: [166][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6310(0.6087) Grad: 4374.3223  LR: 0.00004175  \n",
      "Epoch: [166][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6813(0.6813) Grad: 0.0000  \n",
      "Epoch: [166][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6477) Grad: 0.0000  \n",
      "Epoch 166 - avg_train_loss: 0.6087  avg_val_loss: 0.6477  time: 0s\n",
      "Epoch 166 - Score: 0.9042\n",
      "Epoch: [167][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6263(0.6263) Grad: 904.6314  LR: 0.00004175  \n",
      "Epoch: [167][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6052(0.6095) Grad: 966.1269  LR: 0.00003757  \n",
      "Epoch: [167][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6835(0.6835) Grad: 0.0000  \n",
      "Epoch: [167][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6489) Grad: 0.0000  \n",
      "Epoch 167 - avg_train_loss: 0.6095  avg_val_loss: 0.6489  time: 0s\n",
      "Epoch 167 - Score: 0.9042\n",
      "Epoch: [168][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6072(0.6072) Grad: 4146.9209  LR: 0.00003757  \n",
      "Epoch: [168][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6037(0.6131) Grad: 1941.1112  LR: 0.00003757  \n",
      "Epoch: [168][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6834(0.6834) Grad: 0.0000  \n",
      "Epoch: [168][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6488) Grad: 0.0000  \n",
      "Epoch 168 - avg_train_loss: 0.6131  avg_val_loss: 0.6488  time: 0s\n",
      "Epoch 168 - Score: 0.9042\n",
      "Epoch: [169][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6465(0.6465) Grad: 2461.6516  LR: 0.00003757  \n",
      "Epoch: [169][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5969(0.6122) Grad: 3028.3413  LR: 0.00003757  \n",
      "Epoch: [169][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6843(0.6843) Grad: 0.0000  \n",
      "Epoch: [169][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6492) Grad: 0.0000  \n",
      "Epoch 169 - avg_train_loss: 0.6122  avg_val_loss: 0.6492  time: 0s\n",
      "Epoch 169 - Score: 0.9000\n",
      "Epoch: [170][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6306(0.6306) Grad: 4589.9922  LR: 0.00003757  \n",
      "Epoch: [170][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6008(0.6078) Grad: 6139.6509  LR: 0.00003381  \n",
      "Epoch: [170][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6829(0.6829) Grad: 0.0000  \n",
      "Epoch: [170][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6485) Grad: 0.0000  \n",
      "Epoch 170 - avg_train_loss: 0.6078  avg_val_loss: 0.6485  time: 0s\n",
      "Epoch 170 - Score: 0.9042\n",
      "Epoch: [171][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6393(0.6393) Grad: 3868.5908  LR: 0.00003381  \n",
      "Epoch: [171][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6027(0.6106) Grad: 3613.0364  LR: 0.00003381  \n",
      "Epoch: [171][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6832(0.6832) Grad: 0.0000  \n",
      "Epoch: [171][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6486) Grad: 0.0000  \n",
      "Epoch 171 - avg_train_loss: 0.6106  avg_val_loss: 0.6486  time: 0s\n",
      "Epoch 171 - Score: 0.9042\n",
      "Epoch: [172][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5954(0.5954) Grad: 437.8893  LR: 0.00003381  \n",
      "Epoch: [172][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6101(0.6048) Grad: 960.7285  LR: 0.00003381  \n",
      "Epoch: [172][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6805(0.6805) Grad: 0.0000  \n",
      "Epoch: [172][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6472) Grad: 0.0000  \n",
      "Epoch 172 - avg_train_loss: 0.6048  avg_val_loss: 0.6472  time: 0s\n",
      "Epoch 172 - Score: 0.9042\n",
      "Epoch: [173][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6380(0.6380) Grad: 4707.2344  LR: 0.00003381  \n",
      "Epoch: [173][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6107) Grad: 909.1201  LR: 0.00003043  \n",
      "Epoch: [173][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6777(0.6777) Grad: 0.0000  \n",
      "Epoch: [173][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6457) Grad: 0.0000  \n",
      "Epoch 173 - avg_train_loss: 0.6107  avg_val_loss: 0.6457  time: 0s\n",
      "Epoch 173 - Score: 0.9083\n",
      "Epoch: [174][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6058(0.6058) Grad: 1377.6588  LR: 0.00003043  \n",
      "Epoch: [174][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5947(0.6101) Grad: 420.9977  LR: 0.00003043  \n",
      "Epoch: [174][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6812(0.6812) Grad: 0.0000  \n",
      "Epoch: [174][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6476) Grad: 0.0000  \n",
      "Epoch 174 - avg_train_loss: 0.6101  avg_val_loss: 0.6476  time: 0s\n",
      "Epoch 174 - Score: 0.9042\n",
      "Epoch: [175][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6362(0.6362) Grad: 5343.6860  LR: 0.00003043  \n",
      "Epoch: [175][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6319(0.6192) Grad: 5001.6206  LR: 0.00003043  \n",
      "Epoch: [175][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6820(0.6820) Grad: 0.0000  \n",
      "Epoch: [175][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6480) Grad: 0.0000  \n",
      "Epoch 175 - avg_train_loss: 0.6192  avg_val_loss: 0.6480  time: 0s\n",
      "Epoch 175 - Score: 0.9042\n",
      "Epoch: [176][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6116(0.6116) Grad: 1563.4167  LR: 0.00003043  \n",
      "Epoch: [176][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6082) Grad: 2700.9797  LR: 0.00002465  \n",
      "Epoch: [176][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6818(0.6818) Grad: 0.0000  \n",
      "Epoch: [176][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6479) Grad: 0.0000  \n",
      "Epoch 176 - avg_train_loss: 0.6082  avg_val_loss: 0.6479  time: 0s\n",
      "Epoch 176 - Score: 0.9042\n",
      "Epoch: [177][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6270(0.6270) Grad: 7879.0225  LR: 0.00002739  \n",
      "Epoch: [177][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6052(0.6073) Grad: 1344.5333  LR: 0.00002739  \n",
      "Epoch: [177][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6802(0.6802) Grad: 0.0000  \n",
      "Epoch: [177][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6471) Grad: 0.0000  \n",
      "Epoch 177 - avg_train_loss: 0.6073  avg_val_loss: 0.6471  time: 0s\n",
      "Epoch 177 - Score: 0.9042\n",
      "Epoch: [178][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5961(0.5961) Grad: 691.1462  LR: 0.00002739  \n",
      "Epoch: [178][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5815(0.6067) Grad: 1480.6163  LR: 0.00002739  \n",
      "Epoch: [178][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6824(0.6824) Grad: 0.0000  \n",
      "Epoch: [178][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6483) Grad: 0.0000  \n",
      "Epoch 178 - avg_train_loss: 0.6067  avg_val_loss: 0.6483  time: 0s\n",
      "Epoch 178 - Score: 0.9042\n",
      "Epoch: [179][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6222(0.6222) Grad: 4086.7515  LR: 0.00002739  \n",
      "Epoch: [179][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5719(0.6128) Grad: 5518.3242  LR: 0.00002739  \n",
      "Epoch: [179][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6841(0.6841) Grad: 0.0000  \n",
      "Epoch: [179][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6492) Grad: 0.0000  \n",
      "Epoch 179 - avg_train_loss: 0.6128  avg_val_loss: 0.6492  time: 0s\n",
      "Epoch 179 - Score: 0.9000\n",
      "Epoch: [180][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6340(0.6340) Grad: 848.9581  LR: 0.00002219  \n",
      "Epoch: [180][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6109(0.6141) Grad: 895.2908  LR: 0.00002465  \n",
      "Epoch: [180][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6813(0.6813) Grad: 0.0000  \n",
      "Epoch: [180][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6477) Grad: 0.0000  \n",
      "Epoch 180 - avg_train_loss: 0.6141  avg_val_loss: 0.6477  time: 0s\n",
      "Epoch 180 - Score: 0.9042\n",
      "Epoch: [181][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6117(0.6117) Grad: 2346.5896  LR: 0.00002465  \n",
      "Epoch: [181][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6120(0.6127) Grad: 2088.6018  LR: 0.00002465  \n",
      "Epoch: [181][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6804(0.6804) Grad: 0.0000  \n",
      "Epoch: [181][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6472) Grad: 0.0000  \n",
      "Epoch 181 - avg_train_loss: 0.6127  avg_val_loss: 0.6472  time: 0s\n",
      "Epoch 181 - Score: 0.9042\n",
      "Epoch: [182][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5951(0.5951) Grad: 1014.8083  LR: 0.00002465  \n",
      "Epoch: [182][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6180(0.6114) Grad: 799.6396  LR: 0.00002465  \n",
      "Epoch: [182][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6822(0.6822) Grad: 0.0000  \n",
      "Epoch: [182][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6481) Grad: 0.0000  \n",
      "Epoch 182 - avg_train_loss: 0.6114  avg_val_loss: 0.6481  time: 0s\n",
      "Epoch 182 - Score: 0.9042\n",
      "Epoch: [183][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6263(0.6263) Grad: 1714.5516  LR: 0.00002465  \n",
      "Epoch: [183][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6177(0.6124) Grad: 840.5981  LR: 0.00002219  \n",
      "Epoch: [183][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6810(0.6810) Grad: 0.0000  \n",
      "Epoch: [183][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6475) Grad: 0.0000  \n",
      "Epoch 183 - avg_train_loss: 0.6124  avg_val_loss: 0.6475  time: 0s\n",
      "Epoch 183 - Score: 0.9042\n",
      "Epoch: [184][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5991(0.5991) Grad: 4139.8975  LR: 0.00002219  \n",
      "Epoch: [184][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6189(0.6174) Grad: 1150.5437  LR: 0.00002219  \n",
      "Epoch: [184][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6797(0.6797) Grad: 0.0000  \n",
      "Epoch: [184][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6468) Grad: 0.0000  \n",
      "Epoch 184 - avg_train_loss: 0.6174  avg_val_loss: 0.6468  time: 0s\n",
      "Epoch 184 - Score: 0.9083\n",
      "Epoch: [185][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6285(0.6285) Grad: 5867.0718  LR: 0.00002219  \n",
      "Epoch: [185][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6273(0.6135) Grad: 3785.3311  LR: 0.00002219  \n",
      "Epoch: [185][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6811(0.6811) Grad: 0.0000  \n",
      "Epoch: [185][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6475) Grad: 0.0000  \n",
      "Epoch 185 - avg_train_loss: 0.6135  avg_val_loss: 0.6475  time: 0s\n",
      "Epoch 185 - Score: 0.9042\n",
      "Epoch: [186][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6463(0.6463) Grad: 6519.5190  LR: 0.00002219  \n",
      "Epoch: [186][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6023(0.6107) Grad: 903.4237  LR: 0.00001997  \n",
      "Epoch: [186][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6814(0.6814) Grad: 0.0000  \n",
      "Epoch: [186][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6091(0.6477) Grad: 0.0000  \n",
      "Epoch 186 - avg_train_loss: 0.6107  avg_val_loss: 0.6477  time: 0s\n",
      "Epoch 186 - Score: 0.9042\n",
      "Epoch: [187][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5960(0.5960) Grad: 1303.1414  LR: 0.00001997  \n",
      "Epoch: [187][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6119(0.6104) Grad: 2731.3159  LR: 0.00001997  \n",
      "Epoch: [187][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6818(0.6818) Grad: 0.0000  \n",
      "Epoch: [187][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6479) Grad: 0.0000  \n",
      "Epoch 187 - avg_train_loss: 0.6104  avg_val_loss: 0.6479  time: 0s\n",
      "Epoch 187 - Score: 0.9042\n",
      "Epoch: [188][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5902(0.5902) Grad: 5182.6968  LR: 0.00001997  \n",
      "Epoch: [188][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6417(0.6105) Grad: 4109.7944  LR: 0.00001997  \n",
      "Epoch: [188][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6842(0.6842) Grad: 0.0000  \n",
      "Epoch: [188][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6492) Grad: 0.0000  \n",
      "Epoch 188 - avg_train_loss: 0.6105  avg_val_loss: 0.6492  time: 0s\n",
      "Epoch 188 - Score: 0.9000\n",
      "Epoch: [189][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6197(0.6197) Grad: 1623.7169  LR: 0.00001997  \n",
      "Epoch: [189][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6012(0.6121) Grad: 8314.6934  LR: 0.00001797  \n",
      "Epoch: [189][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6878(0.6878) Grad: 0.0000  \n",
      "Epoch: [189][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6511) Grad: 0.0000  \n",
      "Epoch 189 - avg_train_loss: 0.6121  avg_val_loss: 0.6511  time: 0s\n",
      "Epoch 189 - Score: 0.9000\n",
      "Epoch: [190][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6267(0.6267) Grad: 991.8280  LR: 0.00001797  \n",
      "Epoch: [190][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5866(0.6049) Grad: 8136.4180  LR: 0.00001797  \n",
      "Epoch: [190][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6845(0.6845) Grad: 0.0000  \n",
      "Epoch: [190][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6493) Grad: 0.0000  \n",
      "Epoch 190 - avg_train_loss: 0.6049  avg_val_loss: 0.6493  time: 0s\n",
      "Epoch 190 - Score: 0.9042\n",
      "Epoch: [191][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6448) Grad: 3822.6763  LR: 0.00001797  \n",
      "Epoch: [191][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6343(0.6101) Grad: 2006.2103  LR: 0.00001797  \n",
      "Epoch: [191][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6835(0.6835) Grad: 0.0000  \n",
      "Epoch: [191][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6488) Grad: 0.0000  \n",
      "Epoch 191 - avg_train_loss: 0.6101  avg_val_loss: 0.6488  time: 0s\n",
      "Epoch 191 - Score: 0.9042\n",
      "Epoch: [192][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6022(0.6022) Grad: 779.9210  LR: 0.00001797  \n",
      "Epoch: [192][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6099) Grad: 2445.5413  LR: 0.00001617  \n",
      "Epoch: [192][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6835(0.6835) Grad: 0.0000  \n",
      "Epoch: [192][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6488) Grad: 0.0000  \n",
      "Epoch 192 - avg_train_loss: 0.6099  avg_val_loss: 0.6488  time: 0s\n",
      "Epoch 192 - Score: 0.9042\n",
      "Epoch: [193][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6281(0.6281) Grad: 2048.3198  LR: 0.00001617  \n",
      "Epoch: [193][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5894(0.6125) Grad: 2399.7112  LR: 0.00001617  \n",
      "Epoch: [193][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6851(0.6851) Grad: 0.0000  \n",
      "Epoch: [193][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6497) Grad: 0.0000  \n",
      "Epoch 193 - avg_train_loss: 0.6125  avg_val_loss: 0.6497  time: 0s\n",
      "Epoch 193 - Score: 0.9042\n",
      "Epoch: [194][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6783(0.6783) Grad: 10191.4785  LR: 0.00001617  \n",
      "Epoch: [194][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6312(0.6135) Grad: 5939.6250  LR: 0.00001617  \n",
      "Epoch: [194][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6853(0.6853) Grad: 0.0000  \n",
      "Epoch: [194][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6498) Grad: 0.0000  \n",
      "Epoch 194 - avg_train_loss: 0.6135  avg_val_loss: 0.6498  time: 0s\n",
      "Epoch 194 - Score: 0.9000\n",
      "Epoch: [195][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6048(0.6048) Grad: 7485.2417  LR: 0.00001617  \n",
      "Epoch: [195][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6029(0.6106) Grad: 690.4882  LR: 0.00001456  \n",
      "Epoch: [195][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6847(0.6847) Grad: 0.0000  \n",
      "Epoch: [195][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6495) Grad: 0.0000  \n",
      "Epoch 195 - avg_train_loss: 0.6106  avg_val_loss: 0.6495  time: 0s\n",
      "Epoch 195 - Score: 0.9000\n",
      "Epoch: [196][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6086(0.6086) Grad: 3939.8484  LR: 0.00001456  \n",
      "Epoch: [196][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5787(0.6093) Grad: 3300.2065  LR: 0.00001456  \n",
      "Epoch: [196][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6825(0.6825) Grad: 0.0000  \n",
      "Epoch: [196][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6483) Grad: 0.0000  \n",
      "Epoch 196 - avg_train_loss: 0.6093  avg_val_loss: 0.6483  time: 0s\n",
      "Epoch 196 - Score: 0.9042\n",
      "Epoch: [197][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6283(0.6283) Grad: 3058.2305  LR: 0.00001456  \n",
      "Epoch: [197][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5993(0.6107) Grad: 5003.6431  LR: 0.00001456  \n",
      "Epoch: [197][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6829(0.6829) Grad: 0.0000  \n",
      "Epoch: [197][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6486) Grad: 0.0000  \n",
      "Epoch 197 - avg_train_loss: 0.6107  avg_val_loss: 0.6486  time: 0s\n",
      "Epoch 197 - Score: 0.9042\n",
      "Epoch: [198][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5822(0.5822) Grad: 2813.3484  LR: 0.00001456  \n",
      "Epoch: [198][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5999(0.6080) Grad: 6526.8823  LR: 0.00001179  \n",
      "Epoch: [198][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6848(0.6848) Grad: 0.0000  \n",
      "Epoch: [198][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6094(0.6496) Grad: 0.0000  \n",
      "Epoch 198 - avg_train_loss: 0.6080  avg_val_loss: 0.6496  time: 0s\n",
      "Epoch 198 - Score: 0.9042\n",
      "Epoch: [199][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5964(0.5964) Grad: 521.3856  LR: 0.00001310  \n",
      "Epoch: [199][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6038(0.6066) Grad: 1048.4862  LR: 0.00001310  \n",
      "Epoch: [199][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6834(0.6834) Grad: 0.0000  \n",
      "Epoch: [199][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6488) Grad: 0.0000  \n",
      "Epoch 199 - avg_train_loss: 0.6066  avg_val_loss: 0.6488  time: 0s\n",
      "Epoch 199 - Score: 0.9042\n",
      "Epoch: [200][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6317(0.6317) Grad: 4247.8745  LR: 0.00001310  \n",
      "Epoch: [200][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6104) Grad: 5015.9346  LR: 0.00001310  \n",
      "Epoch: [200][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6850(0.6850) Grad: 0.0000  \n",
      "Epoch: [200][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6497) Grad: 0.0000  \n",
      "Epoch 200 - avg_train_loss: 0.6104  avg_val_loss: 0.6497  time: 0s\n",
      "Epoch 200 - Score: 0.9042\n",
      "========== fold: 3 result ==========\n",
      "Score: 0.9333\n",
      "========== fold: 4 training ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 1.0999(1.0999) Grad: 140039.4688  LR: 0.01000000  \n",
      "Epoch: [1][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8573(0.9452) Grad: 14236.0137  LR: 0.01000000  \n",
      "Epoch: [1][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4493(1.4493) Grad: 0.0000  \n",
      "Epoch: [1][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.5059(1.4757) Grad: 0.0000  \n",
      "Epoch 1 - avg_train_loss: 0.9452  avg_val_loss: 1.4757  time: 0s\n",
      "Epoch 1 - Score: 0.0750\n",
      "Epoch 1 - Save Best Score: 0.0750 Model\n",
      "Epoch: [2][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8783(0.8783) Grad: 15570.6152  LR: 0.01000000  \n",
      "Epoch: [2][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8392(0.8718) Grad: 14979.4590  LR: 0.01000000  \n",
      "Epoch: [2][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4453(1.4453) Grad: 0.0000  \n",
      "Epoch: [2][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.5027(1.4721) Grad: 0.0000  \n",
      "Epoch 2 - avg_train_loss: 0.8718  avg_val_loss: 1.4721  time: 0s\n",
      "Epoch 2 - Score: 0.0750\n",
      "Epoch: [3][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8446(0.8446) Grad: 14218.5928  LR: 0.01000000  \n",
      "Epoch: [3][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8282(0.8346) Grad: 12741.6133  LR: 0.01000000  \n",
      "Epoch: [3][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4469(1.4469) Grad: 0.0000  \n",
      "Epoch: [3][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.5051(1.4741) Grad: 0.0000  \n",
      "Epoch 3 - avg_train_loss: 0.8346  avg_val_loss: 1.4741  time: 0s\n",
      "Epoch 3 - Score: 0.0750\n",
      "Epoch: [4][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8126(0.8126) Grad: 12724.8994  LR: 0.00810000  \n",
      "Epoch: [4][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7999(0.8042) Grad: 11304.1875  LR: 0.00900000  \n",
      "Epoch: [4][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4041(1.4041) Grad: 0.0000  \n",
      "Epoch: [4][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.4502(1.4256) Grad: 0.0000  \n",
      "Epoch 4 - avg_train_loss: 0.8042  avg_val_loss: 1.4256  time: 0s\n",
      "Epoch 4 - Score: 0.0833\n",
      "Epoch 4 - Save Best Score: 0.0833 Model\n",
      "Epoch: [5][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7678(0.7678) Grad: 12144.8701  LR: 0.00900000  \n",
      "Epoch: [5][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7874(0.7772) Grad: 8573.7617  LR: 0.00900000  \n",
      "Epoch: [5][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.2193(1.2193) Grad: 0.0000  \n",
      "Epoch: [5][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 1.2506(1.2339) Grad: 0.0000  \n",
      "Epoch 5 - avg_train_loss: 0.7772  avg_val_loss: 1.2339  time: 0s\n",
      "Epoch 5 - Score: 0.2000\n",
      "Epoch 5 - Save Best Score: 0.2000 Model\n",
      "Epoch: [6][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7998(0.7998) Grad: 8185.5493  LR: 0.00900000  \n",
      "Epoch: [6][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7428(0.7578) Grad: 8052.7188  LR: 0.00900000  \n",
      "Epoch: [6][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7829(0.7829) Grad: 0.0000  \n",
      "Epoch: [6][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.8194(0.8000) Grad: 0.0000  \n",
      "Epoch 6 - avg_train_loss: 0.7578  avg_val_loss: 0.8000  time: 0s\n",
      "Epoch 6 - Score: 0.8875\n",
      "Epoch 6 - Save Best Score: 0.8875 Model\n",
      "Epoch: [7][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7258(0.7258) Grad: 7763.9678  LR: 0.00900000  \n",
      "Epoch: [7][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7616(0.7376) Grad: 10340.4062  LR: 0.00810000  \n",
      "Epoch: [7][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7347(0.7347) Grad: 0.0000  \n",
      "Epoch: [7][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7672(0.7499) Grad: 0.0000  \n",
      "Epoch 7 - avg_train_loss: 0.7376  avg_val_loss: 0.7499  time: 0s\n",
      "Epoch 7 - Score: 0.8875\n",
      "Epoch: [8][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7349(0.7349) Grad: 7520.7114  LR: 0.00810000  \n",
      "Epoch: [8][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7673(0.7226) Grad: 7657.3184  LR: 0.00810000  \n",
      "Epoch: [8][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7384(0.7384) Grad: 0.0000  \n",
      "Epoch: [8][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7668(0.7517) Grad: 0.0000  \n",
      "Epoch 8 - avg_train_loss: 0.7226  avg_val_loss: 0.7517  time: 0s\n",
      "Epoch 8 - Score: 0.8833\n",
      "Epoch: [9][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7527(0.7527) Grad: 6642.0693  LR: 0.00810000  \n",
      "Epoch: [9][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6724(0.7101) Grad: 6205.6714  LR: 0.00810000  \n",
      "Epoch: [9][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7352(0.7352) Grad: 0.0000  \n",
      "Epoch: [9][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7549(0.7444) Grad: 0.0000  \n",
      "Epoch 9 - avg_train_loss: 0.7101  avg_val_loss: 0.7444  time: 0s\n",
      "Epoch 9 - Score: 0.8792\n",
      "Epoch: [10][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7081(0.7081) Grad: 6072.8618  LR: 0.00810000  \n",
      "Epoch: [10][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7107(0.6981) Grad: 4711.7910  LR: 0.00729000  \n",
      "Epoch: [10][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7323(0.7323) Grad: 0.0000  \n",
      "Epoch: [10][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7492(0.7402) Grad: 0.0000  \n",
      "Epoch 10 - avg_train_loss: 0.6981  avg_val_loss: 0.7402  time: 0s\n",
      "Epoch 10 - Score: 0.8792\n",
      "Epoch: [11][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6868(0.6868) Grad: 5841.6294  LR: 0.00729000  \n",
      "Epoch: [11][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6487(0.6907) Grad: 4676.7876  LR: 0.00729000  \n",
      "Epoch: [11][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7135(0.7135) Grad: 0.0000  \n",
      "Epoch: [11][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7333(0.7228) Grad: 0.0000  \n",
      "Epoch 11 - avg_train_loss: 0.6907  avg_val_loss: 0.7228  time: 0s\n",
      "Epoch 11 - Score: 0.8833\n",
      "Epoch: [12][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6712(0.6712) Grad: 5061.5557  LR: 0.00729000  \n",
      "Epoch: [12][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6694(0.6836) Grad: 5199.5142  LR: 0.00729000  \n",
      "Epoch: [12][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6993(0.6993) Grad: 0.0000  \n",
      "Epoch: [12][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7334(0.7152) Grad: 0.0000  \n",
      "Epoch 12 - avg_train_loss: 0.6836  avg_val_loss: 0.7152  time: 0s\n",
      "Epoch 12 - Score: 0.8875\n",
      "Epoch: [13][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6597(0.6597) Grad: 3803.1392  LR: 0.00729000  \n",
      "Epoch: [13][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7078(0.6710) Grad: 4156.5991  LR: 0.00656100  \n",
      "Epoch: [13][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6927(0.6927) Grad: 0.0000  \n",
      "Epoch: [13][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7276(0.7090) Grad: 0.0000  \n",
      "Epoch 13 - avg_train_loss: 0.6710  avg_val_loss: 0.7090  time: 0s\n",
      "Epoch 13 - Score: 0.8875\n",
      "Epoch: [14][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6735(0.6735) Grad: 3817.8196  LR: 0.00656100  \n",
      "Epoch: [14][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6659) Grad: 4408.6284  LR: 0.00656100  \n",
      "Epoch: [14][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6939(0.6939) Grad: 0.0000  \n",
      "Epoch: [14][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7192(0.7057) Grad: 0.0000  \n",
      "Epoch 14 - avg_train_loss: 0.6659  avg_val_loss: 0.7057  time: 0s\n",
      "Epoch 14 - Score: 0.8833\n",
      "Epoch: [15][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6438(0.6438) Grad: 3079.6121  LR: 0.00656100  \n",
      "Epoch: [15][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6836(0.6666) Grad: 2549.2495  LR: 0.00656100  \n",
      "Epoch: [15][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7318(0.7318) Grad: 0.0000  \n",
      "Epoch: [15][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7284(0.7302) Grad: 0.0000  \n",
      "Epoch 15 - avg_train_loss: 0.6666  avg_val_loss: 0.7302  time: 0s\n",
      "Epoch 15 - Score: 0.8375\n",
      "Epoch: [16][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6463(0.6463) Grad: 2841.4883  LR: 0.00656100  \n",
      "Epoch: [16][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6748(0.6626) Grad: 2413.2908  LR: 0.00590490  \n",
      "Epoch: [16][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6873(0.6873) Grad: 0.0000  \n",
      "Epoch: [16][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7137(0.6996) Grad: 0.0000  \n",
      "Epoch 16 - avg_train_loss: 0.6626  avg_val_loss: 0.6996  time: 0s\n",
      "Epoch 16 - Score: 0.8708\n",
      "Epoch: [17][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6682(0.6682) Grad: 5174.8491  LR: 0.00590490  \n",
      "Epoch: [17][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6307(0.6557) Grad: 2434.5505  LR: 0.00590490  \n",
      "Epoch: [17][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6864(0.6864) Grad: 0.0000  \n",
      "Epoch: [17][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7165(0.7005) Grad: 0.0000  \n",
      "Epoch 17 - avg_train_loss: 0.6557  avg_val_loss: 0.7005  time: 0s\n",
      "Epoch 17 - Score: 0.8833\n",
      "Epoch: [18][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6364(0.6364) Grad: 2158.4927  LR: 0.00590490  \n",
      "Epoch: [18][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6668(0.6533) Grad: 2768.5398  LR: 0.00590490  \n",
      "Epoch: [18][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6793(0.6793) Grad: 0.0000  \n",
      "Epoch: [18][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7113(0.6942) Grad: 0.0000  \n",
      "Epoch 18 - avg_train_loss: 0.6533  avg_val_loss: 0.6942  time: 0s\n",
      "Epoch 18 - Score: 0.8833\n",
      "Epoch: [19][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6349(0.6349) Grad: 3252.6130  LR: 0.00590490  \n",
      "Epoch: [19][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6572(0.6489) Grad: 2818.8518  LR: 0.00531441  \n",
      "Epoch: [19][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6618(0.6618) Grad: 0.0000  \n",
      "Epoch: [19][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7030(0.6810) Grad: 0.0000  \n",
      "Epoch 19 - avg_train_loss: 0.6489  avg_val_loss: 0.6810  time: 0s\n",
      "Epoch 19 - Score: 0.8917\n",
      "Epoch 19 - Save Best Score: 0.8917 Model\n",
      "Epoch: [20][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6306(0.6306) Grad: 3166.6641  LR: 0.00531441  \n",
      "Epoch: [20][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6419(0.6418) Grad: 3016.0432  LR: 0.00531441  \n",
      "Epoch: [20][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6645(0.6645) Grad: 0.0000  \n",
      "Epoch: [20][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.7035(0.6827) Grad: 0.0000  \n",
      "Epoch 20 - avg_train_loss: 0.6418  avg_val_loss: 0.6827  time: 0s\n",
      "Epoch 20 - Score: 0.8875\n",
      "Epoch: [21][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6797(0.6797) Grad: 1659.8506  LR: 0.00531441  \n",
      "Epoch: [21][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6527(0.6478) Grad: 2100.4160  LR: 0.00531441  \n",
      "Epoch: [21][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6541(0.6541) Grad: 0.0000  \n",
      "Epoch: [21][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6958(0.6736) Grad: 0.0000  \n",
      "Epoch 21 - avg_train_loss: 0.6478  avg_val_loss: 0.6736  time: 0s\n",
      "Epoch 21 - Score: 0.8917\n",
      "Epoch: [22][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6642(0.6642) Grad: 1370.0026  LR: 0.00531441  \n",
      "Epoch: [22][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6584(0.6470) Grad: 2209.6477  LR: 0.00430467  \n",
      "Epoch: [22][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6439(0.6439) Grad: 0.0000  \n",
      "Epoch: [22][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6761(0.6589) Grad: 0.0000  \n",
      "Epoch 22 - avg_train_loss: 0.6470  avg_val_loss: 0.6589  time: 0s\n",
      "Epoch 22 - Score: 0.9042\n",
      "Epoch 22 - Save Best Score: 0.9042 Model\n",
      "Epoch: [23][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6141(0.6141) Grad: 3141.4910  LR: 0.00478297  \n",
      "Epoch: [23][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6641(0.6417) Grad: 3728.5784  LR: 0.00478297  \n",
      "Epoch: [23][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6662(0.6662) Grad: 0.0000  \n",
      "Epoch: [23][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6935(0.6790) Grad: 0.0000  \n",
      "Epoch 23 - avg_train_loss: 0.6417  avg_val_loss: 0.6790  time: 0s\n",
      "Epoch 23 - Score: 0.8833\n",
      "Epoch: [24][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6153(0.6153) Grad: 1808.1113  LR: 0.00478297  \n",
      "Epoch: [24][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6422(0.6468) Grad: 1846.4449  LR: 0.00478297  \n",
      "Epoch: [24][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6696(0.6696) Grad: 0.0000  \n",
      "Epoch: [24][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6899(0.6791) Grad: 0.0000  \n",
      "Epoch 24 - avg_train_loss: 0.6468  avg_val_loss: 0.6791  time: 0s\n",
      "Epoch 24 - Score: 0.8875\n",
      "Epoch: [25][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6321(0.6321) Grad: 1525.7729  LR: 0.00478297  \n",
      "Epoch: [25][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6174(0.6381) Grad: 1782.0146  LR: 0.00478297  \n",
      "Epoch: [25][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6745(0.6745) Grad: 0.0000  \n",
      "Epoch: [25][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6871(0.6804) Grad: 0.0000  \n",
      "Epoch 25 - avg_train_loss: 0.6381  avg_val_loss: 0.6804  time: 0s\n",
      "Epoch 25 - Score: 0.8875\n",
      "Epoch: [26][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6215(0.6215) Grad: 1941.7936  LR: 0.00387420  \n",
      "Epoch: [26][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6401) Grad: 1754.5182  LR: 0.00430467  \n",
      "Epoch: [26][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6511(0.6511) Grad: 0.0000  \n",
      "Epoch: [26][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6825(0.6658) Grad: 0.0000  \n",
      "Epoch 26 - avg_train_loss: 0.6401  avg_val_loss: 0.6658  time: 0s\n",
      "Epoch 26 - Score: 0.9042\n",
      "Epoch: [27][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6203(0.6203) Grad: 1174.1467  LR: 0.00430467  \n",
      "Epoch: [27][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6401(0.6372) Grad: 898.4889  LR: 0.00430467  \n",
      "Epoch: [27][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6453(0.6453) Grad: 0.0000  \n",
      "Epoch: [27][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6898(0.6661) Grad: 0.0000  \n",
      "Epoch 27 - avg_train_loss: 0.6372  avg_val_loss: 0.6661  time: 0s\n",
      "Epoch 27 - Score: 0.8958\n",
      "Epoch: [28][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6259(0.6259) Grad: 3332.5117  LR: 0.00430467  \n",
      "Epoch: [28][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6439(0.6383) Grad: 2240.2026  LR: 0.00430467  \n",
      "Epoch: [28][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6448) Grad: 0.0000  \n",
      "Epoch: [28][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6752(0.6590) Grad: 0.0000  \n",
      "Epoch 28 - avg_train_loss: 0.6383  avg_val_loss: 0.6590  time: 0s\n",
      "Epoch 28 - Score: 0.9083\n",
      "Epoch 28 - Save Best Score: 0.9083 Model\n",
      "Epoch: [29][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6054(0.6054) Grad: 1777.6021  LR: 0.00430467  \n",
      "Epoch: [29][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6466(0.6322) Grad: 2333.5828  LR: 0.00387420  \n",
      "Epoch: [29][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6436(0.6436) Grad: 0.0000  \n",
      "Epoch: [29][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6725(0.6571) Grad: 0.0000  \n",
      "Epoch 29 - avg_train_loss: 0.6322  avg_val_loss: 0.6571  time: 0s\n",
      "Epoch 29 - Score: 0.9083\n",
      "Epoch: [30][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6116(0.6116) Grad: 1124.8724  LR: 0.00387420  \n",
      "Epoch: [30][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6344) Grad: 1464.1067  LR: 0.00387420  \n",
      "Epoch: [30][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6417(0.6417) Grad: 0.0000  \n",
      "Epoch: [30][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6729(0.6562) Grad: 0.0000  \n",
      "Epoch 30 - avg_train_loss: 0.6344  avg_val_loss: 0.6562  time: 0s\n",
      "Epoch 30 - Score: 0.9042\n",
      "Epoch: [31][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6357(0.6357) Grad: 2360.4854  LR: 0.00387420  \n",
      "Epoch: [31][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6462(0.6382) Grad: 907.3765  LR: 0.00387420  \n",
      "Epoch: [31][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6394(0.6394) Grad: 0.0000  \n",
      "Epoch: [31][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6687(0.6531) Grad: 0.0000  \n",
      "Epoch 31 - avg_train_loss: 0.6382  avg_val_loss: 0.6531  time: 0s\n",
      "Epoch 31 - Score: 0.9042\n",
      "Epoch: [32][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6192(0.6192) Grad: 1249.3188  LR: 0.00387420  \n",
      "Epoch: [32][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6671(0.6346) Grad: 578.2329  LR: 0.00348678  \n",
      "Epoch: [32][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6403(0.6403) Grad: 0.0000  \n",
      "Epoch: [32][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6670(0.6527) Grad: 0.0000  \n",
      "Epoch 32 - avg_train_loss: 0.6346  avg_val_loss: 0.6527  time: 0s\n",
      "Epoch 32 - Score: 0.9000\n",
      "Epoch: [33][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6482(0.6482) Grad: 1472.7439  LR: 0.00348678  \n",
      "Epoch: [33][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6066(0.6314) Grad: 1365.0063  LR: 0.00348678  \n",
      "Epoch: [33][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6322(0.6322) Grad: 0.0000  \n",
      "Epoch: [33][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6449) Grad: 0.0000  \n",
      "Epoch 33 - avg_train_loss: 0.6314  avg_val_loss: 0.6449  time: 0s\n",
      "Epoch 33 - Score: 0.9125\n",
      "Epoch 33 - Save Best Score: 0.9125 Model\n",
      "Epoch: [34][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6383(0.6383) Grad: 856.3535  LR: 0.00348678  \n",
      "Epoch: [34][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6176(0.6306) Grad: 3695.4773  LR: 0.00348678  \n",
      "Epoch: [34][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6307(0.6307) Grad: 0.0000  \n",
      "Epoch: [34][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6595(0.6441) Grad: 0.0000  \n",
      "Epoch 34 - avg_train_loss: 0.6306  avg_val_loss: 0.6441  time: 0s\n",
      "Epoch 34 - Score: 0.9167\n",
      "Epoch 34 - Save Best Score: 0.9167 Model\n",
      "Epoch: [35][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6369(0.6369) Grad: 758.4247  LR: 0.00348678  \n",
      "Epoch: [35][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6456(0.6306) Grad: 720.6453  LR: 0.00313811  \n",
      "Epoch: [35][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6309(0.6309) Grad: 0.0000  \n",
      "Epoch: [35][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6576(0.6434) Grad: 0.0000  \n",
      "Epoch 35 - avg_train_loss: 0.6306  avg_val_loss: 0.6434  time: 0s\n",
      "Epoch 35 - Score: 0.9167\n",
      "Epoch: [36][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6531(0.6531) Grad: 676.7460  LR: 0.00313811  \n",
      "Epoch: [36][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6434(0.6276) Grad: 3337.4827  LR: 0.00313811  \n",
      "Epoch: [36][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6305(0.6305) Grad: 0.0000  \n",
      "Epoch: [36][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6574(0.6430) Grad: 0.0000  \n",
      "Epoch 36 - avg_train_loss: 0.6276  avg_val_loss: 0.6430  time: 0s\n",
      "Epoch 36 - Score: 0.9167\n",
      "Epoch: [37][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5936(0.5936) Grad: 917.6071  LR: 0.00313811  \n",
      "Epoch: [37][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6468(0.6375) Grad: 3329.9961  LR: 0.00313811  \n",
      "Epoch: [37][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6246(0.6246) Grad: 0.0000  \n",
      "Epoch: [37][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6559(0.6392) Grad: 0.0000  \n",
      "Epoch 37 - avg_train_loss: 0.6375  avg_val_loss: 0.6392  time: 0s\n",
      "Epoch 37 - Score: 0.9167\n",
      "Epoch: [38][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6343(0.6343) Grad: 1607.4686  LR: 0.00313811  \n",
      "Epoch: [38][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6283(0.6280) Grad: 1747.6207  LR: 0.00282430  \n",
      "Epoch: [38][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6145(0.6145) Grad: 0.0000  \n",
      "Epoch: [38][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6528(0.6324) Grad: 0.0000  \n",
      "Epoch 38 - avg_train_loss: 0.6280  avg_val_loss: 0.6324  time: 0s\n",
      "Epoch 38 - Score: 0.9250\n",
      "Epoch 38 - Save Best Score: 0.9250 Model\n",
      "Epoch: [39][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6215(0.6215) Grad: 1067.7535  LR: 0.00282430  \n",
      "Epoch: [39][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6265(0.6355) Grad: 2584.3423  LR: 0.00282430  \n",
      "Epoch: [39][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6242(0.6242) Grad: 0.0000  \n",
      "Epoch: [39][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6568(0.6394) Grad: 0.0000  \n",
      "Epoch 39 - avg_train_loss: 0.6355  avg_val_loss: 0.6394  time: 0s\n",
      "Epoch 39 - Score: 0.9208\n",
      "Epoch: [40][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6239(0.6239) Grad: 1053.2891  LR: 0.00282430  \n",
      "Epoch: [40][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6196(0.6322) Grad: 3761.3274  LR: 0.00282430  \n",
      "Epoch: [40][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6288(0.6288) Grad: 0.0000  \n",
      "Epoch: [40][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6565(0.6417) Grad: 0.0000  \n",
      "Epoch 40 - avg_train_loss: 0.6322  avg_val_loss: 0.6417  time: 0s\n",
      "Epoch 40 - Score: 0.9167\n",
      "Epoch: [41][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6303(0.6303) Grad: 1703.7563  LR: 0.00282430  \n",
      "Epoch: [41][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6321(0.6313) Grad: 992.0556  LR: 0.00254187  \n",
      "Epoch: [41][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6192(0.6192) Grad: 0.0000  \n",
      "Epoch: [41][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6555(0.6361) Grad: 0.0000  \n",
      "Epoch 41 - avg_train_loss: 0.6313  avg_val_loss: 0.6361  time: 0s\n",
      "Epoch 41 - Score: 0.9250\n",
      "Epoch: [42][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6000(0.6000) Grad: 4329.2900  LR: 0.00254187  \n",
      "Epoch: [42][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6393(0.6323) Grad: 852.9521  LR: 0.00254187  \n",
      "Epoch: [42][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6149(0.6149) Grad: 0.0000  \n",
      "Epoch: [42][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6554(0.6338) Grad: 0.0000  \n",
      "Epoch 42 - avg_train_loss: 0.6323  avg_val_loss: 0.6338  time: 0s\n",
      "Epoch 42 - Score: 0.9250\n",
      "Epoch: [43][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6673(0.6673) Grad: 4629.2358  LR: 0.00254187  \n",
      "Epoch: [43][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6374(0.6281) Grad: 1974.0465  LR: 0.00254187  \n",
      "Epoch: [43][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6116(0.6116) Grad: 0.0000  \n",
      "Epoch: [43][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6561(0.6324) Grad: 0.0000  \n",
      "Epoch 43 - avg_train_loss: 0.6281  avg_val_loss: 0.6324  time: 0s\n",
      "Epoch 43 - Score: 0.9292\n",
      "Epoch 43 - Save Best Score: 0.9292 Model\n",
      "Epoch: [44][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6418(0.6418) Grad: 2372.1375  LR: 0.00254187  \n",
      "Epoch: [44][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6268(0.6283) Grad: 1052.0641  LR: 0.00205891  \n",
      "Epoch: [44][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6060(0.6060) Grad: 0.0000  \n",
      "Epoch: [44][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6517(0.6273) Grad: 0.0000  \n",
      "Epoch 44 - avg_train_loss: 0.6283  avg_val_loss: 0.6273  time: 0s\n",
      "Epoch 44 - Score: 0.9333\n",
      "Epoch 44 - Save Best Score: 0.9333 Model\n",
      "Epoch: [45][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6296(0.6296) Grad: 3085.6272  LR: 0.00228768  \n",
      "Epoch: [45][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6438(0.6273) Grad: 575.0935  LR: 0.00228768  \n",
      "Epoch: [45][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6041(0.6041) Grad: 0.0000  \n",
      "Epoch: [45][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6255) Grad: 0.0000  \n",
      "Epoch 45 - avg_train_loss: 0.6273  avg_val_loss: 0.6255  time: 0s\n",
      "Epoch 45 - Score: 0.9333\n",
      "Epoch: [46][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6291) Grad: 1043.1671  LR: 0.00228768  \n",
      "Epoch: [46][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6583(0.6313) Grad: 536.9600  LR: 0.00228768  \n",
      "Epoch: [46][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6041(0.6041) Grad: 0.0000  \n",
      "Epoch: [46][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6256) Grad: 0.0000  \n",
      "Epoch 46 - avg_train_loss: 0.6313  avg_val_loss: 0.6256  time: 0s\n",
      "Epoch 46 - Score: 0.9292\n",
      "Epoch: [47][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6139(0.6139) Grad: 647.7590  LR: 0.00228768  \n",
      "Epoch: [47][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6265) Grad: 551.0012  LR: 0.00228768  \n",
      "Epoch: [47][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6019(0.6019) Grad: 0.0000  \n",
      "Epoch: [47][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6231) Grad: 0.0000  \n",
      "Epoch 47 - avg_train_loss: 0.6265  avg_val_loss: 0.6231  time: 0s\n",
      "Epoch 47 - Score: 0.9333\n",
      "Epoch: [48][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6078(0.6078) Grad: 811.0750  LR: 0.00185302  \n",
      "Epoch: [48][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6512(0.6303) Grad: 864.1089  LR: 0.00205891  \n",
      "Epoch: [48][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5993(0.5993) Grad: 0.0000  \n",
      "Epoch: [48][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6464(0.6212) Grad: 0.0000  \n",
      "Epoch 48 - avg_train_loss: 0.6303  avg_val_loss: 0.6212  time: 0s\n",
      "Epoch 48 - Score: 0.9375\n",
      "Epoch 48 - Save Best Score: 0.9375 Model\n",
      "Epoch: [49][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6671(0.6671) Grad: 2666.4180  LR: 0.00205891  \n",
      "Epoch: [49][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6155(0.6322) Grad: 907.9265  LR: 0.00205891  \n",
      "Epoch: [49][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6042(0.6042) Grad: 0.0000  \n",
      "Epoch: [49][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6469(0.6242) Grad: 0.0000  \n",
      "Epoch 49 - avg_train_loss: 0.6322  avg_val_loss: 0.6242  time: 0s\n",
      "Epoch 49 - Score: 0.9333\n",
      "Epoch: [50][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5973(0.5973) Grad: 668.0564  LR: 0.00205891  \n",
      "Epoch: [50][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6230) Grad: 473.6980  LR: 0.00205891  \n",
      "Epoch: [50][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6057(0.6057) Grad: 0.0000  \n",
      "Epoch: [50][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6467(0.6248) Grad: 0.0000  \n",
      "Epoch 50 - avg_train_loss: 0.6230  avg_val_loss: 0.6248  time: 0s\n",
      "Epoch 50 - Score: 0.9333\n",
      "Epoch: [51][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6205(0.6205) Grad: 542.5377  LR: 0.00205891  \n",
      "Epoch: [51][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6800(0.6282) Grad: 558.0762  LR: 0.00185302  \n",
      "Epoch: [51][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6092) Grad: 0.0000  \n",
      "Epoch: [51][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6280) Grad: 0.0000  \n",
      "Epoch 51 - avg_train_loss: 0.6282  avg_val_loss: 0.6280  time: 0s\n",
      "Epoch 51 - Score: 0.9333\n",
      "Epoch: [52][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6219(0.6219) Grad: 835.7578  LR: 0.00185302  \n",
      "Epoch: [52][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6035(0.6248) Grad: 634.1078  LR: 0.00185302  \n",
      "Epoch: [52][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6059(0.6059) Grad: 0.0000  \n",
      "Epoch: [52][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6468(0.6250) Grad: 0.0000  \n",
      "Epoch 52 - avg_train_loss: 0.6248  avg_val_loss: 0.6250  time: 0s\n",
      "Epoch 52 - Score: 0.9333\n",
      "Epoch: [53][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6508) Grad: 739.2490  LR: 0.00185302  \n",
      "Epoch: [53][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6248) Grad: 776.8752  LR: 0.00185302  \n",
      "Epoch: [53][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6064(0.6064) Grad: 0.0000  \n",
      "Epoch: [53][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6488(0.6262) Grad: 0.0000  \n",
      "Epoch 53 - avg_train_loss: 0.6248  avg_val_loss: 0.6262  time: 0s\n",
      "Epoch 53 - Score: 0.9333\n",
      "Epoch: [54][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6331(0.6331) Grad: 525.2203  LR: 0.00185302  \n",
      "Epoch: [54][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6630(0.6288) Grad: 1911.2417  LR: 0.00166772  \n",
      "Epoch: [54][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6071(0.6071) Grad: 0.0000  \n",
      "Epoch: [54][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6515(0.6278) Grad: 0.0000  \n",
      "Epoch 54 - avg_train_loss: 0.6288  avg_val_loss: 0.6278  time: 0s\n",
      "Epoch 54 - Score: 0.9292\n",
      "Epoch: [55][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6256(0.6256) Grad: 494.6794  LR: 0.00166772  \n",
      "Epoch: [55][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6297(0.6280) Grad: 3265.0535  LR: 0.00166772  \n",
      "Epoch: [55][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6046) Grad: 0.0000  \n",
      "Epoch: [55][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6457(0.6238) Grad: 0.0000  \n",
      "Epoch 55 - avg_train_loss: 0.6280  avg_val_loss: 0.6238  time: 0s\n",
      "Epoch 55 - Score: 0.9333\n",
      "Epoch: [56][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6638(0.6638) Grad: 422.0618  LR: 0.00166772  \n",
      "Epoch: [56][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6130(0.6243) Grad: 1545.4351  LR: 0.00166772  \n",
      "Epoch: [56][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6021(0.6021) Grad: 0.0000  \n",
      "Epoch: [56][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6457(0.6224) Grad: 0.0000  \n",
      "Epoch 56 - avg_train_loss: 0.6243  avg_val_loss: 0.6224  time: 0s\n",
      "Epoch 56 - Score: 0.9333\n",
      "Epoch: [57][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6593(0.6593) Grad: 1327.4139  LR: 0.00166772  \n",
      "Epoch: [57][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6455(0.6332) Grad: 1180.4207  LR: 0.00150095  \n",
      "Epoch: [57][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6025(0.6025) Grad: 0.0000  \n",
      "Epoch: [57][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6455(0.6225) Grad: 0.0000  \n",
      "Epoch 57 - avg_train_loss: 0.6332  avg_val_loss: 0.6225  time: 0s\n",
      "Epoch 57 - Score: 0.9375\n",
      "Epoch: [58][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6142(0.6142) Grad: 2237.2214  LR: 0.00150095  \n",
      "Epoch: [58][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6442(0.6279) Grad: 1206.0149  LR: 0.00150095  \n",
      "Epoch: [58][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6080(0.6080) Grad: 0.0000  \n",
      "Epoch: [58][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6466(0.6260) Grad: 0.0000  \n",
      "Epoch 58 - avg_train_loss: 0.6279  avg_val_loss: 0.6260  time: 0s\n",
      "Epoch 58 - Score: 0.9333\n",
      "Epoch: [59][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 1378.8047  LR: 0.00150095  \n",
      "Epoch: [59][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6332(0.6271) Grad: 2138.4358  LR: 0.00150095  \n",
      "Epoch: [59][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6123(0.6123) Grad: 0.0000  \n",
      "Epoch: [59][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6481(0.6290) Grad: 0.0000  \n",
      "Epoch 59 - avg_train_loss: 0.6271  avg_val_loss: 0.6290  time: 0s\n",
      "Epoch 59 - Score: 0.9292\n",
      "Epoch: [60][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6371(0.6371) Grad: 3434.7292  LR: 0.00150095  \n",
      "Epoch: [60][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6112(0.6261) Grad: 713.1857  LR: 0.00135085  \n",
      "Epoch: [60][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6093(0.6093) Grad: 0.0000  \n",
      "Epoch: [60][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6462(0.6265) Grad: 0.0000  \n",
      "Epoch 60 - avg_train_loss: 0.6261  avg_val_loss: 0.6265  time: 0s\n",
      "Epoch 60 - Score: 0.9292\n",
      "Epoch: [61][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6614(0.6614) Grad: 3332.4182  LR: 0.00135085  \n",
      "Epoch: [61][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6054(0.6270) Grad: 2110.1794  LR: 0.00135085  \n",
      "Epoch: [61][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6130(0.6130) Grad: 0.0000  \n",
      "Epoch: [61][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6458(0.6283) Grad: 0.0000  \n",
      "Epoch 61 - avg_train_loss: 0.6270  avg_val_loss: 0.6283  time: 0s\n",
      "Epoch 61 - Score: 0.9250\n",
      "Epoch: [62][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5819(0.5819) Grad: 1681.6949  LR: 0.00135085  \n",
      "Epoch: [62][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6126(0.6227) Grad: 1271.9626  LR: 0.00135085  \n",
      "Epoch: [62][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6109(0.6109) Grad: 0.0000  \n",
      "Epoch: [62][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6463(0.6274) Grad: 0.0000  \n",
      "Epoch 62 - avg_train_loss: 0.6227  avg_val_loss: 0.6274  time: 0s\n",
      "Epoch 62 - Score: 0.9292\n",
      "Epoch: [63][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6027(0.6027) Grad: 414.2325  LR: 0.00135085  \n",
      "Epoch: [63][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6175(0.6262) Grad: 1929.9635  LR: 0.00121577  \n",
      "Epoch: [63][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6022(0.6022) Grad: 0.0000  \n",
      "Epoch: [63][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6220) Grad: 0.0000  \n",
      "Epoch 63 - avg_train_loss: 0.6262  avg_val_loss: 0.6220  time: 0s\n",
      "Epoch 63 - Score: 0.9333\n",
      "Epoch: [64][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6359(0.6359) Grad: 567.5409  LR: 0.00121577  \n",
      "Epoch: [64][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5977(0.6225) Grad: 2347.5408  LR: 0.00121577  \n",
      "Epoch: [64][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6003(0.6003) Grad: 0.0000  \n",
      "Epoch: [64][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6442(0.6208) Grad: 0.0000  \n",
      "Epoch 64 - avg_train_loss: 0.6225  avg_val_loss: 0.6208  time: 0s\n",
      "Epoch 64 - Score: 0.9333\n",
      "Epoch: [65][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6133(0.6133) Grad: 2919.4922  LR: 0.00121577  \n",
      "Epoch: [65][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6564(0.6229) Grad: 1446.6377  LR: 0.00121577  \n",
      "Epoch: [65][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6037(0.6037) Grad: 0.0000  \n",
      "Epoch: [65][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6441(0.6226) Grad: 0.0000  \n",
      "Epoch 65 - avg_train_loss: 0.6229  avg_val_loss: 0.6226  time: 0s\n",
      "Epoch 65 - Score: 0.9292\n",
      "Epoch: [66][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6359(0.6359) Grad: 548.7391  LR: 0.00121577  \n",
      "Epoch: [66][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6406(0.6272) Grad: 1884.6522  LR: 0.00098477  \n",
      "Epoch: [66][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6040(0.6040) Grad: 0.0000  \n",
      "Epoch: [66][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6230) Grad: 0.0000  \n",
      "Epoch 66 - avg_train_loss: 0.6272  avg_val_loss: 0.6230  time: 0s\n",
      "Epoch 66 - Score: 0.9333\n",
      "Epoch: [67][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6455(0.6455) Grad: 2572.4763  LR: 0.00109419  \n",
      "Epoch: [67][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6482(0.6241) Grad: 2693.4690  LR: 0.00109419  \n",
      "Epoch: [67][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6095) Grad: 0.0000  \n",
      "Epoch: [67][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6443(0.6257) Grad: 0.0000  \n",
      "Epoch 67 - avg_train_loss: 0.6241  avg_val_loss: 0.6257  time: 0s\n",
      "Epoch 67 - Score: 0.9292\n",
      "Epoch: [68][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6246(0.6246) Grad: 2775.0188  LR: 0.00109419  \n",
      "Epoch: [68][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6418(0.6251) Grad: 1261.2787  LR: 0.00109419  \n",
      "Epoch: [68][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6103(0.6103) Grad: 0.0000  \n",
      "Epoch: [68][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6265) Grad: 0.0000  \n",
      "Epoch 68 - avg_train_loss: 0.6251  avg_val_loss: 0.6265  time: 0s\n",
      "Epoch 68 - Score: 0.9250\n",
      "Epoch: [69][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6283(0.6283) Grad: 1370.5331  LR: 0.00109419  \n",
      "Epoch: [69][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6256(0.6257) Grad: 4273.4487  LR: 0.00109419  \n",
      "Epoch: [69][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6088(0.6088) Grad: 0.0000  \n",
      "Epoch: [69][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6257) Grad: 0.0000  \n",
      "Epoch 69 - avg_train_loss: 0.6257  avg_val_loss: 0.6257  time: 0s\n",
      "Epoch 69 - Score: 0.9292\n",
      "Epoch: [70][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5953(0.5953) Grad: 1919.4520  LR: 0.00088629  \n",
      "Epoch: [70][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6623(0.6217) Grad: 1726.2166  LR: 0.00098477  \n",
      "Epoch: [70][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6072(0.6072) Grad: 0.0000  \n",
      "Epoch: [70][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6452(0.6250) Grad: 0.0000  \n",
      "Epoch 70 - avg_train_loss: 0.6217  avg_val_loss: 0.6250  time: 0s\n",
      "Epoch 70 - Score: 0.9292\n",
      "Epoch: [71][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6400(0.6400) Grad: 1053.5356  LR: 0.00098477  \n",
      "Epoch: [71][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6539(0.6282) Grad: 2480.6099  LR: 0.00098477  \n",
      "Epoch: [71][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6063(0.6063) Grad: 0.0000  \n",
      "Epoch: [71][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6454(0.6245) Grad: 0.0000  \n",
      "Epoch 71 - avg_train_loss: 0.6282  avg_val_loss: 0.6245  time: 0s\n",
      "Epoch 71 - Score: 0.9292\n",
      "Epoch: [72][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6340(0.6340) Grad: 606.5780  LR: 0.00098477  \n",
      "Epoch: [72][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6347(0.6229) Grad: 483.2868  LR: 0.00098477  \n",
      "Epoch: [72][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6046(0.6046) Grad: 0.0000  \n",
      "Epoch: [72][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6233) Grad: 0.0000  \n",
      "Epoch 72 - avg_train_loss: 0.6229  avg_val_loss: 0.6233  time: 0s\n",
      "Epoch 72 - Score: 0.9333\n",
      "Epoch: [73][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6459(0.6459) Grad: 1339.6445  LR: 0.00098477  \n",
      "Epoch: [73][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6351(0.6231) Grad: 770.6600  LR: 0.00088629  \n",
      "Epoch: [73][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6061(0.6061) Grad: 0.0000  \n",
      "Epoch: [73][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6453(0.6244) Grad: 0.0000  \n",
      "Epoch 73 - avg_train_loss: 0.6231  avg_val_loss: 0.6244  time: 0s\n",
      "Epoch 73 - Score: 0.9292\n",
      "Epoch: [74][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6095(0.6095) Grad: 574.8644  LR: 0.00088629  \n",
      "Epoch: [74][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6049(0.6256) Grad: 2165.0229  LR: 0.00088629  \n",
      "Epoch: [74][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6068(0.6068) Grad: 0.0000  \n",
      "Epoch: [74][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6247) Grad: 0.0000  \n",
      "Epoch 74 - avg_train_loss: 0.6256  avg_val_loss: 0.6247  time: 0s\n",
      "Epoch 74 - Score: 0.9292\n",
      "Epoch: [75][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6036(0.6036) Grad: 459.2555  LR: 0.00088629  \n",
      "Epoch: [75][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6266(0.6232) Grad: 418.6907  LR: 0.00088629  \n",
      "Epoch: [75][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6100(0.6100) Grad: 0.0000  \n",
      "Epoch: [75][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6264) Grad: 0.0000  \n",
      "Epoch 75 - avg_train_loss: 0.6232  avg_val_loss: 0.6264  time: 0s\n",
      "Epoch 75 - Score: 0.9292\n",
      "Epoch: [76][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6350(0.6350) Grad: 454.2484  LR: 0.00088629  \n",
      "Epoch: [76][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5950(0.6262) Grad: 1683.0294  LR: 0.00079766  \n",
      "Epoch: [76][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6130(0.6130) Grad: 0.0000  \n",
      "Epoch: [76][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6278) Grad: 0.0000  \n",
      "Epoch 76 - avg_train_loss: 0.6262  avg_val_loss: 0.6278  time: 0s\n",
      "Epoch 76 - Score: 0.9250\n",
      "Epoch: [77][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5941(0.5941) Grad: 1058.5104  LR: 0.00079766  \n",
      "Epoch: [77][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6298(0.6208) Grad: 3379.2634  LR: 0.00079766  \n",
      "Epoch: [77][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6127(0.6127) Grad: 0.0000  \n",
      "Epoch: [77][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6441(0.6274) Grad: 0.0000  \n",
      "Epoch 77 - avg_train_loss: 0.6208  avg_val_loss: 0.6274  time: 0s\n",
      "Epoch 77 - Score: 0.9250\n",
      "Epoch: [78][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6354(0.6354) Grad: 2440.3289  LR: 0.00079766  \n",
      "Epoch: [78][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6225) Grad: 771.5693  LR: 0.00079766  \n",
      "Epoch: [78][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6127(0.6127) Grad: 0.0000  \n",
      "Epoch: [78][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6444(0.6275) Grad: 0.0000  \n",
      "Epoch 78 - avg_train_loss: 0.6225  avg_val_loss: 0.6275  time: 0s\n",
      "Epoch 78 - Score: 0.9250\n",
      "Epoch: [79][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6402(0.6402) Grad: 4412.0571  LR: 0.00079766  \n",
      "Epoch: [79][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5883(0.6241) Grad: 478.6869  LR: 0.00071790  \n",
      "Epoch: [79][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6129(0.6129) Grad: 0.0000  \n",
      "Epoch: [79][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6446(0.6277) Grad: 0.0000  \n",
      "Epoch 79 - avg_train_loss: 0.6241  avg_val_loss: 0.6277  time: 0s\n",
      "Epoch 79 - Score: 0.9250\n",
      "Epoch: [80][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6250(0.6250) Grad: 734.9008  LR: 0.00071790  \n",
      "Epoch: [80][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6193) Grad: 863.3472  LR: 0.00071790  \n",
      "Epoch: [80][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6142(0.6142) Grad: 0.0000  \n",
      "Epoch: [80][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6444(0.6283) Grad: 0.0000  \n",
      "Epoch 80 - avg_train_loss: 0.6193  avg_val_loss: 0.6283  time: 0s\n",
      "Epoch 80 - Score: 0.9250\n",
      "Epoch: [81][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6032(0.6032) Grad: 717.4485  LR: 0.00071790  \n",
      "Epoch: [81][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6488(0.6240) Grad: 4275.1440  LR: 0.00071790  \n",
      "Epoch: [81][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6149(0.6149) Grad: 0.0000  \n",
      "Epoch: [81][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6445(0.6287) Grad: 0.0000  \n",
      "Epoch 81 - avg_train_loss: 0.6240  avg_val_loss: 0.6287  time: 0s\n",
      "Epoch 81 - Score: 0.9250\n",
      "Epoch: [82][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6258(0.6258) Grad: 393.0123  LR: 0.00071790  \n",
      "Epoch: [82][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6217) Grad: 1373.5751  LR: 0.00064611  \n",
      "Epoch: [82][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6151) Grad: 0.0000  \n",
      "Epoch: [82][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6449(0.6290) Grad: 0.0000  \n",
      "Epoch 82 - avg_train_loss: 0.6217  avg_val_loss: 0.6290  time: 0s\n",
      "Epoch 82 - Score: 0.9250\n",
      "Epoch: [83][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5990(0.5990) Grad: 2226.2119  LR: 0.00064611  \n",
      "Epoch: [83][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6454(0.6269) Grad: 2503.4475  LR: 0.00064611  \n",
      "Epoch: [83][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6138(0.6138) Grad: 0.0000  \n",
      "Epoch: [83][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6450(0.6284) Grad: 0.0000  \n",
      "Epoch 83 - avg_train_loss: 0.6269  avg_val_loss: 0.6284  time: 0s\n",
      "Epoch 83 - Score: 0.9292\n",
      "Epoch: [84][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6392(0.6392) Grad: 1854.5847  LR: 0.00064611  \n",
      "Epoch: [84][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6319(0.6218) Grad: 1192.1918  LR: 0.00064611  \n",
      "Epoch: [84][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6140(0.6140) Grad: 0.0000  \n",
      "Epoch: [84][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6284) Grad: 0.0000  \n",
      "Epoch 84 - avg_train_loss: 0.6218  avg_val_loss: 0.6284  time: 0s\n",
      "Epoch 84 - Score: 0.9292\n",
      "Epoch: [85][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6320(0.6320) Grad: 367.8203  LR: 0.00064611  \n",
      "Epoch: [85][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6175(0.6229) Grad: 826.2689  LR: 0.00058150  \n",
      "Epoch: [85][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6155(0.6155) Grad: 0.0000  \n",
      "Epoch: [85][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6447(0.6291) Grad: 0.0000  \n",
      "Epoch 85 - avg_train_loss: 0.6229  avg_val_loss: 0.6291  time: 0s\n",
      "Epoch 85 - Score: 0.9250\n",
      "Epoch: [86][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6232(0.6232) Grad: 804.3485  LR: 0.00058150  \n",
      "Epoch: [86][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6283(0.6218) Grad: 2388.5811  LR: 0.00058150  \n",
      "Epoch: [86][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6170(0.6170) Grad: 0.0000  \n",
      "Epoch: [86][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6458(0.6305) Grad: 0.0000  \n",
      "Epoch 86 - avg_train_loss: 0.6218  avg_val_loss: 0.6305  time: 0s\n",
      "Epoch 86 - Score: 0.9250\n",
      "Epoch: [87][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6258(0.6258) Grad: 2732.0752  LR: 0.00058150  \n",
      "Epoch: [87][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6274(0.6261) Grad: 1022.6691  LR: 0.00058150  \n",
      "Epoch: [87][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6160(0.6160) Grad: 0.0000  \n",
      "Epoch: [87][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6460(0.6300) Grad: 0.0000  \n",
      "Epoch 87 - avg_train_loss: 0.6261  avg_val_loss: 0.6300  time: 0s\n",
      "Epoch 87 - Score: 0.9250\n",
      "Epoch: [88][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6116(0.6116) Grad: 2626.9648  LR: 0.00058150  \n",
      "Epoch: [88][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6337(0.6304) Grad: 712.0699  LR: 0.00047101  \n",
      "Epoch: [88][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6138(0.6138) Grad: 0.0000  \n",
      "Epoch: [88][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6465(0.6290) Grad: 0.0000  \n",
      "Epoch 88 - avg_train_loss: 0.6304  avg_val_loss: 0.6290  time: 0s\n",
      "Epoch 88 - Score: 0.9292\n",
      "Epoch: [89][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6045(0.6045) Grad: 685.3661  LR: 0.00052335  \n",
      "Epoch: [89][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6238(0.6240) Grad: 778.4621  LR: 0.00052335  \n",
      "Epoch: [89][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6132(0.6132) Grad: 0.0000  \n",
      "Epoch: [89][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6472(0.6291) Grad: 0.0000  \n",
      "Epoch 89 - avg_train_loss: 0.6240  avg_val_loss: 0.6291  time: 0s\n",
      "Epoch 89 - Score: 0.9250\n",
      "Epoch: [90][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5746(0.5746) Grad: 2354.8821  LR: 0.00052335  \n",
      "Epoch: [90][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6391(0.6207) Grad: 4640.1680  LR: 0.00052335  \n",
      "Epoch: [90][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6131(0.6131) Grad: 0.0000  \n",
      "Epoch: [90][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6476(0.6292) Grad: 0.0000  \n",
      "Epoch 90 - avg_train_loss: 0.6207  avg_val_loss: 0.6292  time: 0s\n",
      "Epoch 90 - Score: 0.9292\n",
      "Epoch: [91][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6163(0.6163) Grad: 2196.6047  LR: 0.00052335  \n",
      "Epoch: [91][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6253) Grad: 5330.9053  LR: 0.00052335  \n",
      "Epoch: [91][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6136(0.6136) Grad: 0.0000  \n",
      "Epoch: [91][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6479(0.6296) Grad: 0.0000  \n",
      "Epoch 91 - avg_train_loss: 0.6253  avg_val_loss: 0.6296  time: 0s\n",
      "Epoch 91 - Score: 0.9250\n",
      "Epoch: [92][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6273(0.6273) Grad: 1084.7168  LR: 0.00042391  \n",
      "Epoch: [92][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6050(0.6224) Grad: 1780.5829  LR: 0.00047101  \n",
      "Epoch: [92][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6151(0.6151) Grad: 0.0000  \n",
      "Epoch: [92][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6473(0.6301) Grad: 0.0000  \n",
      "Epoch 92 - avg_train_loss: 0.6224  avg_val_loss: 0.6301  time: 0s\n",
      "Epoch 92 - Score: 0.9292\n",
      "Epoch: [93][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6082(0.6082) Grad: 2772.7925  LR: 0.00047101  \n",
      "Epoch: [93][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5957(0.6219) Grad: 540.3358  LR: 0.00047101  \n",
      "Epoch: [93][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6154(0.6154) Grad: 0.0000  \n",
      "Epoch: [93][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6319) Grad: 0.0000  \n",
      "Epoch 93 - avg_train_loss: 0.6219  avg_val_loss: 0.6319  time: 0s\n",
      "Epoch 93 - Score: 0.9208\n",
      "Epoch: [94][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6152(0.6152) Grad: 2577.2048  LR: 0.00047101  \n",
      "Epoch: [94][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6420(0.6236) Grad: 627.9041  LR: 0.00047101  \n",
      "Epoch: [94][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6161(0.6161) Grad: 0.0000  \n",
      "Epoch: [94][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6311) Grad: 0.0000  \n",
      "Epoch 94 - avg_train_loss: 0.6236  avg_val_loss: 0.6311  time: 0s\n",
      "Epoch 94 - Score: 0.9208\n",
      "Epoch: [95][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6377(0.6377) Grad: 2023.8208  LR: 0.00047101  \n",
      "Epoch: [95][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6410(0.6220) Grad: 958.3342  LR: 0.00042391  \n",
      "Epoch: [95][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 0.0000  \n",
      "Epoch: [95][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6476(0.6317) Grad: 0.0000  \n",
      "Epoch 95 - avg_train_loss: 0.6220  avg_val_loss: 0.6317  time: 0s\n",
      "Epoch 95 - Score: 0.9208\n",
      "Epoch: [96][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6078(0.6078) Grad: 1470.5955  LR: 0.00042391  \n",
      "Epoch: [96][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6441(0.6180) Grad: 3904.9692  LR: 0.00042391  \n",
      "Epoch: [96][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6181) Grad: 0.0000  \n",
      "Epoch: [96][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6485(0.6323) Grad: 0.0000  \n",
      "Epoch 96 - avg_train_loss: 0.6180  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 96 - Score: 0.9167\n",
      "Epoch: [97][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 536.2937  LR: 0.00042391  \n",
      "Epoch: [97][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6255(0.6205) Grad: 1826.0763  LR: 0.00042391  \n",
      "Epoch: [97][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6166(0.6166) Grad: 0.0000  \n",
      "Epoch: [97][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6322) Grad: 0.0000  \n",
      "Epoch 97 - avg_train_loss: 0.6205  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 97 - Score: 0.9208\n",
      "Epoch: [98][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6030(0.6030) Grad: 1162.8950  LR: 0.00042391  \n",
      "Epoch: [98][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6258(0.6230) Grad: 673.8832  LR: 0.00038152  \n",
      "Epoch: [98][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6164(0.6164) Grad: 0.0000  \n",
      "Epoch: [98][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6488(0.6315) Grad: 0.0000  \n",
      "Epoch 98 - avg_train_loss: 0.6230  avg_val_loss: 0.6315  time: 0s\n",
      "Epoch 98 - Score: 0.9208\n",
      "Epoch: [99][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6408(0.6408) Grad: 1281.7084  LR: 0.00038152  \n",
      "Epoch: [99][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6281(0.6213) Grad: 877.0251  LR: 0.00038152  \n",
      "Epoch: [99][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6165(0.6165) Grad: 0.0000  \n",
      "Epoch: [99][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6312) Grad: 0.0000  \n",
      "Epoch 99 - avg_train_loss: 0.6213  avg_val_loss: 0.6312  time: 0s\n",
      "Epoch 99 - Score: 0.9208\n",
      "Epoch: [100][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6473(0.6473) Grad: 1923.3696  LR: 0.00038152  \n",
      "Epoch: [100][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5887(0.6227) Grad: 2706.3452  LR: 0.00038152  \n",
      "Epoch: [100][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6166(0.6166) Grad: 0.0000  \n",
      "Epoch: [100][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6485(0.6315) Grad: 0.0000  \n",
      "Epoch 100 - avg_train_loss: 0.6227  avg_val_loss: 0.6315  time: 0s\n",
      "Epoch 100 - Score: 0.9208\n",
      "Epoch: [101][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5985(0.5985) Grad: 3031.0210  LR: 0.00038152  \n",
      "Epoch: [101][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6734(0.6231) Grad: 5768.5342  LR: 0.00034337  \n",
      "Epoch: [101][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6174(0.6174) Grad: 0.0000  \n",
      "Epoch: [101][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6464(0.6309) Grad: 0.0000  \n",
      "Epoch 101 - avg_train_loss: 0.6231  avg_val_loss: 0.6309  time: 0s\n",
      "Epoch 101 - Score: 0.9250\n",
      "Epoch: [102][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 497.2135  LR: 0.00034337  \n",
      "Epoch: [102][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6114(0.6196) Grad: 2381.0803  LR: 0.00034337  \n",
      "Epoch: [102][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 0.0000  \n",
      "Epoch: [102][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6451(0.6309) Grad: 0.0000  \n",
      "Epoch 102 - avg_train_loss: 0.6196  avg_val_loss: 0.6309  time: 0s\n",
      "Epoch 102 - Score: 0.9208\n",
      "Epoch: [103][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6326(0.6326) Grad: 5211.1074  LR: 0.00034337  \n",
      "Epoch: [103][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6180(0.6220) Grad: 1878.3835  LR: 0.00034337  \n",
      "Epoch: [103][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6176(0.6176) Grad: 0.0000  \n",
      "Epoch: [103][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6455(0.6306) Grad: 0.0000  \n",
      "Epoch 103 - avg_train_loss: 0.6220  avg_val_loss: 0.6306  time: 0s\n",
      "Epoch 103 - Score: 0.9250\n",
      "Epoch: [104][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6172(0.6172) Grad: 911.3747  LR: 0.00034337  \n",
      "Epoch: [104][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6443(0.6239) Grad: 2778.8013  LR: 0.00030903  \n",
      "Epoch: [104][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6178) Grad: 0.0000  \n",
      "Epoch: [104][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6452(0.6306) Grad: 0.0000  \n",
      "Epoch 104 - avg_train_loss: 0.6239  avg_val_loss: 0.6306  time: 0s\n",
      "Epoch 104 - Score: 0.9250\n",
      "Epoch: [105][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6163(0.6163) Grad: 952.6371  LR: 0.00030903  \n",
      "Epoch: [105][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6375(0.6203) Grad: 3375.3030  LR: 0.00030903  \n",
      "Epoch: [105][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 0.0000  \n",
      "Epoch: [105][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6453(0.6311) Grad: 0.0000  \n",
      "Epoch 105 - avg_train_loss: 0.6203  avg_val_loss: 0.6311  time: 0s\n",
      "Epoch 105 - Score: 0.9250\n",
      "Epoch: [106][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5954(0.5954) Grad: 462.2400  LR: 0.00030903  \n",
      "Epoch: [106][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6133(0.6154) Grad: 3486.3408  LR: 0.00030903  \n",
      "Epoch: [106][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 0.0000  \n",
      "Epoch: [106][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6456(0.6312) Grad: 0.0000  \n",
      "Epoch 106 - avg_train_loss: 0.6154  avg_val_loss: 0.6312  time: 0s\n",
      "Epoch 106 - Score: 0.9250\n",
      "Epoch: [107][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6246(0.6246) Grad: 1612.4253  LR: 0.00030903  \n",
      "Epoch: [107][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6168(0.6229) Grad: 837.1737  LR: 0.00027813  \n",
      "Epoch: [107][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6187) Grad: 0.0000  \n",
      "Epoch: [107][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6469(0.6319) Grad: 0.0000  \n",
      "Epoch 107 - avg_train_loss: 0.6229  avg_val_loss: 0.6319  time: 0s\n",
      "Epoch 107 - Score: 0.9250\n",
      "Epoch: [108][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6426(0.6426) Grad: 2353.2014  LR: 0.00027813  \n",
      "Epoch: [108][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6386(0.6170) Grad: 945.9094  LR: 0.00027813  \n",
      "Epoch: [108][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6187) Grad: 0.0000  \n",
      "Epoch: [108][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6479(0.6323) Grad: 0.0000  \n",
      "Epoch 108 - avg_train_loss: 0.6170  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 108 - Score: 0.9208\n",
      "Epoch: [109][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6243(0.6243) Grad: 1256.7106  LR: 0.00027813  \n",
      "Epoch: [109][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6383(0.6214) Grad: 5178.1997  LR: 0.00027813  \n",
      "Epoch: [109][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6200) Grad: 0.0000  \n",
      "Epoch: [109][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6462(0.6322) Grad: 0.0000  \n",
      "Epoch 109 - avg_train_loss: 0.6214  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 109 - Score: 0.9250\n",
      "Epoch: [110][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6521(0.6521) Grad: 4686.6196  LR: 0.00027813  \n",
      "Epoch: [110][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6126(0.6224) Grad: 1559.4612  LR: 0.00022528  \n",
      "Epoch: [110][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 0.0000  \n",
      "Epoch: [110][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6481(0.6325) Grad: 0.0000  \n",
      "Epoch 110 - avg_train_loss: 0.6224  avg_val_loss: 0.6325  time: 0s\n",
      "Epoch 110 - Score: 0.9208\n",
      "Epoch: [111][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5964(0.5964) Grad: 2288.2263  LR: 0.00025032  \n",
      "Epoch: [111][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6029(0.6210) Grad: 3716.0925  LR: 0.00025032  \n",
      "Epoch: [111][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6196(0.6196) Grad: 0.0000  \n",
      "Epoch: [111][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6326) Grad: 0.0000  \n",
      "Epoch 111 - avg_train_loss: 0.6210  avg_val_loss: 0.6326  time: 0s\n",
      "Epoch 111 - Score: 0.9208\n",
      "Epoch: [112][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5982(0.5982) Grad: 3241.8960  LR: 0.00025032  \n",
      "Epoch: [112][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6669(0.6225) Grad: 2375.5571  LR: 0.00025032  \n",
      "Epoch: [112][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6220(0.6220) Grad: 0.0000  \n",
      "Epoch: [112][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6463(0.6333) Grad: 0.0000  \n",
      "Epoch 112 - avg_train_loss: 0.6225  avg_val_loss: 0.6333  time: 0s\n",
      "Epoch 112 - Score: 0.9208\n",
      "Epoch: [113][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6134(0.6134) Grad: 6298.9746  LR: 0.00025032  \n",
      "Epoch: [113][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6359(0.6203) Grad: 2398.6880  LR: 0.00025032  \n",
      "Epoch: [113][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6210(0.6210) Grad: 0.0000  \n",
      "Epoch: [113][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6466(0.6330) Grad: 0.0000  \n",
      "Epoch 113 - avg_train_loss: 0.6203  avg_val_loss: 0.6330  time: 0s\n",
      "Epoch 113 - Score: 0.9208\n",
      "Epoch: [114][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6056(0.6056) Grad: 2071.0857  LR: 0.00020276  \n",
      "Epoch: [114][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6226(0.6211) Grad: 3456.8860  LR: 0.00022528  \n",
      "Epoch: [114][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6212(0.6212) Grad: 0.0000  \n",
      "Epoch: [114][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6463(0.6329) Grad: 0.0000  \n",
      "Epoch 114 - avg_train_loss: 0.6211  avg_val_loss: 0.6329  time: 0s\n",
      "Epoch 114 - Score: 0.9208\n",
      "Epoch: [115][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6047) Grad: 4404.4678  LR: 0.00022528  \n",
      "Epoch: [115][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6159(0.6242) Grad: 2444.5232  LR: 0.00022528  \n",
      "Epoch: [115][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6207(0.6207) Grad: 0.0000  \n",
      "Epoch: [115][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6469(0.6329) Grad: 0.0000  \n",
      "Epoch 115 - avg_train_loss: 0.6242  avg_val_loss: 0.6329  time: 0s\n",
      "Epoch 115 - Score: 0.9167\n",
      "Epoch: [116][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6092) Grad: 653.2852  LR: 0.00022528  \n",
      "Epoch: [116][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6215) Grad: 2030.1775  LR: 0.00022528  \n",
      "Epoch: [116][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6200) Grad: 0.0000  \n",
      "Epoch: [116][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6471(0.6326) Grad: 0.0000  \n",
      "Epoch 116 - avg_train_loss: 0.6215  avg_val_loss: 0.6326  time: 0s\n",
      "Epoch 116 - Score: 0.9167\n",
      "Epoch: [117][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6211(0.6211) Grad: 5143.9717  LR: 0.00022528  \n",
      "Epoch: [117][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6208) Grad: 1856.6863  LR: 0.00020276  \n",
      "Epoch: [117][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 0.0000  \n",
      "Epoch: [117][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6470(0.6318) Grad: 0.0000  \n",
      "Epoch 117 - avg_train_loss: 0.6208  avg_val_loss: 0.6318  time: 0s\n",
      "Epoch 117 - Score: 0.9208\n",
      "Epoch: [118][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5834(0.5834) Grad: 1920.2679  LR: 0.00020276  \n",
      "Epoch: [118][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6132(0.6244) Grad: 3332.0127  LR: 0.00020276  \n",
      "Epoch: [118][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6177(0.6177) Grad: 0.0000  \n",
      "Epoch: [118][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6476(0.6317) Grad: 0.0000  \n",
      "Epoch 118 - avg_train_loss: 0.6244  avg_val_loss: 0.6317  time: 0s\n",
      "Epoch 118 - Score: 0.9208\n",
      "Epoch: [119][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6360(0.6360) Grad: 2415.2566  LR: 0.00020276  \n",
      "Epoch: [119][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6416(0.6223) Grad: 2349.7290  LR: 0.00020276  \n",
      "Epoch: [119][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6178) Grad: 0.0000  \n",
      "Epoch: [119][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6479(0.6318) Grad: 0.0000  \n",
      "Epoch 119 - avg_train_loss: 0.6223  avg_val_loss: 0.6318  time: 0s\n",
      "Epoch 119 - Score: 0.9208\n",
      "Epoch: [120][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6106(0.6106) Grad: 875.9657  LR: 0.00020276  \n",
      "Epoch: [120][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6175(0.6223) Grad: 3274.9309  LR: 0.00018248  \n",
      "Epoch: [120][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6181) Grad: 0.0000  \n",
      "Epoch: [120][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6481(0.6321) Grad: 0.0000  \n",
      "Epoch 120 - avg_train_loss: 0.6223  avg_val_loss: 0.6321  time: 0s\n",
      "Epoch 120 - Score: 0.9208\n",
      "Epoch: [121][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6562(0.6562) Grad: 2204.9126  LR: 0.00018248  \n",
      "Epoch: [121][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6478(0.6181) Grad: 1198.6057  LR: 0.00018248  \n",
      "Epoch: [121][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6189(0.6189) Grad: 0.0000  \n",
      "Epoch: [121][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6326) Grad: 0.0000  \n",
      "Epoch 121 - avg_train_loss: 0.6181  avg_val_loss: 0.6326  time: 0s\n",
      "Epoch 121 - Score: 0.9208\n",
      "Epoch: [122][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6372(0.6372) Grad: 2082.0662  LR: 0.00018248  \n",
      "Epoch: [122][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6327(0.6235) Grad: 4255.3730  LR: 0.00018248  \n",
      "Epoch: [122][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6200) Grad: 0.0000  \n",
      "Epoch: [122][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6332) Grad: 0.0000  \n",
      "Epoch 122 - avg_train_loss: 0.6235  avg_val_loss: 0.6332  time: 0s\n",
      "Epoch 122 - Score: 0.9167\n",
      "Epoch: [123][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6164(0.6164) Grad: 3614.6250  LR: 0.00018248  \n",
      "Epoch: [123][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6470(0.6255) Grad: 1533.9318  LR: 0.00016423  \n",
      "Epoch: [123][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6202(0.6202) Grad: 0.0000  \n",
      "Epoch: [123][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6334) Grad: 0.0000  \n",
      "Epoch 123 - avg_train_loss: 0.6255  avg_val_loss: 0.6334  time: 0s\n",
      "Epoch 123 - Score: 0.9167\n",
      "Epoch: [124][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6097(0.6097) Grad: 3874.3242  LR: 0.00016423  \n",
      "Epoch: [124][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6048(0.6235) Grad: 1625.5179  LR: 0.00016423  \n",
      "Epoch: [124][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6197(0.6197) Grad: 0.0000  \n",
      "Epoch: [124][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6335) Grad: 0.0000  \n",
      "Epoch 124 - avg_train_loss: 0.6235  avg_val_loss: 0.6335  time: 0s\n",
      "Epoch 124 - Score: 0.9208\n",
      "Epoch: [125][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6526(0.6526) Grad: 2943.9048  LR: 0.00016423  \n",
      "Epoch: [125][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6214) Grad: 534.7487  LR: 0.00016423  \n",
      "Epoch: [125][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6189(0.6189) Grad: 0.0000  \n",
      "Epoch: [125][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6336) Grad: 0.0000  \n",
      "Epoch 125 - avg_train_loss: 0.6214  avg_val_loss: 0.6336  time: 0s\n",
      "Epoch 125 - Score: 0.9208\n",
      "Epoch: [126][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6228(0.6228) Grad: 2891.8586  LR: 0.00016423  \n",
      "Epoch: [126][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6195) Grad: 5565.1250  LR: 0.00014781  \n",
      "Epoch: [126][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [126][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6335) Grad: 0.0000  \n",
      "Epoch 126 - avg_train_loss: 0.6195  avg_val_loss: 0.6335  time: 0s\n",
      "Epoch 126 - Score: 0.9208\n",
      "Epoch: [127][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6196(0.6196) Grad: 2153.4805  LR: 0.00014781  \n",
      "Epoch: [127][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6270(0.6210) Grad: 2924.5867  LR: 0.00014781  \n",
      "Epoch: [127][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 0.0000  \n",
      "Epoch: [127][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6335) Grad: 0.0000  \n",
      "Epoch 127 - avg_train_loss: 0.6210  avg_val_loss: 0.6335  time: 0s\n",
      "Epoch 127 - Score: 0.9208\n",
      "Epoch: [128][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5936(0.5936) Grad: 2202.8093  LR: 0.00014781  \n",
      "Epoch: [128][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6042(0.6204) Grad: 1174.8438  LR: 0.00014781  \n",
      "Epoch: [128][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6197(0.6197) Grad: 0.0000  \n",
      "Epoch: [128][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6339) Grad: 0.0000  \n",
      "Epoch 128 - avg_train_loss: 0.6204  avg_val_loss: 0.6339  time: 0s\n",
      "Epoch 128 - Score: 0.9208\n",
      "Epoch: [129][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6571(0.6571) Grad: 2931.1963  LR: 0.00014781  \n",
      "Epoch: [129][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6393(0.6214) Grad: 1890.7910  LR: 0.00013303  \n",
      "Epoch: [129][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6195(0.6195) Grad: 0.0000  \n",
      "Epoch: [129][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6507(0.6340) Grad: 0.0000  \n",
      "Epoch 129 - avg_train_loss: 0.6214  avg_val_loss: 0.6340  time: 0s\n",
      "Epoch 129 - Score: 0.9208\n",
      "Epoch: [130][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6167(0.6167) Grad: 754.6841  LR: 0.00013303  \n",
      "Epoch: [130][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6456(0.6210) Grad: 3317.0015  LR: 0.00013303  \n",
      "Epoch: [130][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6201(0.6201) Grad: 0.0000  \n",
      "Epoch: [130][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6342) Grad: 0.0000  \n",
      "Epoch 130 - avg_train_loss: 0.6210  avg_val_loss: 0.6342  time: 0s\n",
      "Epoch 130 - Score: 0.9167\n",
      "Epoch: [131][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5877(0.5877) Grad: 734.9263  LR: 0.00013303  \n",
      "Epoch: [131][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6217(0.6155) Grad: 1850.4949  LR: 0.00013303  \n",
      "Epoch: [131][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6207(0.6207) Grad: 0.0000  \n",
      "Epoch: [131][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6343) Grad: 0.0000  \n",
      "Epoch 131 - avg_train_loss: 0.6155  avg_val_loss: 0.6343  time: 0s\n",
      "Epoch 131 - Score: 0.9167\n",
      "Epoch: [132][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 1883.9392  LR: 0.00013303  \n",
      "Epoch: [132][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6212(0.6188) Grad: 2503.4937  LR: 0.00010775  \n",
      "Epoch: [132][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6210(0.6210) Grad: 0.0000  \n",
      "Epoch: [132][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6344) Grad: 0.0000  \n",
      "Epoch 132 - avg_train_loss: 0.6188  avg_val_loss: 0.6344  time: 0s\n",
      "Epoch 132 - Score: 0.9167\n",
      "Epoch: [133][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5826(0.5826) Grad: 1205.1710  LR: 0.00011973  \n",
      "Epoch: [133][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6408(0.6181) Grad: 1365.5760  LR: 0.00011973  \n",
      "Epoch: [133][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6210(0.6210) Grad: 0.0000  \n",
      "Epoch: [133][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6489(0.6340) Grad: 0.0000  \n",
      "Epoch 133 - avg_train_loss: 0.6181  avg_val_loss: 0.6340  time: 0s\n",
      "Epoch 133 - Score: 0.9167\n",
      "Epoch: [134][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6018(0.6018) Grad: 1613.3438  LR: 0.00011973  \n",
      "Epoch: [134][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6195(0.6187) Grad: 2262.5549  LR: 0.00011973  \n",
      "Epoch: [134][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6204(0.6204) Grad: 0.0000  \n",
      "Epoch: [134][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6490(0.6338) Grad: 0.0000  \n",
      "Epoch 134 - avg_train_loss: 0.6187  avg_val_loss: 0.6338  time: 0s\n",
      "Epoch 134 - Score: 0.9167\n",
      "Epoch: [135][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6072(0.6072) Grad: 4246.1045  LR: 0.00011973  \n",
      "Epoch: [135][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6274(0.6218) Grad: 1588.1711  LR: 0.00011973  \n",
      "Epoch: [135][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6211(0.6211) Grad: 0.0000  \n",
      "Epoch: [135][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6342) Grad: 0.0000  \n",
      "Epoch 135 - avg_train_loss: 0.6218  avg_val_loss: 0.6342  time: 0s\n",
      "Epoch 135 - Score: 0.9167\n",
      "Epoch: [136][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5986(0.5986) Grad: 5702.6953  LR: 0.00009698  \n",
      "Epoch: [136][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6181) Grad: 2929.2500  LR: 0.00010775  \n",
      "Epoch: [136][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6227(0.6227) Grad: 0.0000  \n",
      "Epoch: [136][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6346) Grad: 0.0000  \n",
      "Epoch 136 - avg_train_loss: 0.6181  avg_val_loss: 0.6346  time: 0s\n",
      "Epoch 136 - Score: 0.9167\n",
      "Epoch: [137][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6285(0.6285) Grad: 1441.6350  LR: 0.00010775  \n",
      "Epoch: [137][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6002(0.6210) Grad: 1040.0582  LR: 0.00010775  \n",
      "Epoch: [137][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6227(0.6227) Grad: 0.0000  \n",
      "Epoch: [137][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6486(0.6348) Grad: 0.0000  \n",
      "Epoch 137 - avg_train_loss: 0.6210  avg_val_loss: 0.6348  time: 0s\n",
      "Epoch 137 - Score: 0.9167\n",
      "Epoch: [138][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6448(0.6448) Grad: 1978.1891  LR: 0.00010775  \n",
      "Epoch: [138][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6085(0.6188) Grad: 3558.5220  LR: 0.00010775  \n",
      "Epoch: [138][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6214(0.6214) Grad: 0.0000  \n",
      "Epoch: [138][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6348) Grad: 0.0000  \n",
      "Epoch 138 - avg_train_loss: 0.6188  avg_val_loss: 0.6348  time: 0s\n",
      "Epoch 138 - Score: 0.9167\n",
      "Epoch: [139][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6191(0.6191) Grad: 1546.9585  LR: 0.00010775  \n",
      "Epoch: [139][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6208(0.6207) Grad: 4082.1072  LR: 0.00009698  \n",
      "Epoch: [139][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6206(0.6206) Grad: 0.0000  \n",
      "Epoch: [139][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6344) Grad: 0.0000  \n",
      "Epoch 139 - avg_train_loss: 0.6207  avg_val_loss: 0.6344  time: 0s\n",
      "Epoch 139 - Score: 0.9167\n",
      "Epoch: [140][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5871(0.5871) Grad: 905.3698  LR: 0.00009698  \n",
      "Epoch: [140][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6301(0.6173) Grad: 1855.3142  LR: 0.00009698  \n",
      "Epoch: [140][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6199(0.6199) Grad: 0.0000  \n",
      "Epoch: [140][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6501(0.6340) Grad: 0.0000  \n",
      "Epoch 140 - avg_train_loss: 0.6173  avg_val_loss: 0.6340  time: 0s\n",
      "Epoch 140 - Score: 0.9208\n",
      "Epoch: [141][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6057(0.6057) Grad: 2018.9496  LR: 0.00009698  \n",
      "Epoch: [141][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6269(0.6187) Grad: 504.9501  LR: 0.00009698  \n",
      "Epoch: [141][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 0.0000  \n",
      "Epoch: [141][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6509(0.6338) Grad: 0.0000  \n",
      "Epoch 141 - avg_train_loss: 0.6187  avg_val_loss: 0.6338  time: 0s\n",
      "Epoch 141 - Score: 0.9208\n",
      "Epoch: [142][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6265(0.6265) Grad: 3498.3499  LR: 0.00009698  \n",
      "Epoch: [142][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5888(0.6205) Grad: 3004.0808  LR: 0.00008728  \n",
      "Epoch: [142][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6192(0.6192) Grad: 0.0000  \n",
      "Epoch: [142][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6338) Grad: 0.0000  \n",
      "Epoch 142 - avg_train_loss: 0.6205  avg_val_loss: 0.6338  time: 0s\n",
      "Epoch 142 - Score: 0.9208\n",
      "Epoch: [143][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5976(0.5976) Grad: 1989.1073  LR: 0.00008728  \n",
      "Epoch: [143][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6329(0.6181) Grad: 2745.8389  LR: 0.00008728  \n",
      "Epoch: [143][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6195(0.6195) Grad: 0.0000  \n",
      "Epoch: [143][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6337) Grad: 0.0000  \n",
      "Epoch 143 - avg_train_loss: 0.6181  avg_val_loss: 0.6337  time: 0s\n",
      "Epoch 143 - Score: 0.9208\n",
      "Epoch: [144][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6404(0.6404) Grad: 1565.7786  LR: 0.00008728  \n",
      "Epoch: [144][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6014(0.6205) Grad: 7102.1143  LR: 0.00008728  \n",
      "Epoch: [144][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6193(0.6193) Grad: 0.0000  \n",
      "Epoch: [144][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6503(0.6338) Grad: 0.0000  \n",
      "Epoch 144 - avg_train_loss: 0.6205  avg_val_loss: 0.6338  time: 0s\n",
      "Epoch 144 - Score: 0.9208\n",
      "Epoch: [145][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6365(0.6365) Grad: 3825.6331  LR: 0.00008728  \n",
      "Epoch: [145][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6113(0.6232) Grad: 4729.1807  LR: 0.00007855  \n",
      "Epoch: [145][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 0.0000  \n",
      "Epoch: [145][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6337) Grad: 0.0000  \n",
      "Epoch 145 - avg_train_loss: 0.6232  avg_val_loss: 0.6337  time: 0s\n",
      "Epoch 145 - Score: 0.9208\n",
      "Epoch: [146][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6467(0.6467) Grad: 1798.3708  LR: 0.00007855  \n",
      "Epoch: [146][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6275(0.6193) Grad: 3543.5796  LR: 0.00007855  \n",
      "Epoch: [146][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6191(0.6191) Grad: 0.0000  \n",
      "Epoch: [146][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6337) Grad: 0.0000  \n",
      "Epoch 146 - avg_train_loss: 0.6193  avg_val_loss: 0.6337  time: 0s\n",
      "Epoch 146 - Score: 0.9208\n",
      "Epoch: [147][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6294(0.6294) Grad: 4230.3423  LR: 0.00007855  \n",
      "Epoch: [147][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6045(0.6203) Grad: 4561.3809  LR: 0.00007855  \n",
      "Epoch: [147][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6187) Grad: 0.0000  \n",
      "Epoch: [147][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6332) Grad: 0.0000  \n",
      "Epoch 147 - avg_train_loss: 0.6203  avg_val_loss: 0.6332  time: 0s\n",
      "Epoch 147 - Score: 0.9208\n",
      "Epoch: [148][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6110(0.6110) Grad: 4188.4062  LR: 0.00007855  \n",
      "Epoch: [148][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6075(0.6199) Grad: 2679.4314  LR: 0.00007070  \n",
      "Epoch: [148][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 0.0000  \n",
      "Epoch: [148][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6493(0.6331) Grad: 0.0000  \n",
      "Epoch 148 - avg_train_loss: 0.6199  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 148 - Score: 0.9208\n",
      "Epoch: [149][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6311(0.6311) Grad: 4375.3105  LR: 0.00007070  \n",
      "Epoch: [149][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6291(0.6230) Grad: 3130.8645  LR: 0.00007070  \n",
      "Epoch: [149][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6180(0.6180) Grad: 0.0000  \n",
      "Epoch: [149][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6331) Grad: 0.0000  \n",
      "Epoch 149 - avg_train_loss: 0.6230  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 149 - Score: 0.9208\n",
      "Epoch: [150][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6260(0.6260) Grad: 772.8933  LR: 0.00007070  \n",
      "Epoch: [150][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5943(0.6167) Grad: 1504.5149  LR: 0.00007070  \n",
      "Epoch: [150][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [150][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6331) Grad: 0.0000  \n",
      "Epoch 150 - avg_train_loss: 0.6167  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 150 - Score: 0.9208\n",
      "Epoch: [151][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6171(0.6171) Grad: 1026.7327  LR: 0.00007070  \n",
      "Epoch: [151][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6140(0.6189) Grad: 3844.4614  LR: 0.00006363  \n",
      "Epoch: [151][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6182) Grad: 0.0000  \n",
      "Epoch: [151][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6331) Grad: 0.0000  \n",
      "Epoch 151 - avg_train_loss: 0.6189  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 151 - Score: 0.9208\n",
      "Epoch: [152][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6260(0.6260) Grad: 1093.0031  LR: 0.00006363  \n",
      "Epoch: [152][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6112(0.6220) Grad: 500.3116  LR: 0.00006363  \n",
      "Epoch: [152][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 0.0000  \n",
      "Epoch: [152][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6508(0.6332) Grad: 0.0000  \n",
      "Epoch 152 - avg_train_loss: 0.6220  avg_val_loss: 0.6332  time: 0s\n",
      "Epoch 152 - Score: 0.9208\n",
      "Epoch: [153][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6343(0.6343) Grad: 1665.3280  LR: 0.00006363  \n",
      "Epoch: [153][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6316(0.6154) Grad: 4554.8794  LR: 0.00006363  \n",
      "Epoch: [153][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6181) Grad: 0.0000  \n",
      "Epoch: [153][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6506(0.6332) Grad: 0.0000  \n",
      "Epoch 153 - avg_train_loss: 0.6154  avg_val_loss: 0.6332  time: 0s\n",
      "Epoch 153 - Score: 0.9208\n",
      "Epoch: [154][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5920(0.5920) Grad: 3020.8687  LR: 0.00006363  \n",
      "Epoch: [154][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6241(0.6160) Grad: 1706.3206  LR: 0.00005154  \n",
      "Epoch: [154][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [154][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6331) Grad: 0.0000  \n",
      "Epoch 154 - avg_train_loss: 0.6160  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 154 - Score: 0.9208\n",
      "Epoch: [155][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6219(0.6219) Grad: 6369.4375  LR: 0.00005726  \n",
      "Epoch: [155][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6539(0.6209) Grad: 2048.5510  LR: 0.00005726  \n",
      "Epoch: [155][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6181) Grad: 0.0000  \n",
      "Epoch: [155][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6330) Grad: 0.0000  \n",
      "Epoch 155 - avg_train_loss: 0.6209  avg_val_loss: 0.6330  time: 0s\n",
      "Epoch 155 - Score: 0.9208\n",
      "Epoch: [156][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6494) Grad: 5558.4692  LR: 0.00005726  \n",
      "Epoch: [156][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6011(0.6204) Grad: 4172.1621  LR: 0.00005726  \n",
      "Epoch: [156][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 0.0000  \n",
      "Epoch: [156][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6493(0.6329) Grad: 0.0000  \n",
      "Epoch 156 - avg_train_loss: 0.6204  avg_val_loss: 0.6329  time: 0s\n",
      "Epoch 156 - Score: 0.9208\n",
      "Epoch: [157][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6349(0.6349) Grad: 1660.6353  LR: 0.00005726  \n",
      "Epoch: [157][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6025(0.6243) Grad: 2452.3428  LR: 0.00005726  \n",
      "Epoch: [157][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 0.0000  \n",
      "Epoch: [157][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6330) Grad: 0.0000  \n",
      "Epoch 157 - avg_train_loss: 0.6243  avg_val_loss: 0.6330  time: 0s\n",
      "Epoch 157 - Score: 0.9208\n",
      "Epoch: [158][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6199(0.6199) Grad: 3439.2593  LR: 0.00004638  \n",
      "Epoch: [158][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6142(0.6179) Grad: 1706.2158  LR: 0.00005154  \n",
      "Epoch: [158][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [158][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6330) Grad: 0.0000  \n",
      "Epoch 158 - avg_train_loss: 0.6179  avg_val_loss: 0.6330  time: 0s\n",
      "Epoch 158 - Score: 0.9208\n",
      "Epoch: [159][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6337(0.6337) Grad: 916.8648  LR: 0.00005154  \n",
      "Epoch: [159][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6401(0.6162) Grad: 2155.8623  LR: 0.00005154  \n",
      "Epoch: [159][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 0.0000  \n",
      "Epoch: [159][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6330) Grad: 0.0000  \n",
      "Epoch 159 - avg_train_loss: 0.6162  avg_val_loss: 0.6330  time: 0s\n",
      "Epoch 159 - Score: 0.9208\n",
      "Epoch: [160][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6029(0.6029) Grad: 750.9719  LR: 0.00005154  \n",
      "Epoch: [160][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6591(0.6198) Grad: 3073.9426  LR: 0.00005154  \n",
      "Epoch: [160][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6187) Grad: 0.0000  \n",
      "Epoch: [160][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6329) Grad: 0.0000  \n",
      "Epoch 160 - avg_train_loss: 0.6198  avg_val_loss: 0.6329  time: 0s\n",
      "Epoch 160 - Score: 0.9208\n",
      "Epoch: [161][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6029(0.6029) Grad: 520.4900  LR: 0.00005154  \n",
      "Epoch: [161][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6328(0.6213) Grad: 3115.1350  LR: 0.00004638  \n",
      "Epoch: [161][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 0.0000  \n",
      "Epoch: [161][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6492(0.6328) Grad: 0.0000  \n",
      "Epoch 161 - avg_train_loss: 0.6213  avg_val_loss: 0.6328  time: 0s\n",
      "Epoch 161 - Score: 0.9208\n",
      "Epoch: [162][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6604(0.6604) Grad: 2339.4309  LR: 0.00004638  \n",
      "Epoch: [162][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6271(0.6182) Grad: 2194.3752  LR: 0.00004638  \n",
      "Epoch: [162][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 0.0000  \n",
      "Epoch: [162][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6330) Grad: 0.0000  \n",
      "Epoch 162 - avg_train_loss: 0.6182  avg_val_loss: 0.6330  time: 0s\n",
      "Epoch 162 - Score: 0.9208\n",
      "Epoch: [163][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6193(0.6193) Grad: 3377.2117  LR: 0.00004638  \n",
      "Epoch: [163][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6172) Grad: 2178.5913  LR: 0.00004638  \n",
      "Epoch: [163][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6182) Grad: 0.0000  \n",
      "Epoch: [163][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6330) Grad: 0.0000  \n",
      "Epoch 163 - avg_train_loss: 0.6172  avg_val_loss: 0.6330  time: 0s\n",
      "Epoch 163 - Score: 0.9208\n",
      "Epoch: [164][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6380(0.6380) Grad: 2127.5566  LR: 0.00004638  \n",
      "Epoch: [164][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6318(0.6243) Grad: 5221.0967  LR: 0.00004175  \n",
      "Epoch: [164][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 0.0000  \n",
      "Epoch: [164][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6504(0.6331) Grad: 0.0000  \n",
      "Epoch 164 - avg_train_loss: 0.6243  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 164 - Score: 0.9208\n",
      "Epoch: [165][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6348(0.6348) Grad: 496.5741  LR: 0.00004175  \n",
      "Epoch: [165][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6007(0.6212) Grad: 5349.9521  LR: 0.00004175  \n",
      "Epoch: [165][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6181) Grad: 0.0000  \n",
      "Epoch: [165][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6497(0.6329) Grad: 0.0000  \n",
      "Epoch 165 - avg_train_loss: 0.6212  avg_val_loss: 0.6329  time: 0s\n",
      "Epoch 165 - Score: 0.9208\n",
      "Epoch: [166][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6024(0.6024) Grad: 558.3600  LR: 0.00004175  \n",
      "Epoch: [166][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6178(0.6200) Grad: 941.5982  LR: 0.00004175  \n",
      "Epoch: [166][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6180(0.6180) Grad: 0.0000  \n",
      "Epoch: [166][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6502(0.6331) Grad: 0.0000  \n",
      "Epoch 166 - avg_train_loss: 0.6200  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 166 - Score: 0.9208\n",
      "Epoch: [167][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6150(0.6150) Grad: 2598.3640  LR: 0.00004175  \n",
      "Epoch: [167][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6266(0.6191) Grad: 1559.2144  LR: 0.00003757  \n",
      "Epoch: [167][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 0.0000  \n",
      "Epoch: [167][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6332) Grad: 0.0000  \n",
      "Epoch 167 - avg_train_loss: 0.6191  avg_val_loss: 0.6332  time: 0s\n",
      "Epoch 167 - Score: 0.9208\n",
      "Epoch: [168][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6243(0.6243) Grad: 795.5328  LR: 0.00003757  \n",
      "Epoch: [168][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6299(0.6173) Grad: 3179.3728  LR: 0.00003757  \n",
      "Epoch: [168][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [168][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6500(0.6331) Grad: 0.0000  \n",
      "Epoch 168 - avg_train_loss: 0.6173  avg_val_loss: 0.6331  time: 0s\n",
      "Epoch 168 - Score: 0.9208\n",
      "Epoch: [169][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6200(0.6200) Grad: 942.3389  LR: 0.00003757  \n",
      "Epoch: [169][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6309(0.6186) Grad: 4883.8628  LR: 0.00003757  \n",
      "Epoch: [169][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 0.0000  \n",
      "Epoch: [169][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6496(0.6330) Grad: 0.0000  \n",
      "Epoch 169 - avg_train_loss: 0.6186  avg_val_loss: 0.6330  time: 0s\n",
      "Epoch 169 - Score: 0.9208\n",
      "Epoch: [170][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 3143.3083  LR: 0.00003757  \n",
      "Epoch: [170][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6290(0.6191) Grad: 3277.9175  LR: 0.00003381  \n",
      "Epoch: [170][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 0.0000  \n",
      "Epoch: [170][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6493(0.6328) Grad: 0.0000  \n",
      "Epoch 170 - avg_train_loss: 0.6191  avg_val_loss: 0.6328  time: 0s\n",
      "Epoch 170 - Score: 0.9208\n",
      "Epoch: [171][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5931(0.5931) Grad: 1634.9565  LR: 0.00003381  \n",
      "Epoch: [171][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6306(0.6211) Grad: 2509.9133  LR: 0.00003381  \n",
      "Epoch: [171][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [171][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6328) Grad: 0.0000  \n",
      "Epoch 171 - avg_train_loss: 0.6211  avg_val_loss: 0.6328  time: 0s\n",
      "Epoch 171 - Score: 0.9208\n",
      "Epoch: [172][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6238(0.6238) Grad: 2868.7712  LR: 0.00003381  \n",
      "Epoch: [172][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6243(0.6210) Grad: 4249.6211  LR: 0.00003381  \n",
      "Epoch: [172][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6181) Grad: 0.0000  \n",
      "Epoch: [172][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6326) Grad: 0.0000  \n",
      "Epoch 172 - avg_train_loss: 0.6210  avg_val_loss: 0.6326  time: 0s\n",
      "Epoch 172 - Score: 0.9208\n",
      "Epoch: [173][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6065(0.6065) Grad: 2283.7344  LR: 0.00003381  \n",
      "Epoch: [173][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5970(0.6179) Grad: 1365.8076  LR: 0.00003043  \n",
      "Epoch: [173][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6182) Grad: 0.0000  \n",
      "Epoch: [173][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6326) Grad: 0.0000  \n",
      "Epoch 173 - avg_train_loss: 0.6179  avg_val_loss: 0.6326  time: 0s\n",
      "Epoch 173 - Score: 0.9208\n",
      "Epoch: [174][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6032(0.6032) Grad: 3474.2964  LR: 0.00003043  \n",
      "Epoch: [174][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6229(0.6169) Grad: 1335.3728  LR: 0.00003043  \n",
      "Epoch: [174][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6182) Grad: 0.0000  \n",
      "Epoch: [174][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6326) Grad: 0.0000  \n",
      "Epoch 174 - avg_train_loss: 0.6169  avg_val_loss: 0.6326  time: 0s\n",
      "Epoch 174 - Score: 0.9208\n",
      "Epoch: [175][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6286(0.6286) Grad: 4934.1592  LR: 0.00003043  \n",
      "Epoch: [175][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6429(0.6191) Grad: 2325.9355  LR: 0.00003043  \n",
      "Epoch: [175][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [175][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6328) Grad: 0.0000  \n",
      "Epoch 175 - avg_train_loss: 0.6191  avg_val_loss: 0.6328  time: 0s\n",
      "Epoch 175 - Score: 0.9208\n",
      "Epoch: [176][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6249(0.6249) Grad: 1003.9105  LR: 0.00003043  \n",
      "Epoch: [176][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6114(0.6176) Grad: 523.4830  LR: 0.00002465  \n",
      "Epoch: [176][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6180(0.6180) Grad: 0.0000  \n",
      "Epoch: [176][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6499(0.6329) Grad: 0.0000  \n",
      "Epoch 176 - avg_train_loss: 0.6176  avg_val_loss: 0.6329  time: 0s\n",
      "Epoch 176 - Score: 0.9208\n",
      "Epoch: [177][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6089(0.6089) Grad: 3776.2854  LR: 0.00002739  \n",
      "Epoch: [177][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6180) Grad: 685.1443  LR: 0.00002739  \n",
      "Epoch: [177][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6181) Grad: 0.0000  \n",
      "Epoch: [177][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6329) Grad: 0.0000  \n",
      "Epoch 177 - avg_train_loss: 0.6180  avg_val_loss: 0.6329  time: 0s\n",
      "Epoch 177 - Score: 0.9208\n",
      "Epoch: [178][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6194) Grad: 3515.6594  LR: 0.00002739  \n",
      "Epoch: [178][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5977(0.6193) Grad: 2230.4006  LR: 0.00002739  \n",
      "Epoch: [178][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6182) Grad: 0.0000  \n",
      "Epoch: [178][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6489(0.6325) Grad: 0.0000  \n",
      "Epoch 178 - avg_train_loss: 0.6193  avg_val_loss: 0.6325  time: 0s\n",
      "Epoch 178 - Score: 0.9208\n",
      "Epoch: [179][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6245(0.6245) Grad: 730.8214  LR: 0.00002739  \n",
      "Epoch: [179][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5935(0.6159) Grad: 3810.0728  LR: 0.00002739  \n",
      "Epoch: [179][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6180(0.6180) Grad: 0.0000  \n",
      "Epoch: [179][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6325) Grad: 0.0000  \n",
      "Epoch 179 - avg_train_loss: 0.6159  avg_val_loss: 0.6325  time: 0s\n",
      "Epoch 179 - Score: 0.9208\n",
      "Epoch: [180][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6317(0.6317) Grad: 2239.4194  LR: 0.00002219  \n",
      "Epoch: [180][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6029(0.6177) Grad: 510.8587  LR: 0.00002465  \n",
      "Epoch: [180][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 0.0000  \n",
      "Epoch: [180][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6494(0.6326) Grad: 0.0000  \n",
      "Epoch 180 - avg_train_loss: 0.6177  avg_val_loss: 0.6326  time: 0s\n",
      "Epoch 180 - Score: 0.9208\n",
      "Epoch: [181][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6194(0.6194) Grad: 2682.4509  LR: 0.00002465  \n",
      "Epoch: [181][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6434(0.6201) Grad: 2071.6816  LR: 0.00002465  \n",
      "Epoch: [181][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6180(0.6180) Grad: 0.0000  \n",
      "Epoch: [181][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6491(0.6325) Grad: 0.0000  \n",
      "Epoch 181 - avg_train_loss: 0.6201  avg_val_loss: 0.6325  time: 0s\n",
      "Epoch 181 - Score: 0.9208\n",
      "Epoch: [182][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6139(0.6139) Grad: 4605.5967  LR: 0.00002465  \n",
      "Epoch: [182][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6367(0.6239) Grad: 1977.3127  LR: 0.00002465  \n",
      "Epoch: [182][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6182) Grad: 0.0000  \n",
      "Epoch: [182][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6485(0.6323) Grad: 0.0000  \n",
      "Epoch 182 - avg_train_loss: 0.6239  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 182 - Score: 0.9208\n",
      "Epoch: [183][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6179(0.6179) Grad: 2266.1538  LR: 0.00002465  \n",
      "Epoch: [183][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6415(0.6208) Grad: 4639.7827  LR: 0.00002219  \n",
      "Epoch: [183][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 0.0000  \n",
      "Epoch: [183][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6322) Grad: 0.0000  \n",
      "Epoch 183 - avg_train_loss: 0.6208  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 183 - Score: 0.9208\n",
      "Epoch: [184][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6098(0.6098) Grad: 1825.1659  LR: 0.00002219  \n",
      "Epoch: [184][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6010(0.6202) Grad: 3054.6587  LR: 0.00002219  \n",
      "Epoch: [184][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [184][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6485(0.6324) Grad: 0.0000  \n",
      "Epoch 184 - avg_train_loss: 0.6202  avg_val_loss: 0.6324  time: 0s\n",
      "Epoch 184 - Score: 0.9208\n",
      "Epoch: [185][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6052(0.6052) Grad: 1998.0829  LR: 0.00002219  \n",
      "Epoch: [185][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6331(0.6152) Grad: 3323.3445  LR: 0.00002219  \n",
      "Epoch: [185][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6181(0.6181) Grad: 0.0000  \n",
      "Epoch: [185][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6490(0.6325) Grad: 0.0000  \n",
      "Epoch 185 - avg_train_loss: 0.6152  avg_val_loss: 0.6325  time: 0s\n",
      "Epoch 185 - Score: 0.9208\n",
      "Epoch: [186][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6031(0.6031) Grad: 1628.7023  LR: 0.00002219  \n",
      "Epoch: [186][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6468(0.6189) Grad: 2481.9753  LR: 0.00001997  \n",
      "Epoch: [186][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 0.0000  \n",
      "Epoch: [186][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6482(0.6324) Grad: 0.0000  \n",
      "Epoch 186 - avg_train_loss: 0.6189  avg_val_loss: 0.6324  time: 0s\n",
      "Epoch 186 - Score: 0.9208\n",
      "Epoch: [187][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6312(0.6312) Grad: 2839.9055  LR: 0.00001997  \n",
      "Epoch: [187][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6379(0.6232) Grad: 2602.5371  LR: 0.00001997  \n",
      "Epoch: [187][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 0.0000  \n",
      "Epoch: [187][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6481(0.6323) Grad: 0.0000  \n",
      "Epoch 187 - avg_train_loss: 0.6232  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 187 - Score: 0.9208\n",
      "Epoch: [188][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6117(0.6117) Grad: 2593.8662  LR: 0.00001997  \n",
      "Epoch: [188][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6199(0.6184) Grad: 5687.1743  LR: 0.00001997  \n",
      "Epoch: [188][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 0.0000  \n",
      "Epoch: [188][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6478(0.6323) Grad: 0.0000  \n",
      "Epoch 188 - avg_train_loss: 0.6184  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 188 - Score: 0.9208\n",
      "Epoch: [189][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6175(0.6175) Grad: 876.2186  LR: 0.00001997  \n",
      "Epoch: [189][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5788(0.6159) Grad: 761.5182  LR: 0.00001797  \n",
      "Epoch: [189][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6184(0.6184) Grad: 0.0000  \n",
      "Epoch: [189][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6479(0.6322) Grad: 0.0000  \n",
      "Epoch 189 - avg_train_loss: 0.6159  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 189 - Score: 0.9208\n",
      "Epoch: [190][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6302(0.6302) Grad: 3793.8228  LR: 0.00001797  \n",
      "Epoch: [190][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6218(0.6209) Grad: 3787.6931  LR: 0.00001797  \n",
      "Epoch: [190][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 0.0000  \n",
      "Epoch: [190][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6479(0.6322) Grad: 0.0000  \n",
      "Epoch 190 - avg_train_loss: 0.6209  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 190 - Score: 0.9208\n",
      "Epoch: [191][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6028(0.6028) Grad: 5154.3623  LR: 0.00001797  \n",
      "Epoch: [191][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6172) Grad: 2202.3196  LR: 0.00001797  \n",
      "Epoch: [191][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6183(0.6183) Grad: 0.0000  \n",
      "Epoch: [191][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6482(0.6322) Grad: 0.0000  \n",
      "Epoch 191 - avg_train_loss: 0.6172  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 191 - Score: 0.9208\n",
      "Epoch: [192][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6405(0.6405) Grad: 430.5521  LR: 0.00001797  \n",
      "Epoch: [192][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6047(0.6187) Grad: 6641.9839  LR: 0.00001617  \n",
      "Epoch: [192][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6187) Grad: 0.0000  \n",
      "Epoch: [192][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6473(0.6321) Grad: 0.0000  \n",
      "Epoch 192 - avg_train_loss: 0.6187  avg_val_loss: 0.6321  time: 0s\n",
      "Epoch 192 - Score: 0.9208\n",
      "Epoch: [193][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6217(0.6217) Grad: 2567.5718  LR: 0.00001617  \n",
      "Epoch: [193][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6362(0.6189) Grad: 1793.2260  LR: 0.00001617  \n",
      "Epoch: [193][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6189(0.6189) Grad: 0.0000  \n",
      "Epoch: [193][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6474(0.6322) Grad: 0.0000  \n",
      "Epoch 193 - avg_train_loss: 0.6189  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 193 - Score: 0.9208\n",
      "Epoch: [194][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6167(0.6167) Grad: 656.9406  LR: 0.00001617  \n",
      "Epoch: [194][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6238(0.6182) Grad: 2959.7361  LR: 0.00001617  \n",
      "Epoch: [194][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6189(0.6189) Grad: 0.0000  \n",
      "Epoch: [194][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6472(0.6321) Grad: 0.0000  \n",
      "Epoch 194 - avg_train_loss: 0.6182  avg_val_loss: 0.6321  time: 0s\n",
      "Epoch 194 - Score: 0.9208\n",
      "Epoch: [195][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6192(0.6192) Grad: 2965.1243  LR: 0.00001617  \n",
      "Epoch: [195][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6498(0.6182) Grad: 423.2756  LR: 0.00001456  \n",
      "Epoch: [195][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6188(0.6188) Grad: 0.0000  \n",
      "Epoch: [195][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6476(0.6323) Grad: 0.0000  \n",
      "Epoch 195 - avg_train_loss: 0.6182  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 195 - Score: 0.9208\n",
      "Epoch: [196][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6328(0.6328) Grad: 969.6559  LR: 0.00001456  \n",
      "Epoch: [196][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6398(0.6173) Grad: 1582.0436  LR: 0.00001456  \n",
      "Epoch: [196][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 0.0000  \n",
      "Epoch: [196][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6478(0.6322) Grad: 0.0000  \n",
      "Epoch 196 - avg_train_loss: 0.6173  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 196 - Score: 0.9208\n",
      "Epoch: [197][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6344(0.6344) Grad: 8846.1738  LR: 0.00001456  \n",
      "Epoch: [197][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6092(0.6172) Grad: 529.3495  LR: 0.00001456  \n",
      "Epoch: [197][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6182(0.6182) Grad: 0.0000  \n",
      "Epoch: [197][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6483(0.6323) Grad: 0.0000  \n",
      "Epoch 197 - avg_train_loss: 0.6172  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 197 - Score: 0.9208\n",
      "Epoch: [198][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6198(0.6198) Grad: 3512.9404  LR: 0.00001456  \n",
      "Epoch: [198][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5842(0.6176) Grad: 1654.0494  LR: 0.00001179  \n",
      "Epoch: [198][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6186(0.6186) Grad: 0.0000  \n",
      "Epoch: [198][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6481(0.6324) Grad: 0.0000  \n",
      "Epoch 198 - avg_train_loss: 0.6176  avg_val_loss: 0.6324  time: 0s\n",
      "Epoch 198 - Score: 0.9208\n",
      "Epoch: [199][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6051(0.6051) Grad: 723.6216  LR: 0.00001310  \n",
      "Epoch: [199][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.5969(0.6212) Grad: 2673.7646  LR: 0.00001310  \n",
      "Epoch: [199][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6185(0.6185) Grad: 0.0000  \n",
      "Epoch: [199][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6480(0.6323) Grad: 0.0000  \n",
      "Epoch 199 - avg_train_loss: 0.6212  avg_val_loss: 0.6323  time: 0s\n",
      "Epoch 199 - Score: 0.9208\n",
      "Epoch: [200][0/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6213(0.6213) Grad: 5062.9282  LR: 0.00001310  \n",
      "Epoch: [200][6/7] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6334(0.6184) Grad: 456.0215  LR: 0.00001310  \n",
      "Epoch: [200][0/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6187(0.6187) Grad: 0.0000  \n",
      "Epoch: [200][1/2] Elapsed 0m 0s (remain 0m 0s) Loss: 0.6476(0.6322) Grad: 0.0000  \n",
      "Epoch 200 - avg_train_loss: 0.6184  avg_val_loss: 0.6322  time: 0s\n",
      "Epoch 200 - Score: 0.9208\n",
      "========== fold: 4 result ==========\n",
      "Score: 0.9375\n",
      "========== CV ==========\n",
      "Score: 0.9283\n"
     ]
    }
   ],
   "source": [
    "def get_result(oof_df):\n",
    "    labels = oof_df[CFG.target].values\n",
    "    preds = oof_df['pred'].values\n",
    "    score = get_score(labels, preds)\n",
    "    print(f'Score: {score:<.4f}')\n",
    "\n",
    "oof_df = pd.DataFrame()\n",
    "for fold in range(CFG.n_fold):\n",
    "    if fold in CFG.trn_fold:\n",
    "        _oof_df = train_loop(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        print(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "oof_df = oof_df.reset_index(drop=True)\n",
    "print(f\"========== CV ==========\")\n",
    "get_result(oof_df)\n",
    "oof_df.to_pickle('oof_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fn(test_loader, model, device):\n",
    "    ret = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs, labels in tk0:\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        _, preds = torch.max(y_preds, dim=1)\n",
    "        ret.append(preds.to('cpu').numpy())\n",
    "    predictions = np.concatenate(ret)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Net1_fold0_best.pth' 'Net1_fold1_best.pth' 'Net1_fold2_best.pth'\n",
      " 'Net1_fold3_best.pth' 'Net1_fold4_best.pth' 'Net1_fold5_best.pth'\n",
      " 'Net1_fold6_best.pth' 'Net1_fold7_best.pth' 'Net1_fold8_best.pth'\n",
      " 'Net1_fold9_best.pth']\n"
     ]
    }
   ],
   "source": [
    "model_path = ''\n",
    "models = np.sort(glob.glob(f\"*best.pth\"))\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TrainDataset(test, features, CFG.target)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                            batch_size=CFG.batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=CFG.num_workers, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1e866a24924db19340e51a180da4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee6aceb708144cfbcd4ca3b11a7e259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cc0b017c5246ac881ae8e21f7e1b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d56239571f4bb8a9700a0c404005ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4ba72ddb844fe6b321d6af750560c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb08fcbbaf94edfb6ed74f70b19b9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d65d9348744463b92b1b37d323563fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1f9d7317ab4f5abebad8a3de0efd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef80ac8941af459a9471a55c484dac81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf37affc7ed4704884d06eb90762ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for model_name in models:\n",
    "    model = Net1(len(features)).to(device)\n",
    "    state = torch.load(model_name, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    df = pd.concat([df, pd.DataFrame(prediction)], axis=1)\n",
    "    del model, state, prediction; gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  0  0  0  0  0  0  0  0  0\n",
       "0    0  0  0  0  0  0  0  0  0  0\n",
       "1    0  0  0  0  0  0  0  0  0  0\n",
       "2    2  2  2  2  2  2  2  2  2  2\n",
       "3    0  0  0  0  0  0  0  0  0  0\n",
       "4    1  1  1  1  1  1  1  1  1  1\n",
       "..  .. .. .. .. .. .. .. .. .. ..\n",
       "795  0  0  0  0  0  0  0  0  0  0\n",
       "796  0  0  0  0  0  0  0  0  0  0\n",
       "797  0  0  0  0  0  0  0  0  0  0\n",
       "798  1  1  1  1  1  1  1  1  2  1\n",
       "799  0  0  0  0  0  0  0  0  0  0\n",
       "\n",
       "[800 rows x 10 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred'] = df.mode(axis=1)[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = test['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['id', 'pred']].to_csv('submission.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f8cdbf2a96461788475085dd1e9d6dd7137de331e19aa3c17c37cb4f0963a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yourenvname')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
