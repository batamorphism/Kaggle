{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Takanori\\\\Desktop\\\\Kaggle\\\\TPS2205\\\\input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet & Library Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    min_batch_size = 1_000  # max_batch_sizeの1000分の1程度が限界か\n",
    "    max_batch_size = 1_000_000\n",
    "    batch_increase_rate = 2\n",
    "    epochs = 10_000\n",
    "    folds = 10\n",
    "    seed = 42\n",
    "    target = 'target'\n",
    "    model_path = \"models\"\n",
    "    test_pred = ['pred' + str(i) for i in range(folds)]  # 各foldで作ったモデル別の、testを予測した結果のカラム名\n",
    "    pred = 'pred'  # trainを予測した結果のカラム名\n",
    "    increase_batch = 10  # 何epoch間lossが改善しなかったbatchを増やすか\n",
    "    early_stopping = 200  # 何epochでearly stoppingをするか\n",
    "    lr = 0.01\n",
    "    lr_factor = 0.2  # lrを何倍にするか\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CFG.model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>...</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.373246</td>\n",
       "      <td>0.238887</td>\n",
       "      <td>-0.243376</td>\n",
       "      <td>0.567405</td>\n",
       "      <td>-0.647715</td>\n",
       "      <td>0.839326</td>\n",
       "      <td>0.113133</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.540739</td>\n",
       "      <td>0.766952</td>\n",
       "      <td>-2.730628</td>\n",
       "      <td>-0.208177</td>\n",
       "      <td>1.363402</td>\n",
       "      <td>ABABDADBAB</td>\n",
       "      <td>67.609153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.697021</td>\n",
       "      <td>-1.710322</td>\n",
       "      <td>-2.230332</td>\n",
       "      <td>-0.545661</td>\n",
       "      <td>1.113173</td>\n",
       "      <td>-1.552175</td>\n",
       "      <td>0.447825</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.278315</td>\n",
       "      <td>-0.633658</td>\n",
       "      <td>-1.217077</td>\n",
       "      <td>-3.782194</td>\n",
       "      <td>-0.058316</td>\n",
       "      <td>ACACCADCEB</td>\n",
       "      <td>377.096415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.681726</td>\n",
       "      <td>0.616746</td>\n",
       "      <td>-1.027689</td>\n",
       "      <td>0.810492</td>\n",
       "      <td>-0.609086</td>\n",
       "      <td>0.113965</td>\n",
       "      <td>-0.708660</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.385775</td>\n",
       "      <td>-0.520558</td>\n",
       "      <td>-0.009121</td>\n",
       "      <td>2.788536</td>\n",
       "      <td>-3.703488</td>\n",
       "      <td>AAAEABCKAD</td>\n",
       "      <td>-195.599702</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.118172</td>\n",
       "      <td>-0.587835</td>\n",
       "      <td>-0.804638</td>\n",
       "      <td>2.086822</td>\n",
       "      <td>0.371005</td>\n",
       "      <td>-0.128831</td>\n",
       "      <td>-0.282575</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572594</td>\n",
       "      <td>-1.653213</td>\n",
       "      <td>1.686035</td>\n",
       "      <td>-2.533098</td>\n",
       "      <td>-0.608601</td>\n",
       "      <td>BDBBAACBCB</td>\n",
       "      <td>210.826205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.148481</td>\n",
       "      <td>-0.176567</td>\n",
       "      <td>-0.664871</td>\n",
       "      <td>-1.101343</td>\n",
       "      <td>0.467875</td>\n",
       "      <td>0.500117</td>\n",
       "      <td>0.407515</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.912929</td>\n",
       "      <td>-1.430366</td>\n",
       "      <td>2.127649</td>\n",
       "      <td>-3.306784</td>\n",
       "      <td>4.371371</td>\n",
       "      <td>BDBCBBCHFE</td>\n",
       "      <td>-217.211798</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
       "0   0 -1.373246  0.238887 -0.243376  0.567405 -0.647715  0.839326  0.113133   \n",
       "1   1  1.697021 -1.710322 -2.230332 -0.545661  1.113173 -1.552175  0.447825   \n",
       "2   2  1.681726  0.616746 -1.027689  0.810492 -0.609086  0.113965 -0.708660   \n",
       "3   3 -0.118172 -0.587835 -0.804638  2.086822  0.371005 -0.128831 -0.282575   \n",
       "4   4  1.148481 -0.176567 -0.664871 -1.101343  0.467875  0.500117  0.407515   \n",
       "\n",
       "   f_07  f_08  ...      f_22      f_23      f_24      f_25      f_26  \\\n",
       "0     1     5  ... -2.540739  0.766952 -2.730628 -0.208177  1.363402   \n",
       "1     1     3  ...  2.278315 -0.633658 -1.217077 -3.782194 -0.058316   \n",
       "2     1     0  ... -1.385775 -0.520558 -0.009121  2.788536 -3.703488   \n",
       "3     3     2  ...  0.572594 -1.653213  1.686035 -2.533098 -0.608601   \n",
       "4     3     3  ... -3.912929 -1.430366  2.127649 -3.306784  4.371371   \n",
       "\n",
       "         f_27        f_28  f_29  f_30  target  \n",
       "0  ABABDADBAB   67.609153     0     0       0  \n",
       "1  ACACCADCEB  377.096415     0     0       1  \n",
       "2  AAAEABCKAD -195.599702     0     2       1  \n",
       "3  BDBBAACBCB  210.826205     0     0       1  \n",
       "4  BDBCBBCHFE -217.211798     0     1       1  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>...</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900000</td>\n",
       "      <td>0.442517</td>\n",
       "      <td>0.174380</td>\n",
       "      <td>-0.999816</td>\n",
       "      <td>0.762741</td>\n",
       "      <td>0.186778</td>\n",
       "      <td>-1.074775</td>\n",
       "      <td>0.501888</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.006400</td>\n",
       "      <td>-1.193879</td>\n",
       "      <td>-2.435736</td>\n",
       "      <td>-2.427430</td>\n",
       "      <td>-1.966887</td>\n",
       "      <td>5.734205</td>\n",
       "      <td>BAAABADLAC</td>\n",
       "      <td>99.478419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900001</td>\n",
       "      <td>-0.605598</td>\n",
       "      <td>-0.305715</td>\n",
       "      <td>0.627667</td>\n",
       "      <td>-0.578898</td>\n",
       "      <td>-1.750931</td>\n",
       "      <td>1.355550</td>\n",
       "      <td>-0.190911</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.382405</td>\n",
       "      <td>0.149442</td>\n",
       "      <td>1.883322</td>\n",
       "      <td>-2.848714</td>\n",
       "      <td>-0.725155</td>\n",
       "      <td>3.194219</td>\n",
       "      <td>AFABBAEGCB</td>\n",
       "      <td>-65.993825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900002</td>\n",
       "      <td>0.303990</td>\n",
       "      <td>2.445110</td>\n",
       "      <td>0.246515</td>\n",
       "      <td>0.818248</td>\n",
       "      <td>0.359731</td>\n",
       "      <td>-1.331845</td>\n",
       "      <td>1.358622</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.026098</td>\n",
       "      <td>1.312277</td>\n",
       "      <td>-5.157192</td>\n",
       "      <td>1.714005</td>\n",
       "      <td>0.585032</td>\n",
       "      <td>0.066898</td>\n",
       "      <td>BBACABBKEE</td>\n",
       "      <td>-87.405622</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>900003</td>\n",
       "      <td>0.154053</td>\n",
       "      <td>0.260126</td>\n",
       "      <td>-1.367092</td>\n",
       "      <td>-0.093175</td>\n",
       "      <td>-1.111034</td>\n",
       "      <td>-0.948481</td>\n",
       "      <td>1.119220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.594532</td>\n",
       "      <td>-3.939475</td>\n",
       "      <td>1.754570</td>\n",
       "      <td>-2.364007</td>\n",
       "      <td>-1.003320</td>\n",
       "      <td>3.893099</td>\n",
       "      <td>AEBEAACQCC</td>\n",
       "      <td>-281.293460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900004</td>\n",
       "      <td>-1.651904</td>\n",
       "      <td>-0.424266</td>\n",
       "      <td>-0.667356</td>\n",
       "      <td>-0.322124</td>\n",
       "      <td>-0.089462</td>\n",
       "      <td>0.181705</td>\n",
       "      <td>1.784983</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>-0.985736</td>\n",
       "      <td>-0.130467</td>\n",
       "      <td>-3.557893</td>\n",
       "      <td>1.210687</td>\n",
       "      <td>1.861884</td>\n",
       "      <td>AEBBBBDABF</td>\n",
       "      <td>25.629415</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      f_00      f_01      f_02      f_03      f_04      f_05  \\\n",
       "0  900000  0.442517  0.174380 -0.999816  0.762741  0.186778 -1.074775   \n",
       "1  900001 -0.605598 -0.305715  0.627667 -0.578898 -1.750931  1.355550   \n",
       "2  900002  0.303990  2.445110  0.246515  0.818248  0.359731 -1.331845   \n",
       "3  900003  0.154053  0.260126 -1.367092 -0.093175 -1.111034 -0.948481   \n",
       "4  900004 -1.651904 -0.424266 -0.667356 -0.322124 -0.089462  0.181705   \n",
       "\n",
       "       f_06  f_07  f_08  ...      f_21      f_22      f_23      f_24  \\\n",
       "0  0.501888     6     6  ... -1.006400 -1.193879 -2.435736 -2.427430   \n",
       "1 -0.190911     1     3  ...  2.382405  0.149442  1.883322 -2.848714   \n",
       "2  1.358622     3     3  ... -7.026098  1.312277 -5.157192  1.714005   \n",
       "3  1.119220     0     0  ... -0.594532 -3.939475  1.754570 -2.364007   \n",
       "4  1.784983     2     2  ...  0.084906 -0.985736 -0.130467 -3.557893   \n",
       "\n",
       "       f_25      f_26        f_27        f_28  f_29  f_30  \n",
       "0 -1.966887  5.734205  BAAABADLAC   99.478419     0     0  \n",
       "1 -0.725155  3.194219  AFABBAEGCB  -65.993825     1     0  \n",
       "2  0.585032  0.066898  BBACABBKEE  -87.405622     0     1  \n",
       "3 -1.003320  3.893099  AEBEAACQCC -281.293460     0     0  \n",
       "4  1.210687  1.861884  AEBBBBDABF   25.629415     0     2  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900002</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>900003</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900004</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  target\n",
       "0  900000     0.5\n",
       "1  900001     0.5\n",
       "2  900002     0.5\n",
       "3  900003     0.5\n",
       "4  900004     0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# データセット読み込み\n",
    "df_train = pd.read_csv(\"../input/tabular-playground-series-may-2022/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/tabular-playground-series-may-2022/test.csv\")\n",
    "df_sub = pd.read_csv(\"../input/tabular-playground-series-may-2022/sample_submission.csv\")\n",
    "display(df_train.head())\n",
    "display(df_test.head())\n",
    "display(df_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要データを削除\n",
    "df_all.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA埋め\n",
    "df_all.fillna(df_all.mean(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_27以外のカテゴリ変数をone_hot_encode\n",
    "# 整数なので、一応値そのものは残しておく\n",
    "cat_features = ['f_' + str(i).zfill(2) for i in range(7, 19)] + ['f_' + str(i).zfill(2) for i in range(29, 31)]\n",
    "new_df = pd.get_dummies(df_all[cat_features].astype('category'))\n",
    "df_all = pd.concat([df_all, new_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lrm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大体10分くらいかかる\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df_all[['lr_a', 'lr_b']] = df_all['f_27'].progress_apply(calc_linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113301it [01:17, 1470.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\Takanori\\Documents\\github\\myprivate\\Kaggle\\TPS2205\\Pytorch_DNN.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=13'>14</a>\u001b[0m ret2 \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, txt \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(tmp)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=15'>16</a>\u001b[0m     a, b \u001b[39m=\u001b[39m calc_linear_regression(txt)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=16'>17</a>\u001b[0m     ret1\u001b[39m.\u001b[39mappend(a)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=17'>18</a>\u001b[0m     ret2\u001b[39m.\u001b[39mappend(b)\n",
      "\u001b[1;32md:\\Users\\Takanori\\Documents\\github\\myprivate\\Kaggle\\TPS2205\\Pytorch_DNN.ipynb Cell 15'\u001b[0m in \u001b[0;36mcalc_linear_regression\u001b[1;34m(txt)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=6'>7</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(arr)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=7'>8</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(y))\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=8'>9</a>\u001b[0m lrm\u001b[39m.\u001b[39;49mfit(x, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000045?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m lrm\u001b[39m.\u001b[39mcoef_[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem(), lrm\u001b[39m.\u001b[39mintercept_\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\sklearn\\linear_model\\_base.py:512\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=507'>508</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=508'>509</a>\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X,\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=509'>510</a>\u001b[0m                                          dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=511'>512</a>\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_data(\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=512'>513</a>\u001b[0m     X, y, fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept, normalize\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize,\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=513'>514</a>\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_X, sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=514'>515</a>\u001b[0m     return_mean\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=516'>517</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=517'>518</a>\u001b[0m     \u001b[39m# Sample weight can be implemented via a simple rescaling.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=518'>519</a>\u001b[0m     X, y \u001b[39m=\u001b[39m _rescale_data(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\sklearn\\linear_model\\_base.py:136\u001b[0m, in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, sample_weight, return_mean, check_input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=132'>133</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=133'>134</a>\u001b[0m         X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mcopy(order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mK\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=135'>136</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(y, dtype\u001b[39m=\u001b[39;49mX\u001b[39m.\u001b[39;49mdtype)\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=137'>138</a>\u001b[0m \u001b[39mif\u001b[39;00m fit_intercept:\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/sklearn/linear_model/_base.py?line=138'>139</a>\u001b[0m     \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39missparse(X):\n",
      "File \u001b[1;32mc:\\Users\\Takanori\\MiniConda3\\envs\\yourenvname\\lib\\site-packages\\numpy\\core\\_asarray.py:102\u001b[0m, in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/numpy/core/_asarray.py?line=98'>99</a>\u001b[0m \u001b[39mif\u001b[39;00m like \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/numpy/core/_asarray.py?line=99'>100</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _asarray_with_like(a, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder, like\u001b[39m=\u001b[39mlike)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Takanori/MiniConda3/envs/yourenvname/lib/site-packages/numpy/core/_asarray.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m array(a, dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# numpy配列にしてから回してみる\n",
    "def c2i(c):\n",
    "    return ord(c)-ord('A')\n",
    "\n",
    "def calc_linear_regression(txt):\n",
    "    arr = [c2i(c) for c in txt]\n",
    "    y = np.array(arr).reshape(-1, 1)\n",
    "    x = np.arange(len(y)).reshape(-1, 1)\n",
    "    lrm.fit(x, y)\n",
    "    return lrm.coef_[0].item(), lrm.intercept_.item()\n",
    "\n",
    "tmp = df_all['f_27'].values\n",
    "ret1 = []\n",
    "ret2 = []\n",
    "for i, txt in tqdm(enumerate(tmp)):\n",
    "    a, b = calc_linear_regression(txt)\n",
    "    ret1.append(a)\n",
    "    ret2.append(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_27を分解\n",
    "def splitter(text):\n",
    "    arr = tuple(text)\n",
    "    return arr\n",
    "\n",
    "column_list = ['f_27_' + str(i) for i in range(10)]\n",
    "df_all[column_list] = df_all.apply(lambda x: splitter(x['f_27']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新設したf_27を分解したものを、one-hot-encoding\n",
    "new_df = pd.get_dummies(df_all[column_list])\n",
    "df_all = pd.concat([df_all, new_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_27_0~9を数値に\n",
    "for column in column_list:\n",
    "    df_all[column] = df_all[column].apply(lambda x: ord(x)-ord('A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形回帰\n",
    "# 遅すぎるので一旦飛ばす\n",
    "def calc_linear_regression(pdseries):\n",
    "    y = pdseries.values.astype(float)\n",
    "    x = np.arange(len(y)).astype(float)\n",
    "    result = np.polyfit(x, y, 1)\n",
    "    return tuple(result.tolist())\n",
    "# df_all[['f_27_linear_regression_a', 'f_27_linear_regression_b']] = df_all.apply(lambda x: calc_linear_regression(x[column_list]), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(pdseries):\n",
    "    return pdseries.mean()\n",
    "\n",
    "df_all['mean'] = df_all.apply(lambda x: calc_mean(x[column_list]), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要になったカテゴリ変数を削除\n",
    "df_all.drop(['f_27'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この時点で数値しかない。正規化\n",
    "scaler = StandardScaler()\n",
    "for column in df_all.columns:\n",
    "    if column == CFG.target:\n",
    "        continue\n",
    "    if df_all[column].dtype == 'object':\n",
    "        continue\n",
    "    df_all[column] = scaler.fit_transform(df_all[column].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all[:len(df_train)]\n",
    "df_test = df_all[len(df_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('train_fe.pkl')\n",
    "df_test.to_pickle('test_fe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('train_fe.pkl')\n",
    "df_test = pd.read_pickle('test_fe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thank you very much https://www.kaggle.com/mburakergenc/ttianic-minimal-pytorch-mlp\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # 5層\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn5 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.silu(self.fc1(x)))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(F.silu(self.fc2(x)))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(F.silu(self.fc3(x)))\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(F.silu(self.fc4(x)))\n",
    "        x = self.bn4(x)\n",
    "        x = self.dropout(F.silu(self.fc5(x)))\n",
    "        x = self.bn5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossChecker:\n",
    "    # 指定した回数、lossが改善されていなければ打ち止めする\n",
    "    def __init__(self, patience=20):\n",
    "        self.patience = patience\n",
    "        self.bef_epoch = 0\n",
    "        self.min_loss = float('inf')\n",
    "\n",
    "    def step(self, epoch, loss):\n",
    "        if self.min_loss > loss:\n",
    "            self.min_loss = loss\n",
    "            self.bef_epoch = epoch\n",
    "        if epoch - self.bef_epoch > self.patience:\n",
    "            self.bef_epoch = epoch\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "early_stopping = LossChecker(CFG.early_stopping)\n",
    "increase_batch = LossChecker(CFG.increase_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習の対象とする特徴量を列挙する\n",
    "all_features = df_train.columns.tolist()\n",
    "all_features.remove(CFG.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalcBatchSize:\n",
    "    # バッチサイズを\n",
    "    # 極大->最小->ちょっと増加->ちょっと増加->...の順で遷移させる\n",
    "    def __init__(self, min_batch_size, max_batch_size, rate=1.2):\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.first_batch_size = max_batch_size\n",
    "        self.current_batch_size = -1\n",
    "        self.cnt = -1\n",
    "        self.rate = rate\n",
    "\n",
    "    def step(self):\n",
    "        self.cnt += 1\n",
    "        if self.cnt == 0:\n",
    "            self.current_batch_size = self.first_batch_size\n",
    "            return self.current_batch_size\n",
    "        elif self.cnt == 1:\n",
    "            self.current_batch_size = self.min_batch_size\n",
    "            return self.current_batch_size\n",
    "        else:\n",
    "            self.current_batch_size *= self.rate\n",
    "            self.current_batch_size = int(self.current_batch_size) + 1\n",
    "            self.current_batch_size = max(self.current_batch_size, self.min_batch_size)\n",
    "            return self.current_batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(hidden_size, n_splits, df_train, df_test):\n",
    "    # df_trainをシャッフル\n",
    "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(skf.split(df_train, df_train[CFG.target])):\n",
    "        print('-----', fold, '-----')\n",
    "        # fold毎に初期化する設定\n",
    "        # model, optimizer, scheduler, その他変数\n",
    "        model = Net(len(all_features), hidden_size, 1).to(CFG.device)\n",
    "        print(model)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=CFG.lr_factor)\n",
    "        calc_batch_size = CalcBatchSize(CFG.min_batch_size, CFG.max_batch_size, CFG.batch_increase_rate)\n",
    "        batch_size = calc_batch_size.step()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_loss_min = np.Inf\n",
    "\n",
    "        # データを分割する。fold毎に一回だけやる\n",
    "        X_train = df_train[all_features].iloc[train_index]\n",
    "        y_train = df_train.iloc[train_index][CFG.target]\n",
    "        X_valid = df_train[all_features].iloc[valid_index]\n",
    "        y_valid = df_train.iloc[valid_index][CFG.target]\n",
    "        X_test = df_test[all_features]\n",
    "        st_time = time.time()\n",
    "\n",
    "        for epoch in range(CFG.epochs):\n",
    "\n",
    "            # TODO Variableを使うのは古いらしい\n",
    "            x0_var = Variable(torch.FloatTensor(X_train.values)).to(CFG.device)\n",
    "            y0_var = Variable(torch.FloatTensor(y_train.values)).to(CFG.device)\n",
    "            i = -1\n",
    "            while True:\n",
    "                i += 1\n",
    "                # ミニバッチ学習\n",
    "                start = i * batch_size\n",
    "                end   = start + batch_size\n",
    "                x_var = x0_var[start:end]\n",
    "                y_var = y0_var[start:end]\n",
    "                if len(x_var) == 1:\n",
    "                    print(\"len(x_var) == 1, cant use bn layer. skip this batch\")\n",
    "                    break\n",
    "                if len(x_var) == 0:\n",
    "                    # not need to train\n",
    "                    break\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_var).squeeze(1)\n",
    "                loss   = criterion(output, y_var)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()*batch_size\n",
    "                if np.isnan(loss.item()):\n",
    "                    print('-----nan-----')\n",
    "                    print(loss.item(), torch.isnan(output).any(), torch.isnan(y_var).any())\n",
    "                    raise \"ERROR loss.item() is nan\"\n",
    "\n",
    "            train_loss = train_loss / len(X_train)\n",
    "            if train_loss <= train_loss_min:\n",
    "                print(\"Epoch: \" + str(epoch+1).zfill(5), \"batch_size: \" + str(batch_size), \"Validation loss decreased ({:6f} ===> {:6f}). Saving the model...\".format(train_loss_min,train_loss))\n",
    "                torch.save(model.state_dict(), CFG.model_path + \"/model\" + str(fold) + \".pt\")\n",
    "                train_loss_min = train_loss\n",
    "\n",
    "            # batch_sizeを増やす ただし初回だけは学習速度向上の視点からbatch_sizeを非常に大きな値にしている\n",
    "            # 暫くlossが変わってなければ、EarlyStoppingする。\n",
    "            if increase_batch.step(epoch, train_loss):\n",
    "                batch_size = calc_batch_size.step()\n",
    "                print('change batch', batch_size)\n",
    "\n",
    "            if early_stopping.step(epoch, train_loss):\n",
    "                print('early stopping', epoch, train_loss)\n",
    "                break\n",
    "\n",
    "            # log\n",
    "            if epoch % 200 == 0:\n",
    "                print('')\n",
    "                print(\"Epoch: \" + str(epoch+1), \"batch_size: \" + str(batch_size), \"Train Loss: {}\".format(train_loss), \"Time per epoch:\", (time.time() - st_time)/(epoch+1))\n",
    "\n",
    "        print('STOP train fold=', fold)\n",
    "        x0_var = Variable(torch.FloatTensor(X_valid.values)).to(CFG.device)\n",
    "        pred = torch.sigmoid(model(x0_var))\n",
    "        df_train.loc[valid_index, CFG.pred] = pred.data.cpu().numpy()\n",
    "        auc = roc_auc_score(df_train.loc[valid_index, CFG.target], df_train.loc[valid_index, CFG.pred])\n",
    "        print('fold', fold, 'AUC:', auc)\n",
    "        del model, optimizer, X_train, y_train, X_valid, y_valid, X_test, x0_var, y0_var, x_var, y_var, pred\n",
    "        gc.collect()\n",
    "\n",
    "    auc = roc_auc_score(df_train[CFG.target], df_train[CFG.pred])\n",
    "    print('-----', 'Training Ended!', '-----')\n",
    "    print('AUC', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 0 -----\n",
      "Net(\n",
      "  (fc1): Linear(in_features=346, out_features=32, bias=True)\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc5): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (bn5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc6): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Epoch: 00001 batch_size: 1000000 Validation loss decreased (   inf ===> 1.193654). Saving the model...\n",
      "\n",
      "Epoch: 1 batch_size: 1000000 Train Loss: 1.1936535437901814 Time per epoch: 23.372233152389526\n",
      "Epoch: 00003 batch_size: 1000000 Validation loss decreased (1.193654 ===> 1.122042). Saving the model...\n",
      "Epoch: 00004 batch_size: 1000000 Validation loss decreased (1.122042 ===> 1.079285). Saving the model...\n",
      "Epoch: 00005 batch_size: 1000000 Validation loss decreased (1.079285 ===> 1.022499). Saving the model...\n",
      "Epoch: 00006 batch_size: 1000000 Validation loss decreased (1.022499 ===> 0.981181). Saving the model...\n",
      "Epoch: 00007 batch_size: 1000000 Validation loss decreased (0.981181 ===> 0.955144). Saving the model...\n",
      "Epoch: 00008 batch_size: 1000000 Validation loss decreased (0.955144 ===> 0.935152). Saving the model...\n",
      "Epoch: 00009 batch_size: 1000000 Validation loss decreased (0.935152 ===> 0.915886). Saving the model...\n",
      "Epoch: 00010 batch_size: 1000000 Validation loss decreased (0.915886 ===> 0.899931). Saving the model...\n",
      "Epoch: 00011 batch_size: 1000000 Validation loss decreased (0.899931 ===> 0.886151). Saving the model...\n",
      "Epoch: 00012 batch_size: 1000000 Validation loss decreased (0.886151 ===> 0.875750). Saving the model...\n",
      "Epoch: 00013 batch_size: 1000000 Validation loss decreased (0.875750 ===> 0.864601). Saving the model...\n",
      "Epoch: 00014 batch_size: 1000000 Validation loss decreased (0.864601 ===> 0.854153). Saving the model...\n",
      "Epoch: 00015 batch_size: 1000000 Validation loss decreased (0.854153 ===> 0.844337). Saving the model...\n",
      "Epoch: 00016 batch_size: 1000000 Validation loss decreased (0.844337 ===> 0.835056). Saving the model...\n",
      "Epoch: 00017 batch_size: 1000000 Validation loss decreased (0.835056 ===> 0.827361). Saving the model...\n",
      "Epoch: 00018 batch_size: 1000000 Validation loss decreased (0.827361 ===> 0.819707). Saving the model...\n",
      "Epoch: 00019 batch_size: 1000000 Validation loss decreased (0.819707 ===> 0.812509). Saving the model...\n",
      "Epoch: 00020 batch_size: 1000000 Validation loss decreased (0.812509 ===> 0.807048). Saving the model...\n",
      "Epoch: 00021 batch_size: 1000000 Validation loss decreased (0.807048 ===> 0.801153). Saving the model...\n",
      "Epoch: 00022 batch_size: 1000000 Validation loss decreased (0.801153 ===> 0.795873). Saving the model...\n",
      "Epoch: 00023 batch_size: 1000000 Validation loss decreased (0.795873 ===> 0.791122). Saving the model...\n",
      "Epoch: 00024 batch_size: 1000000 Validation loss decreased (0.791122 ===> 0.786909). Saving the model...\n",
      "Epoch: 00025 batch_size: 1000000 Validation loss decreased (0.786909 ===> 0.782317). Saving the model...\n",
      "Epoch: 00026 batch_size: 1000000 Validation loss decreased (0.782317 ===> 0.779486). Saving the model...\n",
      "Epoch: 00027 batch_size: 1000000 Validation loss decreased (0.779486 ===> 0.774781). Saving the model...\n",
      "Epoch: 00028 batch_size: 1000000 Validation loss decreased (0.774781 ===> 0.771541). Saving the model...\n",
      "Epoch: 00029 batch_size: 1000000 Validation loss decreased (0.771541 ===> 0.768615). Saving the model...\n",
      "Epoch: 00030 batch_size: 1000000 Validation loss decreased (0.768615 ===> 0.765997). Saving the model...\n",
      "Epoch: 00031 batch_size: 1000000 Validation loss decreased (0.765997 ===> 0.762667). Saving the model...\n",
      "Epoch: 00032 batch_size: 1000000 Validation loss decreased (0.762667 ===> 0.759718). Saving the model...\n",
      "Epoch: 00033 batch_size: 1000000 Validation loss decreased (0.759718 ===> 0.757944). Saving the model...\n",
      "Epoch: 00034 batch_size: 1000000 Validation loss decreased (0.757944 ===> 0.754173). Saving the model...\n",
      "Epoch: 00035 batch_size: 1000000 Validation loss decreased (0.754173 ===> 0.752190). Saving the model...\n",
      "Epoch: 00036 batch_size: 1000000 Validation loss decreased (0.752190 ===> 0.750527). Saving the model...\n",
      "Epoch: 00037 batch_size: 1000000 Validation loss decreased (0.750527 ===> 0.748188). Saving the model...\n",
      "Epoch: 00038 batch_size: 1000000 Validation loss decreased (0.748188 ===> 0.745017). Saving the model...\n",
      "Epoch: 00039 batch_size: 1000000 Validation loss decreased (0.745017 ===> 0.742631). Saving the model...\n",
      "Epoch: 00040 batch_size: 1000000 Validation loss decreased (0.742631 ===> 0.740230). Saving the model...\n",
      "Epoch: 00041 batch_size: 1000000 Validation loss decreased (0.740230 ===> 0.738392). Saving the model...\n",
      "Epoch: 00042 batch_size: 1000000 Validation loss decreased (0.738392 ===> 0.735538). Saving the model...\n",
      "Epoch: 00043 batch_size: 1000000 Validation loss decreased (0.735538 ===> 0.733141). Saving the model...\n",
      "Epoch: 00044 batch_size: 1000000 Validation loss decreased (0.733141 ===> 0.731777). Saving the model...\n",
      "Epoch: 00045 batch_size: 1000000 Validation loss decreased (0.731777 ===> 0.728573). Saving the model...\n",
      "Epoch: 00046 batch_size: 1000000 Validation loss decreased (0.728573 ===> 0.726442). Saving the model...\n",
      "Epoch: 00047 batch_size: 1000000 Validation loss decreased (0.726442 ===> 0.724584). Saving the model...\n",
      "Epoch: 00048 batch_size: 1000000 Validation loss decreased (0.724584 ===> 0.721933). Saving the model...\n",
      "Epoch: 00049 batch_size: 1000000 Validation loss decreased (0.721933 ===> 0.719384). Saving the model...\n",
      "Epoch: 00050 batch_size: 1000000 Validation loss decreased (0.719384 ===> 0.717870). Saving the model...\n",
      "Epoch: 00051 batch_size: 1000000 Validation loss decreased (0.717870 ===> 0.714386). Saving the model...\n",
      "Epoch: 00052 batch_size: 1000000 Validation loss decreased (0.714386 ===> 0.711556). Saving the model...\n",
      "Epoch: 00053 batch_size: 1000000 Validation loss decreased (0.711556 ===> 0.709723). Saving the model...\n",
      "Epoch: 00054 batch_size: 1000000 Validation loss decreased (0.709723 ===> 0.707863). Saving the model...\n",
      "Epoch: 00055 batch_size: 1000000 Validation loss decreased (0.707863 ===> 0.705103). Saving the model...\n",
      "Epoch: 00056 batch_size: 1000000 Validation loss decreased (0.705103 ===> 0.702710). Saving the model...\n",
      "Epoch: 00057 batch_size: 1000000 Validation loss decreased (0.702710 ===> 0.699968). Saving the model...\n",
      "Epoch: 00058 batch_size: 1000000 Validation loss decreased (0.699968 ===> 0.697997). Saving the model...\n",
      "Epoch: 00059 batch_size: 1000000 Validation loss decreased (0.697997 ===> 0.696198). Saving the model...\n",
      "Epoch: 00060 batch_size: 1000000 Validation loss decreased (0.696198 ===> 0.694121). Saving the model...\n",
      "Epoch: 00061 batch_size: 1000000 Validation loss decreased (0.694121 ===> 0.692150). Saving the model...\n",
      "Epoch: 00062 batch_size: 1000000 Validation loss decreased (0.692150 ===> 0.690499). Saving the model...\n",
      "Epoch: 00063 batch_size: 1000000 Validation loss decreased (0.690499 ===> 0.688440). Saving the model...\n",
      "Epoch: 00064 batch_size: 1000000 Validation loss decreased (0.688440 ===> 0.686216). Saving the model...\n",
      "Epoch: 00065 batch_size: 1000000 Validation loss decreased (0.686216 ===> 0.684024). Saving the model...\n",
      "Epoch: 00066 batch_size: 1000000 Validation loss decreased (0.684024 ===> 0.681400). Saving the model...\n",
      "Epoch: 00067 batch_size: 1000000 Validation loss decreased (0.681400 ===> 0.679693). Saving the model...\n",
      "Epoch: 00068 batch_size: 1000000 Validation loss decreased (0.679693 ===> 0.678087). Saving the model...\n",
      "Epoch: 00069 batch_size: 1000000 Validation loss decreased (0.678087 ===> 0.676843). Saving the model...\n",
      "Epoch: 00070 batch_size: 1000000 Validation loss decreased (0.676843 ===> 0.674387). Saving the model...\n",
      "Epoch: 00071 batch_size: 1000000 Validation loss decreased (0.674387 ===> 0.671454). Saving the model...\n",
      "Epoch: 00072 batch_size: 1000000 Validation loss decreased (0.671454 ===> 0.670254). Saving the model...\n",
      "Epoch: 00073 batch_size: 1000000 Validation loss decreased (0.670254 ===> 0.668650). Saving the model...\n",
      "Epoch: 00074 batch_size: 1000000 Validation loss decreased (0.668650 ===> 0.665633). Saving the model...\n",
      "Epoch: 00075 batch_size: 1000000 Validation loss decreased (0.665633 ===> 0.663609). Saving the model...\n",
      "Epoch: 00076 batch_size: 1000000 Validation loss decreased (0.663609 ===> 0.661958). Saving the model...\n",
      "Epoch: 00077 batch_size: 1000000 Validation loss decreased (0.661958 ===> 0.659420). Saving the model...\n",
      "Epoch: 00078 batch_size: 1000000 Validation loss decreased (0.659420 ===> 0.657743). Saving the model...\n",
      "Epoch: 00079 batch_size: 1000000 Validation loss decreased (0.657743 ===> 0.655494). Saving the model...\n",
      "Epoch: 00080 batch_size: 1000000 Validation loss decreased (0.655494 ===> 0.652433). Saving the model...\n",
      "Epoch: 00081 batch_size: 1000000 Validation loss decreased (0.652433 ===> 0.650247). Saving the model...\n",
      "Epoch: 00082 batch_size: 1000000 Validation loss decreased (0.650247 ===> 0.648584). Saving the model...\n",
      "Epoch: 00083 batch_size: 1000000 Validation loss decreased (0.648584 ===> 0.644896). Saving the model...\n",
      "Epoch: 00084 batch_size: 1000000 Validation loss decreased (0.644896 ===> 0.643810). Saving the model...\n",
      "Epoch: 00085 batch_size: 1000000 Validation loss decreased (0.643810 ===> 0.640834). Saving the model...\n",
      "Epoch: 00086 batch_size: 1000000 Validation loss decreased (0.640834 ===> 0.637433). Saving the model...\n",
      "Epoch: 00087 batch_size: 1000000 Validation loss decreased (0.637433 ===> 0.635087). Saving the model...\n",
      "Epoch: 00088 batch_size: 1000000 Validation loss decreased (0.635087 ===> 0.632815). Saving the model...\n",
      "Epoch: 00089 batch_size: 1000000 Validation loss decreased (0.632815 ===> 0.629884). Saving the model...\n",
      "Epoch: 00090 batch_size: 1000000 Validation loss decreased (0.629884 ===> 0.627326). Saving the model...\n",
      "Epoch: 00091 batch_size: 1000000 Validation loss decreased (0.627326 ===> 0.625200). Saving the model...\n",
      "Epoch: 00092 batch_size: 1000000 Validation loss decreased (0.625200 ===> 0.622381). Saving the model...\n",
      "Epoch: 00093 batch_size: 1000000 Validation loss decreased (0.622381 ===> 0.619116). Saving the model...\n",
      "Epoch: 00094 batch_size: 1000000 Validation loss decreased (0.619116 ===> 0.617823). Saving the model...\n",
      "Epoch: 00095 batch_size: 1000000 Validation loss decreased (0.617823 ===> 0.614804). Saving the model...\n",
      "Epoch: 00096 batch_size: 1000000 Validation loss decreased (0.614804 ===> 0.612298). Saving the model...\n",
      "Epoch: 00097 batch_size: 1000000 Validation loss decreased (0.612298 ===> 0.609462). Saving the model...\n",
      "Epoch: 00098 batch_size: 1000000 Validation loss decreased (0.609462 ===> 0.607149). Saving the model...\n",
      "Epoch: 00099 batch_size: 1000000 Validation loss decreased (0.607149 ===> 0.603691). Saving the model...\n",
      "Epoch: 00100 batch_size: 1000000 Validation loss decreased (0.603691 ===> 0.602479). Saving the model...\n",
      "Epoch: 00101 batch_size: 1000000 Validation loss decreased (0.602479 ===> 0.599346). Saving the model...\n",
      "Epoch: 00102 batch_size: 1000000 Validation loss decreased (0.599346 ===> 0.597050). Saving the model...\n",
      "Epoch: 00103 batch_size: 1000000 Validation loss decreased (0.597050 ===> 0.596119). Saving the model...\n",
      "Epoch: 00104 batch_size: 1000000 Validation loss decreased (0.596119 ===> 0.594814). Saving the model...\n",
      "Epoch: 00105 batch_size: 1000000 Validation loss decreased (0.594814 ===> 0.592008). Saving the model...\n",
      "Epoch: 00106 batch_size: 1000000 Validation loss decreased (0.592008 ===> 0.589774). Saving the model...\n",
      "Epoch: 00107 batch_size: 1000000 Validation loss decreased (0.589774 ===> 0.587490). Saving the model...\n",
      "Epoch: 00108 batch_size: 1000000 Validation loss decreased (0.587490 ===> 0.586885). Saving the model...\n",
      "Epoch: 00109 batch_size: 1000000 Validation loss decreased (0.586885 ===> 0.583790). Saving the model...\n",
      "Epoch: 00110 batch_size: 1000000 Validation loss decreased (0.583790 ===> 0.582390). Saving the model...\n",
      "Epoch: 00111 batch_size: 1000000 Validation loss decreased (0.582390 ===> 0.580863). Saving the model...\n",
      "Epoch: 00112 batch_size: 1000000 Validation loss decreased (0.580863 ===> 0.579053). Saving the model...\n",
      "Epoch: 00113 batch_size: 1000000 Validation loss decreased (0.579053 ===> 0.578108). Saving the model...\n",
      "Epoch: 00114 batch_size: 1000000 Validation loss decreased (0.578108 ===> 0.576048). Saving the model...\n",
      "Epoch: 00115 batch_size: 1000000 Validation loss decreased (0.576048 ===> 0.573285). Saving the model...\n",
      "Epoch: 00116 batch_size: 1000000 Validation loss decreased (0.573285 ===> 0.572144). Saving the model...\n",
      "Epoch: 00117 batch_size: 1000000 Validation loss decreased (0.572144 ===> 0.570690). Saving the model...\n",
      "Epoch: 00118 batch_size: 1000000 Validation loss decreased (0.570690 ===> 0.569413). Saving the model...\n",
      "Epoch: 00119 batch_size: 1000000 Validation loss decreased (0.569413 ===> 0.567546). Saving the model...\n",
      "Epoch: 00120 batch_size: 1000000 Validation loss decreased (0.567546 ===> 0.565213). Saving the model...\n",
      "Epoch: 00121 batch_size: 1000000 Validation loss decreased (0.565213 ===> 0.564951). Saving the model...\n",
      "Epoch: 00122 batch_size: 1000000 Validation loss decreased (0.564951 ===> 0.562702). Saving the model...\n",
      "Epoch: 00123 batch_size: 1000000 Validation loss decreased (0.562702 ===> 0.560775). Saving the model...\n",
      "Epoch: 00125 batch_size: 1000000 Validation loss decreased (0.560775 ===> 0.558530). Saving the model...\n",
      "Epoch: 00126 batch_size: 1000000 Validation loss decreased (0.558530 ===> 0.557307). Saving the model...\n",
      "Epoch: 00127 batch_size: 1000000 Validation loss decreased (0.557307 ===> 0.555999). Saving the model...\n",
      "Epoch: 00128 batch_size: 1000000 Validation loss decreased (0.555999 ===> 0.553778). Saving the model...\n",
      "Epoch: 00129 batch_size: 1000000 Validation loss decreased (0.553778 ===> 0.551983). Saving the model...\n",
      "Epoch: 00130 batch_size: 1000000 Validation loss decreased (0.551983 ===> 0.550930). Saving the model...\n",
      "Epoch: 00131 batch_size: 1000000 Validation loss decreased (0.550930 ===> 0.548989). Saving the model...\n",
      "Epoch: 00132 batch_size: 1000000 Validation loss decreased (0.548989 ===> 0.546861). Saving the model...\n",
      "Epoch: 00133 batch_size: 1000000 Validation loss decreased (0.546861 ===> 0.545234). Saving the model...\n",
      "Epoch: 00134 batch_size: 1000000 Validation loss decreased (0.545234 ===> 0.544234). Saving the model...\n",
      "Epoch: 00135 batch_size: 1000000 Validation loss decreased (0.544234 ===> 0.543102). Saving the model...\n",
      "Epoch: 00136 batch_size: 1000000 Validation loss decreased (0.543102 ===> 0.541236). Saving the model...\n",
      "Epoch: 00137 batch_size: 1000000 Validation loss decreased (0.541236 ===> 0.538254). Saving the model...\n",
      "Epoch: 00138 batch_size: 1000000 Validation loss decreased (0.538254 ===> 0.536060). Saving the model...\n",
      "Epoch: 00139 batch_size: 1000000 Validation loss decreased (0.536060 ===> 0.533124). Saving the model...\n",
      "Epoch: 00141 batch_size: 1000000 Validation loss decreased (0.533124 ===> 0.531818). Saving the model...\n",
      "Epoch: 00142 batch_size: 1000000 Validation loss decreased (0.531818 ===> 0.529734). Saving the model...\n",
      "Epoch: 00143 batch_size: 1000000 Validation loss decreased (0.529734 ===> 0.526821). Saving the model...\n",
      "Epoch: 00144 batch_size: 1000000 Validation loss decreased (0.526821 ===> 0.523932). Saving the model...\n",
      "Epoch: 00145 batch_size: 1000000 Validation loss decreased (0.523932 ===> 0.523168). Saving the model...\n",
      "Epoch: 00146 batch_size: 1000000 Validation loss decreased (0.523168 ===> 0.520673). Saving the model...\n",
      "Epoch: 00147 batch_size: 1000000 Validation loss decreased (0.520673 ===> 0.518495). Saving the model...\n",
      "Epoch: 00148 batch_size: 1000000 Validation loss decreased (0.518495 ===> 0.515625). Saving the model...\n",
      "Epoch: 00149 batch_size: 1000000 Validation loss decreased (0.515625 ===> 0.514284). Saving the model...\n",
      "Epoch: 00150 batch_size: 1000000 Validation loss decreased (0.514284 ===> 0.512392). Saving the model...\n",
      "Epoch: 00151 batch_size: 1000000 Validation loss decreased (0.512392 ===> 0.509259). Saving the model...\n",
      "Epoch: 00152 batch_size: 1000000 Validation loss decreased (0.509259 ===> 0.506875). Saving the model...\n",
      "Epoch: 00153 batch_size: 1000000 Validation loss decreased (0.506875 ===> 0.505234). Saving the model...\n",
      "Epoch: 00154 batch_size: 1000000 Validation loss decreased (0.505234 ===> 0.504472). Saving the model...\n",
      "Epoch: 00155 batch_size: 1000000 Validation loss decreased (0.504472 ===> 0.503449). Saving the model...\n",
      "Epoch: 00156 batch_size: 1000000 Validation loss decreased (0.503449 ===> 0.500958). Saving the model...\n",
      "Epoch: 00157 batch_size: 1000000 Validation loss decreased (0.500958 ===> 0.499005). Saving the model...\n",
      "Epoch: 00158 batch_size: 1000000 Validation loss decreased (0.499005 ===> 0.497037). Saving the model...\n",
      "Epoch: 00159 batch_size: 1000000 Validation loss decreased (0.497037 ===> 0.495527). Saving the model...\n",
      "Epoch: 00160 batch_size: 1000000 Validation loss decreased (0.495527 ===> 0.493946). Saving the model...\n",
      "Epoch: 00161 batch_size: 1000000 Validation loss decreased (0.493946 ===> 0.493924). Saving the model...\n",
      "Epoch: 00162 batch_size: 1000000 Validation loss decreased (0.493924 ===> 0.491430). Saving the model...\n",
      "Epoch: 00163 batch_size: 1000000 Validation loss decreased (0.491430 ===> 0.490763). Saving the model...\n",
      "Epoch: 00164 batch_size: 1000000 Validation loss decreased (0.490763 ===> 0.489074). Saving the model...\n",
      "Epoch: 00165 batch_size: 1000000 Validation loss decreased (0.489074 ===> 0.487057). Saving the model...\n",
      "Epoch: 00166 batch_size: 1000000 Validation loss decreased (0.487057 ===> 0.486571). Saving the model...\n",
      "Epoch: 00167 batch_size: 1000000 Validation loss decreased (0.486571 ===> 0.484547). Saving the model...\n",
      "Epoch: 00168 batch_size: 1000000 Validation loss decreased (0.484547 ===> 0.482330). Saving the model...\n",
      "Epoch: 00169 batch_size: 1000000 Validation loss decreased (0.482330 ===> 0.482241). Saving the model...\n",
      "Epoch: 00171 batch_size: 1000000 Validation loss decreased (0.482241 ===> 0.479132). Saving the model...\n",
      "Epoch: 00172 batch_size: 1000000 Validation loss decreased (0.479132 ===> 0.477795). Saving the model...\n",
      "Epoch: 00173 batch_size: 1000000 Validation loss decreased (0.477795 ===> 0.477189). Saving the model...\n",
      "Epoch: 00174 batch_size: 1000000 Validation loss decreased (0.477189 ===> 0.476096). Saving the model...\n",
      "Epoch: 00175 batch_size: 1000000 Validation loss decreased (0.476096 ===> 0.474115). Saving the model...\n",
      "Epoch: 00176 batch_size: 1000000 Validation loss decreased (0.474115 ===> 0.472915). Saving the model...\n",
      "Epoch: 00177 batch_size: 1000000 Validation loss decreased (0.472915 ===> 0.471912). Saving the model...\n",
      "Epoch: 00179 batch_size: 1000000 Validation loss decreased (0.471912 ===> 0.470192). Saving the model...\n",
      "Epoch: 00180 batch_size: 1000000 Validation loss decreased (0.470192 ===> 0.469222). Saving the model...\n",
      "Epoch: 00181 batch_size: 1000000 Validation loss decreased (0.469222 ===> 0.468673). Saving the model...\n",
      "Epoch: 00182 batch_size: 1000000 Validation loss decreased (0.468673 ===> 0.467245). Saving the model...\n",
      "Epoch: 00183 batch_size: 1000000 Validation loss decreased (0.467245 ===> 0.466977). Saving the model...\n",
      "Epoch: 00184 batch_size: 1000000 Validation loss decreased (0.466977 ===> 0.464887). Saving the model...\n",
      "Epoch: 00186 batch_size: 1000000 Validation loss decreased (0.464887 ===> 0.462644). Saving the model...\n",
      "Epoch: 00187 batch_size: 1000000 Validation loss decreased (0.462644 ===> 0.462255). Saving the model...\n",
      "Epoch: 00188 batch_size: 1000000 Validation loss decreased (0.462255 ===> 0.462037). Saving the model...\n",
      "Epoch: 00189 batch_size: 1000000 Validation loss decreased (0.462037 ===> 0.460733). Saving the model...\n",
      "Epoch: 00190 batch_size: 1000000 Validation loss decreased (0.460733 ===> 0.460309). Saving the model...\n",
      "Epoch: 00191 batch_size: 1000000 Validation loss decreased (0.460309 ===> 0.457772). Saving the model...\n",
      "Epoch: 00193 batch_size: 1000000 Validation loss decreased (0.457772 ===> 0.457462). Saving the model...\n",
      "Epoch: 00194 batch_size: 1000000 Validation loss decreased (0.457462 ===> 0.455516). Saving the model...\n",
      "Epoch: 00196 batch_size: 1000000 Validation loss decreased (0.455516 ===> 0.455235). Saving the model...\n",
      "Epoch: 00197 batch_size: 1000000 Validation loss decreased (0.455235 ===> 0.453720). Saving the model...\n",
      "Epoch: 00199 batch_size: 1000000 Validation loss decreased (0.453720 ===> 0.452074). Saving the model...\n",
      "Epoch: 00201 batch_size: 1000000 Validation loss decreased (0.452074 ===> 0.450469). Saving the model...\n",
      "\n",
      "Epoch: 201 batch_size: 1000000 Train Loss: 0.45046911520826893 Time per epoch: 0.7815494478045412\n",
      "Epoch: 00204 batch_size: 1000000 Validation loss decreased (0.450469 ===> 0.449314). Saving the model...\n",
      "Epoch: 00205 batch_size: 1000000 Validation loss decreased (0.449314 ===> 0.449135). Saving the model...\n",
      "Epoch: 00206 batch_size: 1000000 Validation loss decreased (0.449135 ===> 0.448991). Saving the model...\n",
      "Epoch: 00207 batch_size: 1000000 Validation loss decreased (0.448991 ===> 0.448001). Saving the model...\n",
      "Epoch: 00208 batch_size: 1000000 Validation loss decreased (0.448001 ===> 0.445637). Saving the model...\n",
      "Epoch: 00212 batch_size: 1000000 Validation loss decreased (0.445637 ===> 0.443022). Saving the model...\n",
      "Epoch: 00213 batch_size: 1000000 Validation loss decreased (0.443022 ===> 0.442804). Saving the model...\n",
      "Epoch: 00215 batch_size: 1000000 Validation loss decreased (0.442804 ===> 0.441955). Saving the model...\n",
      "Epoch: 00217 batch_size: 1000000 Validation loss decreased (0.441955 ===> 0.441399). Saving the model...\n",
      "Epoch: 00218 batch_size: 1000000 Validation loss decreased (0.441399 ===> 0.440241). Saving the model...\n",
      "Epoch: 00219 batch_size: 1000000 Validation loss decreased (0.440241 ===> 0.439956). Saving the model...\n",
      "Epoch: 00220 batch_size: 1000000 Validation loss decreased (0.439956 ===> 0.438797). Saving the model...\n",
      "Epoch: 00222 batch_size: 1000000 Validation loss decreased (0.438797 ===> 0.437225). Saving the model...\n",
      "Epoch: 00224 batch_size: 1000000 Validation loss decreased (0.437225 ===> 0.437103). Saving the model...\n",
      "Epoch: 00225 batch_size: 1000000 Validation loss decreased (0.437103 ===> 0.435948). Saving the model...\n",
      "Epoch: 00226 batch_size: 1000000 Validation loss decreased (0.435948 ===> 0.434870). Saving the model...\n",
      "Epoch: 00227 batch_size: 1000000 Validation loss decreased (0.434870 ===> 0.434164). Saving the model...\n",
      "Epoch: 00228 batch_size: 1000000 Validation loss decreased (0.434164 ===> 0.433697). Saving the model...\n",
      "Epoch: 00230 batch_size: 1000000 Validation loss decreased (0.433697 ===> 0.431731). Saving the model...\n",
      "Epoch: 00232 batch_size: 1000000 Validation loss decreased (0.431731 ===> 0.430650). Saving the model...\n",
      "Epoch: 00233 batch_size: 1000000 Validation loss decreased (0.430650 ===> 0.429701). Saving the model...\n",
      "Epoch: 00235 batch_size: 1000000 Validation loss decreased (0.429701 ===> 0.428898). Saving the model...\n",
      "Epoch: 00236 batch_size: 1000000 Validation loss decreased (0.428898 ===> 0.427922). Saving the model...\n",
      "Epoch: 00237 batch_size: 1000000 Validation loss decreased (0.427922 ===> 0.426471). Saving the model...\n",
      "Epoch: 00238 batch_size: 1000000 Validation loss decreased (0.426471 ===> 0.425597). Saving the model...\n",
      "Epoch: 00239 batch_size: 1000000 Validation loss decreased (0.425597 ===> 0.425023). Saving the model...\n",
      "Epoch: 00240 batch_size: 1000000 Validation loss decreased (0.425023 ===> 0.423862). Saving the model...\n",
      "Epoch: 00242 batch_size: 1000000 Validation loss decreased (0.423862 ===> 0.421416). Saving the model...\n",
      "Epoch: 00243 batch_size: 1000000 Validation loss decreased (0.421416 ===> 0.420009). Saving the model...\n",
      "Epoch: 00244 batch_size: 1000000 Validation loss decreased (0.420009 ===> 0.419983). Saving the model...\n",
      "Epoch: 00245 batch_size: 1000000 Validation loss decreased (0.419983 ===> 0.418216). Saving the model...\n",
      "Epoch: 00246 batch_size: 1000000 Validation loss decreased (0.418216 ===> 0.418142). Saving the model...\n",
      "Epoch: 00247 batch_size: 1000000 Validation loss decreased (0.418142 ===> 0.416844). Saving the model...\n",
      "Epoch: 00248 batch_size: 1000000 Validation loss decreased (0.416844 ===> 0.416488). Saving the model...\n",
      "Epoch: 00249 batch_size: 1000000 Validation loss decreased (0.416488 ===> 0.414487). Saving the model...\n",
      "Epoch: 00250 batch_size: 1000000 Validation loss decreased (0.414487 ===> 0.414261). Saving the model...\n",
      "Epoch: 00251 batch_size: 1000000 Validation loss decreased (0.414261 ===> 0.413797). Saving the model...\n",
      "Epoch: 00252 batch_size: 1000000 Validation loss decreased (0.413797 ===> 0.412203). Saving the model...\n",
      "Epoch: 00253 batch_size: 1000000 Validation loss decreased (0.412203 ===> 0.411080). Saving the model...\n",
      "Epoch: 00254 batch_size: 1000000 Validation loss decreased (0.411080 ===> 0.411074). Saving the model...\n",
      "Epoch: 00255 batch_size: 1000000 Validation loss decreased (0.411074 ===> 0.408730). Saving the model...\n",
      "Epoch: 00256 batch_size: 1000000 Validation loss decreased (0.408730 ===> 0.407276). Saving the model...\n",
      "Epoch: 00257 batch_size: 1000000 Validation loss decreased (0.407276 ===> 0.406465). Saving the model...\n",
      "Epoch: 00258 batch_size: 1000000 Validation loss decreased (0.406465 ===> 0.405098). Saving the model...\n",
      "Epoch: 00259 batch_size: 1000000 Validation loss decreased (0.405098 ===> 0.404151). Saving the model...\n",
      "Epoch: 00260 batch_size: 1000000 Validation loss decreased (0.404151 ===> 0.403232). Saving the model...\n",
      "Epoch: 00262 batch_size: 1000000 Validation loss decreased (0.403232 ===> 0.400316). Saving the model...\n",
      "Epoch: 00263 batch_size: 1000000 Validation loss decreased (0.400316 ===> 0.400209). Saving the model...\n",
      "Epoch: 00264 batch_size: 1000000 Validation loss decreased (0.400209 ===> 0.398717). Saving the model...\n",
      "Epoch: 00266 batch_size: 1000000 Validation loss decreased (0.398717 ===> 0.398207). Saving the model...\n",
      "Epoch: 00268 batch_size: 1000000 Validation loss decreased (0.398207 ===> 0.396188). Saving the model...\n",
      "Epoch: 00270 batch_size: 1000000 Validation loss decreased (0.396188 ===> 0.394664). Saving the model...\n",
      "Epoch: 00271 batch_size: 1000000 Validation loss decreased (0.394664 ===> 0.393750). Saving the model...\n",
      "Epoch: 00273 batch_size: 1000000 Validation loss decreased (0.393750 ===> 0.392625). Saving the model...\n",
      "Epoch: 00274 batch_size: 1000000 Validation loss decreased (0.392625 ===> 0.391744). Saving the model...\n",
      "Epoch: 00276 batch_size: 1000000 Validation loss decreased (0.391744 ===> 0.391166). Saving the model...\n",
      "Epoch: 00277 batch_size: 1000000 Validation loss decreased (0.391166 ===> 0.391157). Saving the model...\n",
      "Epoch: 00278 batch_size: 1000000 Validation loss decreased (0.391157 ===> 0.390215). Saving the model...\n",
      "Epoch: 00279 batch_size: 1000000 Validation loss decreased (0.390215 ===> 0.389815). Saving the model...\n",
      "Epoch: 00280 batch_size: 1000000 Validation loss decreased (0.389815 ===> 0.389483). Saving the model...\n",
      "Epoch: 00281 batch_size: 1000000 Validation loss decreased (0.389483 ===> 0.388614). Saving the model...\n",
      "Epoch: 00282 batch_size: 1000000 Validation loss decreased (0.388614 ===> 0.388429). Saving the model...\n",
      "Epoch: 00283 batch_size: 1000000 Validation loss decreased (0.388429 ===> 0.386729). Saving the model...\n",
      "Epoch: 00284 batch_size: 1000000 Validation loss decreased (0.386729 ===> 0.386599). Saving the model...\n",
      "Epoch: 00285 batch_size: 1000000 Validation loss decreased (0.386599 ===> 0.385524). Saving the model...\n",
      "Epoch: 00286 batch_size: 1000000 Validation loss decreased (0.385524 ===> 0.383596). Saving the model...\n",
      "Epoch: 00290 batch_size: 1000000 Validation loss decreased (0.383596 ===> 0.383166). Saving the model...\n",
      "Epoch: 00292 batch_size: 1000000 Validation loss decreased (0.383166 ===> 0.381587). Saving the model...\n",
      "Epoch: 00294 batch_size: 1000000 Validation loss decreased (0.381587 ===> 0.381163). Saving the model...\n",
      "Epoch: 00296 batch_size: 1000000 Validation loss decreased (0.381163 ===> 0.380890). Saving the model...\n",
      "Epoch: 00297 batch_size: 1000000 Validation loss decreased (0.380890 ===> 0.380333). Saving the model...\n",
      "Epoch: 00298 batch_size: 1000000 Validation loss decreased (0.380333 ===> 0.380153). Saving the model...\n",
      "Epoch: 00299 batch_size: 1000000 Validation loss decreased (0.380153 ===> 0.379882). Saving the model...\n",
      "Epoch: 00300 batch_size: 1000000 Validation loss decreased (0.379882 ===> 0.379343). Saving the model...\n",
      "Epoch: 00301 batch_size: 1000000 Validation loss decreased (0.379343 ===> 0.377761). Saving the model...\n",
      "Epoch: 00306 batch_size: 1000000 Validation loss decreased (0.377761 ===> 0.376887). Saving the model...\n",
      "Epoch: 00307 batch_size: 1000000 Validation loss decreased (0.376887 ===> 0.375570). Saving the model...\n",
      "Epoch: 00309 batch_size: 1000000 Validation loss decreased (0.375570 ===> 0.374749). Saving the model...\n",
      "Epoch: 00313 batch_size: 1000000 Validation loss decreased (0.374749 ===> 0.374558). Saving the model...\n",
      "Epoch: 00314 batch_size: 1000000 Validation loss decreased (0.374558 ===> 0.374014). Saving the model...\n",
      "Epoch: 00315 batch_size: 1000000 Validation loss decreased (0.374014 ===> 0.372236). Saving the model...\n",
      "Epoch: 00318 batch_size: 1000000 Validation loss decreased (0.372236 ===> 0.370281). Saving the model...\n",
      "Epoch: 00324 batch_size: 1000000 Validation loss decreased (0.370281 ===> 0.369030). Saving the model...\n",
      "Epoch: 00327 batch_size: 1000000 Validation loss decreased (0.369030 ===> 0.368586). Saving the model...\n",
      "Epoch: 00328 batch_size: 1000000 Validation loss decreased (0.368586 ===> 0.367718). Saving the model...\n",
      "Epoch: 00330 batch_size: 1000000 Validation loss decreased (0.367718 ===> 0.367416). Saving the model...\n",
      "Epoch: 00332 batch_size: 1000000 Validation loss decreased (0.367416 ===> 0.367128). Saving the model...\n",
      "Epoch: 00333 batch_size: 1000000 Validation loss decreased (0.367128 ===> 0.365981). Saving the model...\n",
      "Epoch: 00338 batch_size: 1000000 Validation loss decreased (0.365981 ===> 0.365508). Saving the model...\n",
      "Epoch: 00339 batch_size: 1000000 Validation loss decreased (0.365508 ===> 0.365396). Saving the model...\n",
      "Epoch: 00340 batch_size: 1000000 Validation loss decreased (0.365396 ===> 0.365296). Saving the model...\n",
      "Epoch: 00341 batch_size: 1000000 Validation loss decreased (0.365296 ===> 0.364556). Saving the model...\n",
      "Epoch: 00343 batch_size: 1000000 Validation loss decreased (0.364556 ===> 0.364300). Saving the model...\n",
      "Epoch: 00345 batch_size: 1000000 Validation loss decreased (0.364300 ===> 0.363474). Saving the model...\n",
      "Epoch: 00346 batch_size: 1000000 Validation loss decreased (0.363474 ===> 0.363104). Saving the model...\n",
      "Epoch: 00347 batch_size: 1000000 Validation loss decreased (0.363104 ===> 0.362420). Saving the model...\n",
      "Epoch: 00349 batch_size: 1000000 Validation loss decreased (0.362420 ===> 0.361649). Saving the model...\n",
      "Epoch: 00354 batch_size: 1000000 Validation loss decreased (0.361649 ===> 0.361394). Saving the model...\n",
      "Epoch: 00356 batch_size: 1000000 Validation loss decreased (0.361394 ===> 0.361067). Saving the model...\n",
      "Epoch: 00360 batch_size: 1000000 Validation loss decreased (0.361067 ===> 0.360576). Saving the model...\n",
      "Epoch: 00363 batch_size: 1000000 Validation loss decreased (0.360576 ===> 0.360365). Saving the model...\n",
      "Epoch: 00365 batch_size: 1000000 Validation loss decreased (0.360365 ===> 0.359629). Saving the model...\n",
      "Epoch: 00370 batch_size: 1000000 Validation loss decreased (0.359629 ===> 0.359118). Saving the model...\n",
      "Epoch: 00373 batch_size: 1000000 Validation loss decreased (0.359118 ===> 0.358739). Saving the model...\n",
      "Epoch: 00374 batch_size: 1000000 Validation loss decreased (0.358739 ===> 0.358062). Saving the model...\n",
      "Epoch: 00375 batch_size: 1000000 Validation loss decreased (0.358062 ===> 0.356679). Saving the model...\n",
      "Epoch: 00381 batch_size: 1000000 Validation loss decreased (0.356679 ===> 0.355334). Saving the model...\n",
      "Epoch: 00392 batch_size: 1000000 Validation loss decreased (0.355334 ===> 0.354798). Saving the model...\n",
      "Epoch: 00395 batch_size: 1000000 Validation loss decreased (0.354798 ===> 0.354483). Saving the model...\n",
      "Epoch: 00398 batch_size: 1000000 Validation loss decreased (0.354483 ===> 0.354395). Saving the model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\Takanori\\Documents\\github\\myprivate\\Kaggle\\TPS2205\\Pytorch_DNN.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000030?line=0'>1</a>\u001b[0m fit(\u001b[39m32\u001b[39;49m, \u001b[39m3\u001b[39;49m, df_train, df_test)\n",
      "\u001b[1;32md:\\Users\\Takanori\\Documents\\github\\myprivate\\Kaggle\\TPS2205\\Pytorch_DNN.ipynb Cell 31'\u001b[0m in \u001b[0;36mfit\u001b[1;34m(hidden_size, n_splits, df_train, df_test)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000029?line=27'>28</a>\u001b[0m st_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000029?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(CFG\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000029?line=30'>31</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000029?line=31'>32</a>\u001b[0m     \u001b[39m# TODO Variableを使うのは古いらしい\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000029?line=32'>33</a>\u001b[0m     x0_var \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39;49mFloatTensor(X_train\u001b[39m.\u001b[39;49mvalues))\u001b[39m.\u001b[39;49mto(CFG\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000029?line=33'>34</a>\u001b[0m     y0_var \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mFloatTensor(y_train\u001b[39m.\u001b[39mvalues))\u001b[39m.\u001b[39mto(CFG\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Takanori/Documents/github/myprivate/Kaggle/TPS2205/Pytorch_DNN.ipynb#ch0000029?line=34'>35</a>\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(32, 3, df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(64, 10, df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(128, 10, df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(256, 10, df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.sort(glob.glob(f\"./{CFG.model_path}/*.pt\"))\n",
    "print(models)\n",
    "# fold別に作った10個のモデルをロードする\n",
    "with torch.no_grad():\n",
    "    for i, model_name in enumerate(models):\n",
    "        model = Net(len(all_features), 512, 1).to(CFG.device)\n",
    "        model.load_state_dict(torch.load(model_name,))\n",
    "        X_test = df_test[all_features]\n",
    "        x0_var = Variable(torch.FloatTensor(X_test.values)).to(CFG.device)\n",
    "        pred = F.sigmoid(model(x0_var))\n",
    "        df_test[CFG.test_pred[i]] = pred.data.cpu().numpy()\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[CFG.target] = df_test[CFG.test_pred].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[CFG.target] = (df_sub[CFG.target] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_3layer_512.csv', index=False)\n",
    "df_test.to_csv('test_3layer_512.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60f8cdbf2a96461788475085dd1e9d6dd7137de331e19aa3c17c37cb4f0963a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yourenvname')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
